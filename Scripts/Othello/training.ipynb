{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training d'un agent Othello"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importation des modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Board import Board\n",
    "from Agent import Memory, Model, encode_action, reward_from_signal, game_over_from_signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## Définition d'un mmodèle de réseau de neurones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Input (2, 8, 8,)\n",
    "        # Output (65,)\n",
    "        self.input_layer = nn.Conv2d(2, 128, 3, padding=1)\n",
    "        self.hidden_layer_1 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.hidden_layer_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.hidden_layer_3 = nn.Linear(128*8*8, 128*8)\n",
    "        self.hidden_layer_4 = nn.Linear(128*8, 128)\n",
    "        self.output_layer = nn.Linear(128, 65)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden_layer_1(x))\n",
    "        x = F.relu(self.hidden_layer_2(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.hidden_layer_3(x))\n",
    "        x = F.relu(self.hidden_layer_4(x))\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "source": [
    "## Méthode de visualisation des q_matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_q(q: list):\n",
    "    dimension = (128, 128)\n",
    "    max_negative = np.min([np.min(q), 0])\n",
    "    max_positive = np.max([np.max(q), 0])\n",
    "    image = Image.new('RGBA', dimension, (255,255,225,255))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    y = 0\n",
    "    i = 0\n",
    "    for l in range(8):\n",
    "        x = 0\n",
    "        for c in range(8):\n",
    "            xy = [(x + 1, y + 1), (x + 14, y + 14)]\n",
    "            r = q[i]\n",
    "            if r > 0:\n",
    "                # a = int(r / max_positive * 255)\n",
    "                a = min(255, max(0, int(r * 255)))\n",
    "                color = (0,0,255,a)\n",
    "                draw.rectangle(xy, fill=color)\n",
    "            elif r < 0:\n",
    "                a = min(255, max(0, int(-r * 255)))\n",
    "                # a = int(r / max_negative * 255)\n",
    "                color = (255,0,0,a)\n",
    "                draw.rectangle(xy, fill=color)\n",
    "            x += 16\n",
    "            i += 1\n",
    "        y += 16\n",
    "    display(image)"
   ]
  },
  {
   "source": [
    "## Premiere étape : apprentissage d'un seul coup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, before training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228E758B910>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC5klEQVR4nO3dy04VQRCA4WohQbyieN2cef/H6tmIF/CIKJII5cKeF6gmMfH/v32lAvnTm9Mz0zJ7hrAOIyJa2z2pDGeuVxER0Zb3pe3ZP4z9r4r7v4z5w+L87zH/rDh/GRERbTmuzEf267H/tLj/fHb/g9Kg/hsGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDNn4PZPAHg/v6O3paD0nT224j5+wSt7Z4W57+P+ZfF+YuIiGjLm8p8ZP809rfi/hzzs/cRHlbmI/svTwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA47wPAeQLAbe8HmH2+fvb38H96nyDaclKZj+z7sb80n7nux/6j4v6bMV/6/0X2K08AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOO8DwHkCwG3vB3hXms5+FjF/HyDa8rq4//PYP3ufYWp+5vn8MT/1fgK/F6AyA4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA47wPAeQLAbfcBaiFkvxvzpff1R/aLiIjWdqX7CJnr2dh/Uty/H/ufF/d/G/NT7ze4h/9/6XsDkf3SEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO+wBwngBw2/cCTivDmev5mC+FlLnejfnHxfkfERHRltLz/ZF9ez/A7N//ojj/dczP3ocov5/AEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO+wBwngBw93IfINpyUNqe/XbMHxXnb8Z86fn8yH4VEdHa7qQ0nut+7C+9XyCyb+8XKH0vIXOd/l6CJwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc9wHgPAHgtu8FTP2efw/Pt7fi/hz73xb3fxz7HxX3/xz7j4v7r8f+qfsMM+9H8ASAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOA8z4A3B8ZBAXlrnwcuQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228E758B8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZwQmAMBAAwSj2Yf9lWUnsQYUIO/MPucdyn9vmvOYga189AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAuGP1AIwxtvPZu3m9/toGiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A418A/+OCq95QNECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADibsPvCMKxJoiKAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228E758B910>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC+UlEQVR4nO3dyZIUNxRG4V/QDGYyYHaO6Hz/x1JGsGPqNkNjsC8b5QvcrBXnfPsbiqo+rU2qlKNqVoR1lSRjXP/RGa7avyVJxvZXa/Wa79f6rfmq/Zi/35z/b80/ac5/TZKM7WFnPjX/Xev/2Vz/Zq3f+vul5rd7rUH9NgwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4IaPg9ncAeCukiRjG63ptXuMcf24NV773Zp/1pz/vObPPk8/e56h9f1V7cf3d+rzZ2yPOvOp+d0dAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM7zAHDuAHDH/QBXneGq/WeSZGyt3+en5kV+nz/G9Yvm/G2SZGzPO/Op+c9a/+zz/Nb3n5rH9+/9AOoxADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPACcOwDccT/Am9Z0zXdr/tT9Ahnbq+b8x+Qi7wt40Jz/kSQZW+t+hNS8W/OnPv+Z9xW4A8AZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcB5HgDOHQDuOA9w6vf9GVvrvv7UvEmSMa5ft8Zr/7DWb90PkJq3a/3W/QBV+3E/wKn7DTK23j9izf/XfPt+A3cAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOM8DwLkDwB3vCzj1PPzs8+wL3Ld/9n0FL5vrf1rzrfMQVftNkmRsf3fmU/Ptmm/db5CaP9wB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4DwPAOcOAHecBzj7PPvs79vbz7PX/NPm/JfkIuchWucZUvPzWr/1voCq/XhfQvt9B+4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcJ4HgHMHgDveF3DVmq75M7nI8+zRnK81/6Y5/y5JMrbHnfnUvFvrt+ar9ru1/tnzEO3zGO4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcJ4HgPsFWP0C5Rs+4OAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Get initial state from the board\n",
    "initial_s, signal = board.reset()\n",
    "initial_s = torch.tensor(initial_s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "# Predict an action\n",
    "a_scores = model(initial_s)\n",
    "a = torch.argmax(a_scores).item()\n",
    "encoded_a = encode_action(a)\n",
    "\n",
    "# Perform the action in the game\n",
    "s, signal = board.step(encoded_a)\n",
    "next_s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "# Process the signal\n",
    "r = reward_from_signal(signal, board)\n",
    "done = game_over_from_signal(signal, board)\n",
    "\n",
    "# Q Learning\n",
    "target_q = torch.zeros((65,), dtype=torch.float)\n",
    "\n",
    "# Add the reward and action\n",
    "target_q[a] = r\n",
    "\n",
    "# Render model before the training\n",
    "print(\"Q_matrix of initial state, before training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])\n",
    "\n",
    "# Toggle on train mode\n",
    "model.train()\n",
    "\n",
    "for i in range(16):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "    loss = criterion(torch.tensor(target_q, dtype=torch.float), q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Target Q matrix:\")\n",
    "render_q(target_q.tolist())\n",
    "\n",
    "# Render model after the training\n",
    "print(\"Q_matrix of initial state, after training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Deuxième étape : apprentissage du premier état avec historique"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, before training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228CBA91610>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC8UlEQVR4nO3dy24TQRBG4WqwuAYwhASxGb//Y/VsEAkJTgjhIkix6XmBmllxzrf/ZYROejPtccvsGcLaRUS0Nr2ojDPn+7F/X9x/iYiIdjit7CP71fj8Vf/+aIe3xc//OvYnxf1dRERr01lpnvPl2s9/VBrqv2EAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGANd8HMzmCQC3i4iIdvhYWmf/NPYvi/vvY78v7o8REa1Nr0vznG/H5699nr/2PsJ58fMvxueX/pAz5wdPADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADjvA8B5AsAt9wFWfT++telxaZ7z37F/U9zfbPH50Q6l5/mRfXmeX/tDyv6wxb61aV+a53z0BIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPgCcJwDcch+g9P36yL58v37t8+x3pXnO1xvt1/5ewtPi/tfY74r7PxER0Q6tso/s6QkAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA530AOE8AuOU+wJPSOvvviE3el196P0Bkvxn7tfcZ1v5eQuk+QmS/Hvtnxf3Psd8X974fgM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM77AHCeAHCb/F5AtMNZcX859h+K+89jv/Z9/fvi/jj2J8X93diX3i8Q2Zf3C5TuQ2TOt54AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4HgPMEgFvuA5TeVx/Zl/fVnxf3Fxvt136//lVx/y0iorWpdB8gc17uA5Tf9z/2p8X9lScAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPcB4DwB4HYREa1NzyvjzPlHRKx+nr7B+/ZLv1cQ2e8jIlqbSr9XkDnfjP2+uD+Ofel5fuZ8Nfal/7/M+doTAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM77AHD/AK0W+9ZWgYqFAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228CBA91040>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABkUlEQVR4nO3awQ3CQBAEwT1EHJB/WJDIkYOFBHZX/U/+tPbjWXu/9pB1n5mZ9Tz2er/G+3O/vx17yVUIIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNA3PI7uM0FiLMHiL93AeIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgzh4gzgWIsweYmbUex57v91e+bw/AzwggTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcPUCcCxBnD/CF92feE7gAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBBnDxDnAsT9xR7gzP/Tz/7eBYgTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOHuAOBcg7i/2APYE9gD8iADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcTZA8S5AHGX2AN4bw/AQQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBBnDxDnAsTZA8TfuwBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEGcPEPcB7Emn1h2zBaIAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228E7591220>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC10lEQVR4nO2dS2pUURgGzxXXEVfgwCcaMSIaifiOcSCuTxwY4xuDUcSIUXwOXEH3Ro6T/27gO46sqnlx6e7in5z/np56X/QmWA631lqbjqxFdl/sl38s9H+Xfzv0n5V/IfQ/ln8/9B+Vvxn6O+UfDf0/5d8I/ZeHIlH+GwwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgBn8jiYjRMAzrwP8CCy++Jh+RdD/0P566G/V/7Z0P9S/lbob5d/OvS/lR+f55d/KfTfOwHgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI77AHCcAHDmfYCbkd0XL8q/Fvqvyx89T18N/YPWWpumlTOR3pdf6/mj39/V0H9Tfnw/ghMAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOC4DwDHCQBn3gcYPY+O308v/1Tofy//ROj/LH8j9Hdba22aVqLP3/ty/vyj/3cQ70M4AeAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjvsAcJwAcOZ9gNH7/ofu65+mlZOR3pc/6vmj9xOMvt8/6t8J/aflx/scTgA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgOM+ABwnAJx5H+BuZPfFk/Kvh/6r8m+F/vPWWpumlfOR3pef6vmb4fN3yr8S+m/LH93HWAv9fScAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcBxHwCOEwDOvA8wdB7/D95vH91HuBz678pfD/298ofuRxg5zy//Xug/dgLAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABz3AeA4AeDM+wDnIrsvPpcfn0eXvxH6u+Vvhf52+aPn+cdD/1f5q6F/UH78+zkB4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACO+wBw/gJ/YefWfSHFjgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Render model before the training\n",
    "print(\"Q_matrix of initial state, before training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])\n",
    "\n",
    "# (Action, Reward) container for each action\n",
    "history = []\n",
    "\n",
    "# Populating the history with every possible move\n",
    "for i_action in range(65):\n",
    "    # Reset the board\n",
    "    board.reset()\n",
    "\n",
    "    # Predict an action\n",
    "    a = i_action\n",
    "    encoded_a = encode_action(a)\n",
    "\n",
    "    # Perform the action in the game\n",
    "    s, signal = board.step(encoded_a)\n",
    "\n",
    "    # Process the signal\n",
    "    r = reward_from_signal(signal, board)\n",
    "\n",
    "    # Add the record to history\n",
    "    history.append((a, r))\n",
    "\n",
    "# Q Learning matrix for the initial state\n",
    "target_q = torch.zeros((65,), dtype=torch.float)\n",
    "\n",
    "# Add the reward and action for each game in history\n",
    "# NOTE: ça overwrite si y'a plusieurs valeurs\n",
    "for (a, r) in history:\n",
    "    target_q[a] = r\n",
    "\n",
    "print(\"Target Q matrix:\")\n",
    "render_q(target_q.tolist())\n",
    "\n",
    "# Toggle on train mode\n",
    "model.train()\n",
    "\n",
    "for i_batch in range(16):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "    loss = criterion(torch.tensor(target_q, dtype=torch.float), q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Render model after the training\n",
    "print(f\"Q_matrix of initial state, after training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Troisième étape : à combien de coup est-ce que l'agent \"comprend\" le premier état ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E070>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACo0lEQVR4nO3dy2oUURRG4X9r8G5MRCNOqt//saomYhQT4yUq6HFSDRnvLmh0rW++2R1YOZM6XV1jzCPCOkmS1O5Ra3rMP5KkarpojY/lcp1/3pz/kiSpXWc8GXPW+YP+/tTufnP+9zrfGr/z+dvz93qT+l8YAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcCVj4PZPAHg9vcBetPr8+iq6U1rfCzvt9if2p0156832n/o/OPm/O06f96cv/IEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+AJwnANxJklRNLzrDYyyfkxz8/fiqqfU8fIzldp1/0Jz/leSg5+nr/mO/3+Blc/6TJwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc9wHgPAHgNnk/wLHnq6ZqjY9lbLT/dXP/h432nzb333gCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwHkfAM4TAG6r+wCt5+EZ84ckqZp642PZ72/N33meftbcf73R/ovm/st1f+8fecx/PAHgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgvA8A5wkAt8l9gKrpVWt8LB+32J/avW3Ov1vnW7+XkDHvfy+h/Tw+Oe77DTwB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4LwPAOcJAHeSHP999//67wWkdk+a89+TpGp61hofy9d1f2u99wFkAHQGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDnfQA4TwC4/fsBnramx/xtne9t3+55/mlz/8063xq/8/kftsbH8nOj/efN/VeeAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDeB4D7C8Wx3dZqqZGOAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E1C0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADAklEQVR4nO3dz24URxRG8a+CFSAExyAgSiJNv/9jdUsJAiwwGCcxUnzZVL/A7VnlnN/+qvznTG2qpntUrRVhXSRJxvK0NV3rP0kyxulNa7y293P+eXP+ds4/bs7fJ0nG8qQzn1r/nfOPmvP/zfnW759ab+f8q+b89Q+tQf1vGACcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHADY+D2dwB4C6Ss5zH/9qcf5ckGcuzznxqvZvzV835mzl/6Dx/jFPrg1S1Pcz1j95HeNGc/+QOAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd9ADh3ALj9PsAvneGq7XOSc5ynt55PULXtzyf4sTn/LUkylped+dT6ca5/2Vz/y5xv3Qeo2vb7AO2f3x0AzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzvsAcO4AcPt9gEPn2RnL69bqtX6Y86fm/DbnR3O+5vxvzfm3STLGqfW8/qrtes63fv6qreZ8+//nDgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDnfQA4dwC4/T5A63n9Vdv+vP5D9wHGOP3RXP/Puf5Vc/2buX7reftV26e5/tHf/+j7Fnof5Fof3AHgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgvA8A5w4At98HaIVQtT3M+dZ5eNW2n4cfWv/o9/uP3ic4ch6fnOX5AO33LbgDwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwHkfAM4dAG6/D3DoPD9j+bm1eq1f5/xVc/4mOct9gkPn+RnLT835v5NkjFPr71e17X+/1vsKUuu1OwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc9wHg3AHgLpIkY2m9LyC13iXHvp8+1299Pz777jWWy+b8lyQZ4/S4NV7b/TnmM5bfO/Op9a+5fvt9B+4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4HgPsOnXYC5ZkhDwgAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 32\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E070>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC/UlEQVR4nO3dMavOYRjH8evSKV6BQfo/ZWCwYaIspBQpZfAKKDEbjAYzKV6B4ZQSpcSimLAZGNT/TgavgOk2nPv0nPk6/zrl+/3sPzf17V6e2/Nk73MPYW1ERESuaus+b81zulKa9/ZiifMjV4+K+9sREZnTtdK8t82xf1Dc342IJf79t4r7x/tqJ+t/YQBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAl34czOYNALcREZE5/aiMe29HIiIiV4dKp/f519iX5js+D5+K+7bQ+bvaZ05PS/PebozzzxTP/+ANAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd7ADhvALjt9wCnKuPe26eIiMjVsdLpff429heL+9cREZnTwdK8t99jXzu+j+cEOZ0r7t9t/QF79x7BGwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO9wBw3gBwi/xewF7vM6erpXlvz5c4P3JV+r2B6PPmEudnTvtL897+egPAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHA+R4AzhsAbvs9wOHSus8/x752+vrz7Np8/P/8Bc4/Wjz/+xLnL/D3P16a9/bVGwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO9wBw3gBwi3w/QOZ0vjTv7e04/33x/LPj/Np8/Z7gRPH8L2N/urj/GBGROd0rzXu7P84vHe/vBcgA6AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM73AHDeAHAbERGZ083KuPf2JCIiclX8QH5uY1+a73iP8Ko07+3SQufv9vsFDhTP/zP2pbnvAWQAdAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd7ADhvALjt7we4U1r3+eHY105ff1/+yeL+c0RE5vSsNO/t+ji/dPxS3/cfuXpZ3F+OiMicLpTmvb3xBoAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPQDcP5GW3dbPY6NNAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 32\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E1C0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADW0lEQVR4nO2dQYuOYRiFz61Z2NioKRbet6Q0E4qFKGpokJBmml9AycbKTlkpOysbKX6BRkLCxBRFFhSaSUm9j4WpKRub2d0Wc39/4IyVc6791fkWV/dmnt6JzCFhZBkDAET/jLJzOA0AEd1hSs/2rvafkPtny79I+vcAIKI7ROnZ3pc/S/rzAIDoXzI+cjhRPrWPHOY3UaL5b3AA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkCc8J+DtfEFEGcMACK6O4yc2S4DAKLfTK3nsFb+MulPlL+D9H+W/5r0j/0LP6K7ROnZ7tb+JLm/5AsgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4PYA4vgDijN4D7GbkzPYNABD9Nmo9h5XyJ0h/GQAiui2Unu1P+Q9Jf6Z86vdntvV3ENE/YHzkMFc+/X0FXwBxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMfvAcTxBRBn9P8CHlF2DufL/0D6B8tfJP0pAIjoDlB6to+1v9Hfv5/0P5X/gvRPAkBER32fIbOt+QKI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDh+DyCOL4A4o/cAWyk7h9/lb/R7+V8pPdue2n9F7h+v/Z3k/o/ap74vgBxmyp8n/VkAiOio7zNkthVfAHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQx+8BxPEFEGf0HmCBsnOYBoCIbi+lZ/tS+zfJ/Wu1f5/cv1D7k+T+Uvm7SP87AER0c5Se7UHtPyb3z/kCiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4fg8gji+AOGMAENGdYuTM9hwAEP12aj2HX+Wvkv44AER0Vyk9263aXyT3p2p/nNxfLT9If/16R/+G8ZHDUV8AcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDH7wHE8QUQZ/R9gGnKzmGh/Lekf6T8nvQHAIjorlB6ttu1/5TcP1P+IulPlX+D9K8DQES3j9KzffYFEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABx/B5AnL+NLfrWKspZbAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 62\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228EF698130>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACrUlEQVR4nO3coYpVURxG8f+WUQyOYTCfC2aDiGWaWCxW9QkMJrsPYDcZfAK1WiximyJiMAv3ZDE4BlFwG+6+cPN3Dwy61q9/7FEWu8ye03pf9xLWQVVVtVW27uvNvE1TNO/zPM5/H55/a+xfhPuHVVWtTTejeZ8/jPOj47f/f9VWP8P9xbF/He7vnYuG+m8YAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcA1fx3M5g0At8h7gGqrLKS+/rPQ+VfC/dex/xTur4/9t3B/NPbRfOff/yzcP/YGgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM9AJw3ANz2PcBhtO7r06qq1qYL0bzPv8b5x+H5J2MfzXe+b5DN++bzBgv8Pv/M9t4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcL4HgPMGgFvq+wD0/eVw/32h8+O9NwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc7wHgvAHgDqqqWpuOknHv8+Y7+f/43+e3Nt0Nz3+zxPkL7C+F+x/eAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHC+B4DzBoBb5PsArU13onmf3y50/ml4/uE4/0l4/tOxPx/uf499NPf7ANqbAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc7wHgvAHgtt8HeJCMe59fVtWZ/318a9P9aN7nV+P85+H5j8Y+mu/8/MfRvM8n+57vDQBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDnewA4bwC47fcBbkfrvn439l/C/dWxj+YLfm//Y7i/sdD5n8P9tX3P9waAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOA8z0A3F/bNM3WvTg63QAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 62\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E730>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAAC8UlEQVR4nO3cIYsVURiH8fes64qgCLJ5bjELsmmjFoNZ/QYGi3k/gdli8Buo2WDRuGkRzJaZvAiiIK66x/IOTn7nguDz/Pqfg/Jwypy9rfexh7B2IyKibc5L6z7uRES0NgyleZ+mPP998fzbuX9R3D+KiGhtOCjN+3SS56/6/4u2+V7cX879q+L+wU5pqP+GAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQBc83MwmzcA3Pwe4Fdp3cd5Xwupj+e5L82jj5H7/eL+NPcfivtbuT8t7vdzX5ov/v3Pivsn3gBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwvgeA8waAm7/nXy2t+/g1IqK1Ya8079NZnn9YPP8496X5/D29teF3ad6nC3n+2vcUa/fl3yfwBoAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPQCcNwDc/D25tv779+lrv2f/6/PX/t7/qvcU0Tal9wjRx/k9gu8BVGMAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd7ADhvALjdiIjWhuuVce/T54hY/T29taE271Pk+aX94vcB7hXPf7ON87fwHuFKcf/NGwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO9wBw3gBwW/n7+NaGu6V5n97m+aXjF9/zvxTPv5bnHxXPf5r7i8X9z9z/KO4v5d7fB1CNAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc7wHgvAHg5t8HeFgZ9z69jIht/F5/7RbqY4uIaG24X5r36XWe/7x4/uPcl+aL9wyHpXmfjvP88v+fNwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc7wHgvAHg5u/xd0rrPr7L/afi/kbuS/PF7+2vek8QbXNS3B9s6fyPxf3N3J8V93veAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHC+B4D7A42r5NaEWmJIAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 92\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E8ABE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACWUlEQVR4nO3dsS5EURCA4RmRKBT6La6otQqNSHSeQIiH8CgeQogn0ElEo9CqxS30CoXGUbhCPXsKu///9ZPZyJ/T7GRlay8thLUaERG5WZtuL9/jOcxK4218nfZfF/cfTfMXxfnTiIjM4bA03sabaf97cf/6NF8a//n7zzO/UpvUsjAAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgEu/DmbzBYDrdQ+wVhpv40eP/ZGb28X5p077X4vzs077r4rzx74AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4DwPkCwHW5B+gwv1ecv++xP3OojbcxeuzvMP9ZnF/xBYAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzHgDOFwDuv9wDLPT8It8T+ALAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAeQ8A5wsAtxoRkTnsVoZbGx8iYuG/T88cdor7H3vs7zCfxfnmCwBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDnPQCcLwBcl98HyBwOSuNtvO20vzb+e09wXtx/Nu3fKO5/m/aX1vv7AJqbAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc9wBwvgBwS/H/AjKHrdJ4G5+n/ZfF/SfTfGn8z+ffL4238W7e/b4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4DwPkCwC3FPYDz3gOoyADgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAznsAuC/4X8TWRp4T1AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 92\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x228F4E7E1C0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADMElEQVR4nO2dP4tVVxRH91FJaSdY3ZeAWIhNQERIKhEzjhMyk0KjgoGEVEGw9CNYCiFViKDgvxTOBMdxFLGKIBJIE1JIILm3CthZCnLS7PcF9rzKtVa/+DWL3bwzd1rvYw/BsiciItpss2T3cSUiorVhf0nv03+5f7m4fy3974v+jxERrQ0HSnqf/s7968X9b9J/XPQ/S3+j6K/uKony3mAAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4zZ+D2XgB4MzfA9wo2X38OiKitWFvSe/Tm9wvzUcfI/0Pi/6/6T8s+qfT/6nof5f+/aL/ZfqXiv4PXgA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgON7ADheADgLeQ+wgN+zS3+fH32c/33+VtFfjohobfilpPfpTO7/Xtw/kv520V9K/27R/8oLAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwfA8AxwsAZ/4e4HbJ7uP59F8U/WPp/1H0P07/t6L/afpPi/6JiIjWhvWS3qe1RexHmz0p+ie9AHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAx/cAcLwAcPZERLQ2fFSRe5/+iYiINnteWu/jJ7n/qLh/Kvf/Ku4fyv2Dxf1XuV/+PT79m0X/YvofFP23XgA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgON7ADheADjz7wNsluw+rkREtDYcLel9epn7G8X91dy/Vdy/kPvfFvd/zv3dxf13ub/T7zP8WvS/8ALAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABzfA8DxAsCZvwe4U7L7eG5B/oOi/3lERGvDvpLep9e5f6W4fzX97aK/FBHR2nC4pPfpz9z3/wVIDQOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcDxPQAcLwCc+XuAeyW7j2fTL33vP/o4/97/s6J/PP0dvSeINlsv+msL8jeK/mr6W0V/2QsAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHB8DwDnfzvS79bXXy2pAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "batch_size = 32\n",
    "n_epochs = 128 # On aura pas joué tous les coups mais OK\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# (Action, Reward) container for each action\n",
    "history = []\n",
    "\n",
    "# Populating the history with every possible move\n",
    "for i_epoch in range(n_epochs):\n",
    "    # Reset the board\n",
    "    initial_s, _ = board.reset()\n",
    "    initial_s = torch.tensor(initial_s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Play a random action\n",
    "    a = torch.randint(0, 65, (1,)).item()\n",
    "    encoded_a = encode_action(a)\n",
    "\n",
    "    # Perform the action in the game\n",
    "    s, signal = board.step(encoded_a)\n",
    "\n",
    "    # Process the signal\n",
    "    r = reward_from_signal(signal, board)\n",
    "\n",
    "    # Add the record to history\n",
    "    history.append((a, r))\n",
    "\n",
    "    # Continue playing if we don't have enough moves to learn in the history\n",
    "    if len(history) <= batch_size:\n",
    "        continue\n",
    "    # Keep history the size of the batch size by removing first element in list\n",
    "    history.pop(0)\n",
    "\n",
    "    # Q Learning matrix for the initial state\n",
    "    # NOTE: Je pense qu'il faudrait partir de la prédiction du modèle\n",
    "    # et y ajouter les rewards, les cases où il y a 0 peuvent peut-être\n",
    "    # poser problème\n",
    "    target_q = model(initial_s).view(-1)# torch.zeros((65,), dtype=torch.float)\n",
    "\n",
    "    # Add the reward and action for each game in history\n",
    "    # NOTE: ça overwrite si y'a plusieurs valeurs, donc batch_size devrait être < 65\n",
    "    # TODO: moyenne des valeurs ? pas d'overwrite ?\n",
    "    for (a, r) in random.sample(history, batch_size):\n",
    "        target_q[a] = r\n",
    "\n",
    "    # Toggle on train mode\n",
    "    model.train()\n",
    "\n",
    "    # for i_batch in range(4):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "    loss = criterion(torch.tensor(target_q, dtype=torch.float), q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    # Display matrixes\n",
    "    if (i_epoch - batch_size) % 30 == 1:\n",
    "        # Render model after the training\n",
    "        print(f\"Target Q matrix: step {i_epoch - batch_size + 1}\")\n",
    "        render_q(target_q.tolist())\n",
    "\n",
    "        print(f\"Q_matrix of initial state, after training: step {i_epoch - batch_size + 1}\")\n",
    "        q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "        render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Etape 4 : mouvements futurs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}