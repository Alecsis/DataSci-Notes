{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.Othello.Board import Board\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## Replay Memory\n",
    "\n",
    "Nous utiliserons la mémoire de relecture de l'expérience pour former notre DQN. Elle stocke les transitions que l'agent observe, ce qui nous permet de réutiliser ces données plus tard. En prélevant des échantillons de façon aléatoire, les transitions qui constituent un lot sont décorrélées. Il a été démontré que cela stabilise et améliore grandement la procédure d'entraînement du DQN.\n",
    "\n",
    "Pour cela, nous allons avoir besoin de deux classes :\n",
    "\n",
    "- **Transition** - un tuple nommé représentant une seule transition dans notre environnement. Il fait essentiellement correspondre les paires (état, action) à leur résultat (next_state, récompense), l'état étant l'image de différence d'écran comme décrit plus loin.\n",
    "- **ReplayMemory** - un tampon cyclique de taille limitée qui contient les transitions observées récemment. Il met également en œuvre une méthode .sample() pour sélectionner un lot aléatoire de transitions pour l'entraînement.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "source": [
    "DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted, cumulative reward Rt0=∑∞t=t0γt−t0rt\n",
    ", where Rt0 is also known as the return. The discount, γ, should be a constant between 0 and 1\n",
    "\n",
    "that ensures the sum converges. It makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function Q∗:State×Action→R\n",
    "\n",
    ", that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "π∗(s)=argmaxa Q∗(s,a)\n",
    "\n",
    "However, we don’t know everything about the world, so we don’t have access to Q∗\n",
    ". But, since neural networks are universal function approximators, we can simply create one and train it to resemble Q∗\n",
    "\n",
    ".\n",
    "\n",
    "For our training update rule, we’ll use a fact that every Q\n",
    "\n",
    "function for some policy obeys the Bellman equation:\n",
    "Qπ(s,a)=r+γQπ(s′,π(s′))\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, δ\n",
    "\n",
    ":\n",
    "δ=Q(s,a)−(r+γmaxaQ(s′,a))\n",
    "\n",
    "To minimise this error, we will use the Huber loss. The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of Q\n",
    "are very noisy. We calculate this over a batch of transitions, B\n",
    "\n",
    ", sampled from the replay memory:\n",
    "L=1|B|∑(s,a,s′,r) ∈ BL(δ)\n",
    "whereL(δ)=⎧⎩⎨⎪⎪12δ2|δ|−12for |δ|≤1,otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}