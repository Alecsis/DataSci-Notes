{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.Othello.Board import Board\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## Replay Memory\n",
    "\n",
    "Nous utiliserons la mémoire de relecture de l'expérience pour former notre DQN. Elle stocke les transitions que l'agent observe, ce qui nous permet de réutiliser ces données plus tard. En prélevant des échantillons de façon aléatoire, les transitions qui constituent un lot sont décorrélées. Il a été démontré que cela stabilise et améliore grandement la procédure d'entraînement du DQN.\n",
    "\n",
    "Pour cela, nous allons avoir besoin de deux classes :\n",
    "\n",
    "- **Transition** - un tuple nommé représentant une seule transition dans notre environnement. Il fait essentiellement correspondre les paires (état, action) à leur résultat (next_state, récompense), l'état étant l'image de différence d'écran comme décrit plus loin.\n",
    "- **ReplayMemory** - un tampon cyclique de taille limitée qui contient les transitions observées récemment. Il met également en œuvre une méthode .sample() pour sélectionner un lot aléatoire de transitions pour l'entraînement.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  }
 ]
}