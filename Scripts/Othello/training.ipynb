{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training d'un agent Othello"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importation des modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Board import Board, Signal\n",
    "from Agent import Memory, Model, encode_action\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "## Définition d'un mmodèle de réseau de neurones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Input (2, 8, 8,)\n",
    "        # Output (65,)\n",
    "        self.input_layer = nn.Linear(128, 128)\n",
    "        self.hidden_layer_1 = nn.Linear(128, 128)\n",
    "        self.hidden_layer_2 = nn.Linear(128, 128)\n",
    "        self.hidden_layer_3 = nn.Linear(128, 128)\n",
    "        self.hidden_layer_4 = nn.Linear(128, 128)\n",
    "        self.hidden_layer_5 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_6 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_7 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_8 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_9 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_10 = nn.Linear(128, 128)\n",
    "        # self.hidden_layer_11 = nn.Linear(128, 128)\n",
    "        # self.input_layer = nn.Conv2d(2, 128, 3, padding=1)\n",
    "        # self.hidden_layer_1 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        # self.hidden_layer_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        # self.hidden_layer_3 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        # self.hidden_layer_4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        # self.hidden_layer_5 = nn.Linear(128*8*8, 128*8)\n",
    "        # self.hidden_layer_6 = nn.Linear(128*8, 128)\n",
    "        self.output_layer = nn.Linear(128, 65)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden_layer_1(x))\n",
    "        x = F.relu(self.hidden_layer_2(x))\n",
    "        x = F.relu(self.hidden_layer_3(x))\n",
    "        x = F.relu(self.hidden_layer_4(x))\n",
    "        x = F.relu(self.hidden_layer_5(x))\n",
    "        # x = F.relu(self.hidden_layer_6(x))\n",
    "        # x = F.relu(self.hidden_layer_7(x))\n",
    "        # x = F.relu(self.hidden_layer_8(x))\n",
    "        # x = F.relu(self.hidden_layer_9(x))\n",
    "        # x = F.relu(self.hidden_layer_10(x))\n",
    "        # x = F.relu(self.hidden_layer_11(x))\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_from_signal(signal: Signal, board: Board):\n",
    "    reward = 0\n",
    "    if signal is Signal.ILLEGAL_MOVE:\n",
    "            reward = -1\n",
    "    elif signal is Signal.VALID_MOVE:\n",
    "        reward = 1\n",
    "    else: # Game over\n",
    "        winner = board.get_winner()\n",
    "        if winner == 1: # White\n",
    "            reward = 100\n",
    "        elif winner == -1: # Black\n",
    "            reward = -100\n",
    "        else: # Draw\n",
    "            reward = -50\n",
    "    return reward\n",
    "\n",
    "def game_over_from_signal(signal: Signal, board: Board):\n",
    "    game_over = signal in [Signal.GAME_OVER, Signal.ILLEGAL_MOVE]\n",
    "    return game_over"
   ]
  },
  {
   "source": [
    "## Méthode de visualisation des q_matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_q(q: list):\n",
    "    dimension = (128, 128)\n",
    "    max_negative = np.min([np.min(q), 0])\n",
    "    max_positive = np.max([np.max(q), 0])\n",
    "    image = Image.new('RGBA', dimension, (255,255,225,255))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    y = 0\n",
    "    i = 0\n",
    "    for l in range(8):\n",
    "        x = 0\n",
    "        for c in range(8):\n",
    "            xy = [(x + 1, y + 1), (x + 14, y + 14)]\n",
    "            r = q[i]\n",
    "            if r > 0:\n",
    "                # a = min(255, max(0, int(r * 1 * 255)))\n",
    "                a = int(r / max_positive * 255)\n",
    "                color = (0,0,255,a)\n",
    "                draw.rectangle(xy, fill=color)\n",
    "            elif r < 0:\n",
    "                # a = min(255, max(0, int(-r * 1 * 255)))\n",
    "                a = int(r / max_negative * 255)\n",
    "                color = (255,0,0,a)\n",
    "                draw.rectangle(xy, fill=color)\n",
    "            x += 16\n",
    "            i += 1\n",
    "        y += 16\n",
    "    display(image)"
   ]
  },
  {
   "source": [
    "## Premiere étape : apprentissage d'un seul coup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, before training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CD235F2760>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADRklEQVR4nO2dT4uNcRiG75+mbMRSFs6rKEosNEmhJKWQj8CCDaVBmrGQhSyY5E9qbFjwEYRS0hRKmixIUZT3tZAl2dh4LDznCzxj5b6u/dW9uebZnN+c0yL6ENgyIUmtjY5W5IjhtiSpdSdL69FfT3+66M9KUmujLSU9hte5/7G4vy73txb3X6V/qeiflSS1blvFV/Qvl5RE+G8gAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMxpfBzsDRfAnPF7gI0VOWJ4l/7hon9XktS6IxVf0d9Jf0XR/57+pqL/VpJaGx0s6THcT39l0f8mSWrdh4qv6NdzAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhPYA5XABzxu8BnlfkiGFH+jNF/7IkqXXLK76i/5H7m4v7b9J/VPT3pX+66F9N/1DRvydJat1kxVf0C1wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc3gPYA4XwJwJSVLrXpTs6Lenf6boX5EW//0Eat1Ucf9G7h8v7s+lf6PoT6Vf+kOMGH6n/6To7+ECmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMN7AHO4AOaM3wM8KNnRH5Ck1kajkh7DkPulz+MV/Vz6F4r++fTXFP3P6R8r+rekf/J7A6eK/jUugDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDm8BzCHC2DO+PcCvlTkiGG1pEV/Ht/aaGdx/1nuLyvu/8z9mh5/nzOodQvF/cncf1zc35v7T4v7u7kA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5vAewBwugDnj7wdYVbKj/yr9k++731X059OfLvqz6S/2PcKGiq/o36e/tuh/kqTWRrMlPYZpLoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5vAcwhwtgzvg9QM2OXumX/z89/V1Ff16SWhtdLOkxnMv9/cX9h7k/U9y/nPsnivs3c39pcf8XF8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc3gOY8wd3OAPlhusikwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCD4689BE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQmAQAwAwVPsw/7LspKzBxFO3Jl/II8ln2xzXnOQta9egLUEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNA3LF6gU/Yzmdz83p3jwVcgDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKI8w0c4xdfvadcgDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQdwO0DQjCJ5xeEwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CD235F2AC0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJElEQVR4nO2dQYtOcRSHf38NkygZGRvuVWIhUlaams2wsPQplBRZsLOyYyFKyqewtMBGTVZKZEHKvWxmZKRIw+LYnPcLnKuU3/Psn867eDqbe+59W8QQAlvmJKm1br4iR4ybkqTW7yxNj+F7+nuK/hdJaq07UdJjfJnzdxXnf8v5J4vzX6T/uugfkyS1vlV8xRBbSiL8NxCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5jQeB3vDBjBndg+wtyJHjJ/TXyr6q5Kk1i9UfMWwkf7Wov9bklrrFkt6jOvp3y/6FyRN/v1q/cGi/4ENYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAcyZ3QPcqsgR49X0HxT98+kfLvrv0j9U9N+nf6PoX0//StG/nf7lon8n/X1Ff40NYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAcyZkyS1vivZMYzpT/3e/5GSHuPbnL+7OP9rzr9YnH8v/cdF/4wkqfXbKr5i+JXza3qMbAB3CMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gzuweoPR+uWJYk6Y/T1frdxTn/0h/6vf2p/pTn+cfLekxvkl/ueg/YwOYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz2AOWwAc2b/F/CoIkeMZyVNfp7fWne6OP9J+gtFf0OS1PqKLsWg9BeL/roktdatlvQYl3L+geL8j2wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMIcNYM7s+wBT34/fXvR/SlJr3f6SHuOn9B8W/XPpnyr6zyX98+8btNbdLOkxXmMDmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHNm9wC1F+RjGNKf+n788ZIe46v07xb9S5L+xvf+V4rzn+b8+eL8zfRb0Q82gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jzB2gnA+XEGH/hAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Get initial state from the board\n",
    "initial_s, signal = board.reset()\n",
    "initial_s = torch.tensor(initial_s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "# Predict an action\n",
    "a_scores = model(initial_s)\n",
    "a = torch.argmax(a_scores).item()\n",
    "encoded_a = encode_action(a)\n",
    "\n",
    "# Perform the action in the game\n",
    "s, signal = board.step(encoded_a)\n",
    "next_s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "# Process the signal\n",
    "r = reward_from_signal(signal, board)\n",
    "done = game_over_from_signal(signal, board)\n",
    "\n",
    "# Q Learning\n",
    "target_q = torch.zeros((65,), dtype=torch.float)\n",
    "\n",
    "# Add the reward and action\n",
    "target_q[a] = r\n",
    "\n",
    "# Render model before the training\n",
    "print(\"Q_matrix of initial state, before training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])\n",
    "\n",
    "# Toggle on train mode\n",
    "model.train()\n",
    "\n",
    "for i in range(16):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "    loss = criterion(torch.tensor(target_q, dtype=torch.float), q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Target Q matrix:\")\n",
    "render_q(target_q.tolist())\n",
    "\n",
    "# Render model after the training\n",
    "print(\"Q_matrix of initial state, after training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Deuxième étape : apprentissage du premier état avec historique"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, before training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CC89361610>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADHklEQVR4nO2d2WpUURBF95GOKDibIA65jfOAov//FYrikETDue1ASJwFxYjlS90fqO4n91rvi0rCol66crpF9BDYMpMktflGyY6+L0mtDTdKeow76Z8r+p/Sv1X0t9JfK/qHkqQ2P1LxFf1vzh+K88ecf7Q4/3ftB4f/BgIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAnMbHwd6wAcyZ7gE2S3b0RfrHi/7P9C8X/XeS1NowK+kx/sn5J4vzv6d/t+i/kKTWhtLfP2JcpF++x2ADmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHOme4DS/6cr+ihJrQ2nS3qMX3N+6f/7FX0r/fWifyAt/z7ACn7/sxVf0T/n/CvF+W/ZAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc6R7gVMmO/i39E0X/hyS1NsxLeow9/dL7AhHj9L7A1aK/K2npe4bWhjvF+S9zfvmegw1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzJnuAS6U7Oh76W8U/f30Lxb9D+nfLvqv0l/2fYLS9xUo+vR9BeeL/sf0S/cMir7LBjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz2ADmTPcApe+fV/Sd9MufR6d/v+g/k5Z/X0Btfq04/03Of1ic/yTnHyvO/5XzS/ccEeMeG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOYM5Ok1obrFTlifC1JavPSe/eKPr13f6nov5ek1obNkh7jIv2bRX9b0ireByjNV/RtSWpteFDSY3zKBjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz2ADmTPcA9ypyxPhc0ire239U9B+nf6bof0m/Ff1If63oH0rLv2/Q2rBe9A/YAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2DOP0dk/daRPyk+AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CC8937BBE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABkUlEQVR4nO3awQ3CQBAEwT1EHJB/WJDIkYOFBHZX/U/+tPbjWXu/9pB1n5mZ9Tz2er/G+3O/vx17yVUIIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNA3PI7uM0FiLMHiL93AeIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgzh4gzgWIsweYmbUex57v91e+bw/AzwggTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcPUCcCxBnD/CF92feE7gAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBBnDxDnAsT9xR7gzP/Tz/7eBYgTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOHuAOBcg7i/2APYE9gD8iADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcTZA8S5AHGX2AN4bw/AQQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBBnDxDnAsTZA8TfuwBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEGcPEPcB7Emn1h2zBaIAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CC8937BBE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADGElEQVR4nO2dyWoVURQAz9Vs4oARNcGpnyjO6ELw/z9AcKE4o9htBhIVE6Jmoxw3t3/gvKysqn1xX0JxNn36dsscMwTLSkREtMWVkp3jdkREa8OipOc0dv9s0T/s/o2i/zkiItriZMWPHP92f7XoH0VEtDZcLek5bfXzS/+/yPHwREmU/wYDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAaT4OZuMEgDPvA9wq2Tl+7P5a0d/v/s2i/6n7K0X/T/cvFv1v3X9U9F9GRLQ2rJf0nPa6f63obzoB4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACO+wBwnABwjmUfoLWh9H585nTUz39YPP9V90vv10eOWxHHcj/Bsn//pYofOX7t528Uz991AsAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHPcB4DgB4Mz7AEs9j462uFD0v0dEtDaUvleQOc3fKyj9/sxpfp6+7H39pe8lRI7z9xJK+xiZ03w/Q/l+BScAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcBxHwCOEwDOvA9Qum8+ctzsful5fuS43f3zRf9H9x8X/Rfdf1D0X3f/VNH/3f3LRX+n+/eK/lsnABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAcR8AjhMAzrwPUH6e3P07Rf99958U/ecRx/J+/f3i+W/6+XeL57/r558rnn/Qz18rnr/vBIBjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOO4DwHECwFmJiGhtuF6RM6cvEXEc7+cve9/+eknPaa/7Q9GfIiKiLc5U/MjxZ/eX2sdobbhd0nP64ASAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADjuA8BxAsCZ9wHKz5MjYun366Mtnhb9Z93fKPq73V8t+kfdP130f0VEtDaUvheQOe10v3S/QOZ04ASAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADjuA8D5B4dP+9ZVfiSQAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Render model before the training\n",
    "print(\"Q_matrix of initial state, before training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])\n",
    "\n",
    "# (Action, Reward) container for each action\n",
    "history = []\n",
    "\n",
    "# Populating the history with every possible move\n",
    "for i_action in range(65):\n",
    "    # Reset the board\n",
    "    board.reset()\n",
    "\n",
    "    # Predict an action\n",
    "    a = i_action\n",
    "    encoded_a = encode_action(a)\n",
    "\n",
    "    # Perform the action in the game\n",
    "    s, signal = board.step(encoded_a)\n",
    "\n",
    "    # Process the signal\n",
    "    r = reward_from_signal(signal, board)\n",
    "\n",
    "    # Add the record to history\n",
    "    history.append((a, r))\n",
    "\n",
    "# Q Learning matrix for the initial state\n",
    "target_q = torch.zeros((65,), dtype=torch.float)\n",
    "\n",
    "# Add the reward and action for each game in history\n",
    "# NOTE: ça overwrite si y'a plusieurs valeurs\n",
    "for (a, r) in history:\n",
    "    target_q[a] = r\n",
    "\n",
    "print(\"Target Q matrix:\")\n",
    "render_q(target_q.tolist())\n",
    "\n",
    "# Toggle on train mode\n",
    "model.train()\n",
    "\n",
    "for i_batch in range(1):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "    loss = criterion(torch.tensor(target_q, dtype=torch.float), q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Render model after the training\n",
    "print(f\"Q_matrix of initial state, after training:\")\n",
    "q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Troisième étape : à combien de coup est-ce que l'agent \"comprend\" le premier état ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80A30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZ0lEQVR4nO3ZsQ3DMAwAQcrIHt5/LE8iF9kgCKDi73oCLB5suPZ+9pB1nV6AswQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNA3GdmZtb92/R+/rgKJ7gAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBD3/Qb66mW9CTIIwvpxAjwAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJUlEQVR4nO2dy2oWURAGv9aAF1SiYiSgM5rEG6jv/xwqaMxFzygEI8agEiMY2oVnXqDHlV/VvmgCRW9O55/IbCmwZUWSFONWyc622/0nRf+1JEUMF0t6Tqd9/oPi/J0+f6M4f7/PXyvOP+zzV4vzj/v8C8X5v86VRPhvIABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMCZ6DvWEDmLMi/ZP3+Dul6dk+9vkrxfm/+/x7xfnvuz8U/an760X/oPvni/6ZJEUMz0t6Ti/YAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc+R6g9B6dOZ1JkmK8Wpqe7XufX/r//sxpp89f+p6+6B4iYtgs+nvdj6L/d3vHeKPiK9sRG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOYM38v4HbJzvZJWn5PEDFcK/rfJEkxPqz4yva2+6X5yjbPH4t+6/6jor8tSRHD5ZKe0wkbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5gz3wPcKtnZPktSxPC4pOf0ps9f+h5eeo/PnOb3+K3i/N3ul97jle2k+0+L/itJihjulvScPrABzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDnz9wJWK3LmdNz9Re/xS38fIGK4WfS/dH+96B90/1nRf9n90j1E5rQtSYqxdA+gbNwDuEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHPme4Cl7+GbRX9PkhTjlYqvbD+6X/pegbLN3ysozc+c5vmlv1/Z9vr88u/9d3+t6B+yAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhHsAcNoA58z3ARkXOnPYlLf69e8V4o+gfSVLEcKmk5/Sz+xeL/mn3F90TRAz3i/47SVKM1yu+sn1lA5hDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPYA5fwCWIAXlvrxrcgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 11\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80280>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZwQmAMBAAwSj2Yf9lWUnsQYUIO/MPucdyn9vmvOYga189AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4C4Y/UAjDG289m7eb3+2gaIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDjXwD/44Kr3lA0QJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADibv5cCMJngdmBAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 11\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80A30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADKklEQVR4nO2dwU4VURAFz1VAwEQRNRrReRgX/v/nuDBwr2AwKqCJgIC2C3t+oMeVp2pf6fdIpTevh2kRPQS2rEmS2upFyY7+Mf03Rf+9JLU2rZf0GDc5f9Hnb216Vpz/KefvFuef5vzt4vyLnL9WnH97pyTCfwMBmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYE7j52Bv2ADmrElSa9NGRY4Y15KktnpZmh79KOeXQowYv3P+XnH+cfpL7yGeFv3P6d8r+j8lqbXpeUmPccIGMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId7AHPYAObM9wBLf4/fKk2Pfpnzlz6fX34+PucvuodobXpS9L+k34r+3+3dVo8qvqKfsQHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOfP7AnZKdvRzSWptulvSY/xKv/R7fsS4lSS11X7FV/TD9DeL/lX6pefzFf0k/f2ifyhJrU2lzx8xrtgA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5nAPYA4bwJz5HuBhyY7+TZJam0rvC4gYRzn/VXH+h5xfej4+Ypzl/NfF+Qfprxf9m/Snoj8kqbVpu6THuGADmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHPm9wXcr8gR40f6S//ffy3E6L9zfumeIWLM9wwPiv739Ev3BBHjIP29on8sSWqrxxVf0b+yAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhHsAcNoA58z3AbkWOGKfpL70H2Kr4in6Zful9BYo+v69g0fsO1Fal76/o/+oeovT3ixiXbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gznwPsFORI8a5JKmt3pamR3+Xfun5fEWfn8/fLOkxrtLfKPrXkqS2Wqv4in6b85feU+wU55+zAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhHsCcP8UnAuUw+GMZAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 21\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQmAQAwAQRX7sP+yrOTs4RBO3Jl/II8ln+xj3GMj61i9AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAuHP1Ar+wX3Nz4353jwkuQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHE+Qa+4QNfvVkuQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIewAMtwjCk4jCzAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 21\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADB0lEQVR4nO3d3W4dRRBF4d3EkIAJhBgsfqRzxPs/FJqRCMjEibExBmNT3PS8QI0lpKz13ZcqiVb6ZnrOjKqlIqyTJMk4nrWma7mc8z80539OkjEOJ63xWh/m/vPm/ou5/1Vz/9Xc/7K5/2bu/6S5/37uf9bc//hRa1AfDAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAuOHjYDZPALiTJBnj0Aqhav03STKO37e21/Jm7h/N/TX3f9Pc/9sTzb9uzr+b85825++SZIzDi9Z4rX95AsAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcB5HwDOEwBuuw+w93n889b2Wv6e+79o7r9+ov27fp9gjMNpc/52zu/99/+yM59afvcEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+AJwnANz2vYDW8/jUcp0kYxxav1dftT7O/a3n8all+17Ad835X+b8rvsEe57Hz/lvm/O/Jvu+N+AJAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd9ADhPALjtPsDL1nQtN0kyxuHr1nitb+f+8+b+i7m/9Xv7Vevd3H9o7l/nfO8/Ui3b9xb2fq+gfZ/CEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO+wBwngBw2/cCWu/HV63b7+1/1Zx/nyQZx88786nlj7m/NV+1bvPt9+vnfOv9/qp1e7//rDl/mWTXfQ5PADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADjvA8B5AsBt9wFaz5Or1u33AT5rzv+ZJBnH1vP41HI/5/e+n9/63kFq2b53sPd7Af/b398TAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM77AHCeAHDbfYCPO8NV6z9JknH8sbW9lp/m/Glz/jZ5kvf7W7+3X7U+JHmKP/+r5v6rub81n1quPAHgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgvA8A9x+cqv7WH6LGCAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 31\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80A30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQnDQAwAQV1wH+m/LFdy7sEELrAzf4Eeiz5ae997yPqcXoCzBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQNx1eoGZmVnfd3P7/u0eQS5AnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcT9xzfQV+8YFyBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxD2kKwjCkdUJ6QAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 31\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACz0lEQVR4nO3dTW7UQBBA4WqI+AuEgNiBPPc/li3YIUhCQkIEU2zKF6geCYn3vn2pouipN+6xR+aaIayziIgYh4vWdK43Nf+xOf8lImKMZbTGc8va/765/1vtP2/uv6v9r5r7f9b+s+b+37X/SXP/sTeo/4YBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZANzwcTCbJwDcfh+g9Tw+9tNjHD415z9H/Pv7ADEO75rz32v+sjl/VfOz9wna/z9PADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADjvA8B5AsCd6j7As+b8Y0TEGEtrPnN7rP0vmvsfav/T5v4/NT/195/gPsTrznzkeusJAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd9ADhPALiTfC9gjKUVUuZ2rP1Tv4+PcfjQnP9a81P3GWIcXjbn72t+9nsHvh9APQYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4HgPMEgNvvA7R/Xx4RMcbytjWe23Xtv2zuv6r9U7/vn/3ewQner9D6/0Wu+//P9wOoxwDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzvsAcJ4AcGcRJ3lffut5dOZ2GxHTz8PHWFrfC8jcHmr/1PP8MZbW7/szt/33/W+a8z8iYup7CZ4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4HgPMEgNvvA7Ted5+53dd8+331ERExDs8785Hrr5rvhZzr/r2C2f2z3ztoP8+v+fb3DjwB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4LwPAOcJALd/L2D2efrSnN9qvnUfIXLd7yO0/v7M7Vjzs98bmP3ewnlz/13tv2juv/EEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+ANxfRLn61gg9hycAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 41\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQmAQAwAQRX7sP+yrOTs4RBO3Jl/II8ln+xj3GMj61i9AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngLhz9QK/sF9zc+N+d48JLkCcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxPkGvuEDX71ZLkCcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIewDSOwjCA3bmSwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 41\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACyElEQVR4nO3dy24TQRBA0WowIbxCEBsUpPH/f9aMRMQGkQcQYwjFpvsHqi0hce/Zlzqybnoz5XHLXDOEtYuIiLZ/WZrO9Uef/1Cc/9znnxbnH/v8m+L8fUREa8t5aTy3Qz//efH8n/38J8Xz//Tzy59f6WD9PwwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4JqPg9m8AeDGPkAthFzH8+ir4vx1RERrSyuN55b9/Ivi+Xcnmr8szt/0+dl9jLPi/NEbAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM59ADhvALixD1B6Hh/j9mj7XXH+94nOf1Gcf4iY/35+a0vp+/mZ22Ofn92HeF2Zj1y/eQPAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAuQ8A5w0AN/YBpt63f4Ln+e+K81/7/GVx/qbPz+4zTP1eQLT92+L8bZ/39wJUYwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA5z4AnDcA3NgHmHpffWtLaT5zG++7L3+/vc/P/t7Bx+L8pz4/uw8x+3sB5X0KbwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA49wHgvAHgxj5A+fvlERGtLaX39WduD/3898Xzv/Tzz4rnH/v5pfnI9djPL71fIXO77/Ozn9+zynzk+ssbAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM59ADhvALhdxL9/nn6C9+3Pvh/gVXH+e5+f/ftL+wCR69gHOC/OH7wB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4NwHgPMGgBvvB5h9Hn1VnL/u87PP06fe19/aUvpHyNzGPsFF8fy7fn7peX7mdpg93xsAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzn0AuL8MGfHW05uuxwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 51\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQmAQAwAwVPsw/7LspKzBxFO3Jl/II8ln2xzXnOQta9egLUEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQNyxeoFP2M5nc/N6d48FXIA4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiPMNHOMXX72nXIA4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQdwN5oAjCllmO0wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 51\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACuklEQVR4nO3dPW4UQRBA4WpYbGyMJQILSMb3P9ZOYhABkjF/XqCdVF+gyhHvfXmpETw6mZrZMedxhrAOERExbs9L0/P4O+ffF+c/5/zL4vzfnH9TnP8eETHG9qo0PvdTnn9WPP8x518U5//l/KE4/6d2sP4bBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBww8fBbN4AcGsfoPs8/mNx/i4iYoxtlMbnPvP8q+L5Dznf2ieIcfuuOP815y+L8z9yvryP4A0AZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA5z4AnDcA3HPtA3Tny++353zrefoYW+k/wpz7ej+/tM8Q6/btz5f3GbwB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4NwHgPMGgFv7AN3347vPs7u/N/C2OP8t57u/l9Ddh7guzt/nfOn3DmIeT94AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcO4DwHkDwK19gO779a9L43P/ledfFM//mfPd5/HdfYTuPkR3H+GmOP/FGwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOfQA4bwC4tQ/Q+l7/GFvpefac+3qe3XoeP8ZWej9+zv2U558Vz3/M80v7FHPua5+i+/dX/vfzBoAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4BzHwDOGwDuEPEsz9O73+svfV8g5nF9X6D7fn73z1/+Xn/Ot77P0Pm9B28AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOPcB4LwB4Nb3AcrPk3P+Q3H+U853n6e3fi9gjK20TzDnvvYJrovn3+f53X2Mq+L5D94AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcO4DwD0BnWju1pss5UYAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 61\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZUlEQVR4nO3ZwQnDQAwAQdukj/Rf1lUi93AEDrIzf4Eeiz66Z9ZcZD2nF+AsAcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBD3Ob0AP3B/9+ZmuQB1AogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEOcb+A9mbY+6AHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBZFzCMJ0w+nmAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 61\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACsUlEQVR4nO3dOY4UQRBA0UhodjCAGYQwau5/rCoDIWYAg30NnMwLRLSExP/PD6XUfNKpqJqRuWcI6xQREePqTmk6959z/rI4fz3nbxfnf8/5h8X5LxERY2yn0ngev+b53d+vdH7kvs4vz98qDeq/YQBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAN3wczOYNAHeufYCL4vxNRMQY2yiN55Hz/MfF8z/N+dY+QYyr58X5d3P+fnH+25x/UJz/6g0AZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA5z4AnDcA3NoH6L6f3n2//15x/vucf1Sc/xwRMcZW+o+QefyZ53d/v+4+hvsAqjEAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgHMfAM4bAG7tA7Sep59hH+BVcf71nO++3999P7/0fYNYt2//9y/vU3gDwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAOcNALf2AVrP08fY7pbG8/gxzy/NR+5rvvt+/ovi/Ns5392H6H4f4GVx/o03AJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJz7AHDeAHBn+XsBY2yl+cxjPY+/LJ5/fabzW3+vYIytNJ95rPnSPkDmsfYByvsI3gBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPAeQPAnSIixthKz5Mzj/V++7PS6bm/n/P/+nv9T4rzH+d89/sA3b93UJ73BoAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4BzHwDOGwBufR+g+zz8ojh/M+e73/vvft+gtE+Qeax9gqfF8z/M87vfNyj/+3kDwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAPcXm6ju1s99qaoAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 71\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZklEQVR4nO3ZwQ2AIBAAQTH2Yf9lUQn2YEiM2Zk/HI8NnxtrzXWQdX79AL4lgDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOKuLbeM+925NbeM5z0/QJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHE7dkG2ur91gMRIwjC79S04AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 71\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACsUlEQVR4nO3d22oUURBG4b3NGA8RQTwQEDrv/1jdIIgHAmI0B2N5U/0CVX3lWt/9z8yExb6ZnZ4ZscYQ1mmMMca8OpXWsf7J/dvi/nvunxb3D7l/Udz/HmOMOZez0jy2x3z97t/vvLi/z3357/ekNNR/wwDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAbvp1MJsnANx+H+B5aR3rbe7fFfffcj+L+8h96z7AmFfPivu73F8W959z/7K4/5X78uf3BIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPgCcJwDcUc8HqIUU69/cd+8jXBT3N2OMMedS+vwR21Gfv/t8gPJ9Bk8AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOO8DwHkCwO33Abr/X9/9Pvxjcf8p993337qPMOdSel5/xLb/3kH3PkR57wkAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA530AOE8AuP0+QOt59Qf8f/1Z8fUfc999vsH74v5r7kv3AUas+32A7vsv/16BJwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc9wHgPAHg9vsA5efN534W95H71vfxcy6lkCO2Q36voPt8gDmX0n2IiG2/D1F+PoMnAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJz3AeA8AeBOYxzyffSH0qvH+iX358X9/UH7V8X9z9x3fy+h+3sHF8X9jScAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPcB4DwB4PbnA7wurWP9kfs3xf117rvPy2/t51xKzzeI2PbnG7Q+/wH3Mcr3GTwB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4LwPAPcP/l/t1qi6+g4AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 81\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZwQmAMBAAwSj2Yf9lWUnsQYUIO/MPucdyn9vmvOYga189AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4C4Y/UAjDG289m7eb3+2gaIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDjXwD/44Kr3lA0QJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADibv5cCMJngdmBAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 81\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACj0lEQVR4nO3dzWoUURBA4bo6Go0uohBEApP3f6xuCCKCZhH/Jyk3t1+gqmHAc7590Znh5G66pntkLhnCOkRExLh9UZrO5e+cvyrO3+90/VfF+V8REWMcn5XGc32a139evP7jnO9+/vJ86YPr/2EAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGADe8HczmCQC37QO8Lk3n8nPOvyvOf5vzozifc761DxDj9mVx/s+c/1ic/zTnu9//ZXH+hycAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPsAcJ4AcHs9H6B7P798P3vOt+6nj3Es/b4/c91+3999PsDZ9hk8AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAODcB4DzBIDb9gG696MPxfnTnL8pzt/N+Yvi/O8539pHGONY2qfIXHd530Fn3hMAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzn0AOE8AuL3eF9B9PkB3vvt8g+vi/Jedrt99vsCH4vxnTwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA49wHgPAHg9no+wNnuZ0dEjHEshZy5Ps3rd983UHo+QuZ6mvPdv7+8T+EJAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOc+AJwnANwhYpf70ef+fX33fQFvi/MPc772j5TL9v11r/+mOP/dEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOfQA4TwC47fkAV6XpXO53mi/9vj5yOc357j5B930F74vzXyN22cdwH0A1BgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPA/QNfYevWbaY21wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 91\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQnDQAwAQV1wH+m/LFdy7sEELrAzf4Eeiz5ae997yPqcXoCzBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNA3HV6gZmZWd93c/v+7R5BLkCcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxP3HN9BX7xgXIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxD0vUQjCENxblAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 91\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E460>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACrUlEQVR4nO3dS47UMBRGYRuKZ6slEM0ABsX+l5UMYAAIpFbTvIrLxN7A70hInPPNr5IqHXliJ+lVWzVhnVprrfV3j6Lp2n6N+etw/vag6z8N57+31lrv5wfReO1/xvUfhte/jPlTOP97zD8J539EP1z/DwOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAuO52MJsrANw8D/Asmq7tfsy/DOe/jPkezteYj/fDx/zjcP7nmH8Tzn8Y86v//1U4f+cKAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd5ADhXALh5HmB1PzwLqbb5fH28nz3mV98PED2fX7XP5/NXn+9f/f/j3+8KAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOd5ADhXALij3g+w+r78t+H8+zG/dB5g9ff3fo6+d1C1z+8dPA+v/23Mx/fvCgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDneQA4VwC4eR5g9X37S8/H936OvhdQtc/vBUT78a22uR//Opz/eND1V9+vEN+/KwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc5wHgXAHgjvpewD/dj+/9HIVctc/99KXn83s/R+9HqNovY371/qPzFK22cgWAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOA8zwAnCsA3Km1Q57Pv4muXtunMb96HmH1ewHX4fztmI/348f8VTh/tzrvCgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDneQA4VwC4+X6AF9F0bV8Pmo+er2+1Xcb86vsJVvfzX4Xzn1s75DxGfJ7BFQDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADO8wBwfwHotOzW0hTfggAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 101\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABbElEQVR4nO3ZwQnDQAwAwVxIHe6/raQRpQYbw4XszF+gx6KP1sx7HmQ9dy/AXgKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIO61e4F/sNZxaW7mc/Mm57kAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBDnG3iDX/jqXeUCxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAuC8l5ArCtNTzMgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 101\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACkElEQVR4nO3dz2pUQRBG8W4d/6MkATcKk/d/rBnQjaBBMdEk2m7qvsBXd+U5v30xw3Doza3bM9c6rSGswxhjjHn9Mppep181/yac/1nzz8L5h5p/Ec7/HmOMOY8zGl/nVZ//JPz8vzV/COcfa/5VOH+XfXH9NwwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4KaPg9k8AeC2fYD4eXLNX4bz32o+eh4/ttNrXj8P5+93mv8Qzn+u+e7v7z6AMgYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcO4DwHkCwG37AN3n4d33+1+H87c137qfYM7j02h8nf/U53d/v+79DPG8JwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc+wBwngBw2z5A6779He7L/xjOf6r57v8VtPYR5jxG7+evdd7e7+/uQ3g/gDIGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5AsDtdT9Aa36H9/O79+1fhfNfa/4Qzj/WfPf/EuLv7wkAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA5z4AnCcA3F73A3Sfh78P57+MMcacx+h5+lrn7Xl6a5+gu8+ww/eP9wk8AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAODcB4DzBIDb9gG69/23nufvcD9B936At+H8j5rvvt/f/f7+X4AyBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPAeQLAbfsA76Lpdfpe8xfh/E3Nd+8XaM3v8H7+Rfj5NzXf3ceIP98TAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM59ALh/hHHr1ozcut8AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 111\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZ0lEQVR4nO3ZwQmAQAwAQRX7sP+yrOTs4RAO3Zl/II8ln+xj3GMj61i9AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4C4c/UCr9ivublxv7vHB7kAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBD3j2+gr940FyBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQ9N0IIwlw/AOQAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 111\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACnklEQVR4nO3dOXLcQAxAUbQt75vswMqo+x+LzOTAu7xbcNJ9AWCqpsr/vxxFLX86IYYcmXuGsC4iImJcPylN5/59zj8rzt/O+YfF+V9z/nFx/kdExBjbKI3nkfP694rXv5vzF8X5P3O+/P+r/eD6bxgAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwA1vB7N5AsCtfYDu/fzL4vzHiJPcj39UvP7PE81fFedv5nx3H8N9ANUYAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcC5DwDnCQC39gEelKZz/z3nW9/Pj3H9vDj/dc637qePsZV+/8xj/f7d5xuc7fkIngBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPAeQLArX2A7vfju8+7f1ucfzfnu/sET4vz3yIixthKz1fIPNbzFbrPB3AfQDUGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5AsCtfYCzfr99jO1+aTyPv/P63ecTvC7Of5jz3X2I0vsSYp3ejZ/fEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOfQA4TwC4tQ/QvZ/efd/AVXH+JiJijK10Pz3zWPfTu+8bKH2QMo+7ef3aBzH3NV/eJ/AEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgHMfAM4TAG7tA3TvR78pzr+f8919gu4+w8vi/Oc53/1+f+t9Bb4vQGUGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5AsCtfYAXpencv8z5V8X5T3O+uw/Qel7/CZ4vcO59iPLf3xMAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzn0AuH/MqOvW72zv7wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 121\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZwQmAMBAAwSj2Yf9lWUnsQYUIO/MPucdyn9vmvOYga189AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngLhj9QCMMbbz2bt5vf7aBogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAONfAP/jgqveUDRAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADibtosCMIDENMTAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 121\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACnElEQVR4nO3dO3IUQRAG4S60IF4CBWBgzd7/WDsWBhACPZBAonFqLlA1Fpmf/8eAIredaWljztMcwjqMMcaI48vSep7uc/+quP+V+xfF/e89nh+xRGk+15nPL+3H9uGL46G4f8x9+f//rDTUf8MA4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAC58HczmCQC33Qfovs+/LO6vxtjlfXz3PsN5cf+Q+8/F/Zfcd//95b0nAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJz3AeA8AeC2+wDd38/vvk9/U9zf5r71Pj1iOSvN5/qUz+/+/Lp77wOoxgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzvsAcJ4AcHvdB3he3P/J/afi/mvuXxf3d3vsI5aL0nyu1/n8t8Xn3+Te7wtQjQHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPcB4DwB4Lb7AN3f72/tI5ZDaT7Xx3x+9/sOPhT333PfvQ9R+r6EsZ3ecfxY3H/zBIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPgCcJwDcdh+g+/313b8v0Pr7ABFL6X36nOv2Pr11nyBiKX2Q5lz/5vNrH8R5au89AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOC8DwDnCQC33Qfovo++LO6vct+9T9C9z/C+uP+R++7v93e/L6D88/MEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+AJwnANx2H+CitJ6n69y/K+5/5v6suH/Kfes+wQ5/X6B7n6B7H6L88/cEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+ANw/y8jr1iKJshIAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 131\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaUlEQVR4nO3ZwQmAQAwAQRX7sP+yrOTs4RBO3Jl/II8ln+xj3GMj61i9AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngLhz9QK/sF9zc+N+d48JLkCcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxPkGvuEDX71ZLkCcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIewDSOwjCA3bmSwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 131\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACnklEQVR4nO3dzU4VQRBA4W4FUUBBw8LV3Pd/rJmVCwMI/iFgu6l+gapJSDzn21f6hhx6M3Xn9jHW0YR11FprrR9OUtNjfYj50+T8z5h/k5z/s8f5vS89NT62Eeen5tv85+uH4+T8Y8y/S87/epUa1H/DAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDACu+ziYzRsAbu4DpJ8nx/xlcv62tV2ex1f3Car7EJ+T819ivvr53ybnf3sDwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAOcNADf3AV7seXTMnyfnv+9xfu/LUWp8bE9xfmq+jXXOV//+6XlvADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADj3AeC8AeDmPkD1ffWvk/PPMf8pOX8d89XfKyjtI/S+vE+Nj+0+zr9Mnn8b874fQDkGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5A8Dt9X6A0nzvS2p+jG2e/6K/d7DDPkXq9xLavL0L+xTeAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5A8DNfYDq9+ur+wRXyfmvMV99nl7aJ+h9SZ0/xjbPr/7eQPr9DN4AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcO4DwHkDwM19gFwIY/0b8x+T8zcxX/1+/Vly/kfMp97338Y63/df/fuVfq+gso/hDQBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgDnPgCcNwDc3AeoPo+uPk9Pf7895k+S8w+t7fL9/ovk+d9ivvT5Wz98SM7feQPAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAuQ8A9w+UVerW8MJ8qQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 141\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZUlEQVR4nO3ZwQnDQAwAQdukj/Rf1lUi93AEDrIzf4Eeiz66Z9ZcZD2nF+AsAcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBD3Ob0AP3B/9+ZmuQB1AogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEOcb+A9mbY+6AHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBZFzCMJ0w+nmAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 141\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACkElEQVR4nO3dOU4tMRBAUZt5HoQISB77X9brhAAh5hlkkuoNVHXEvScvtf77Fyd2u/sY69GEtdVaa61f76amx/oz5g+T868xv5Oc/4r5o+T8S2ut9b7qqfExjXh+ar7Nf3z9eis5/xPz+8n5943UoP4NA4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwC47nYwmysA3Hwe4CA1PdZvMX+WnH+I+ep+evU8Q3o/PeavkvM3Mb+XnP+Iec8DKMcA4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM7zAHCuAHBL3Q9Q3U+v3i9Q2k/vfZV6P3+MaX4/fzv5/O+Yr96PkJ53BYAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4DzPACcKwDcfB6gup9dve/+PDl/H/PV+w2q3xs4SY2P6Smef5x8/nPMp89DuALAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAeR4AzhUAbqnzAKX53lep+TGm+fnV8wCnyfnHmK/+ftXvJVwk5+9cAeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOA8DwDnCgA3nwcovR+/wPcGLpPztzFf3U8vfe+g91Xq+WNM8/Or9ytsJud/XQHgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgPA8A5woAN58HSO8nx3z1vv/q+/XV+/ar5yGqv99hcv415tO/nysAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnOcB4FwB4Ja6H6C6H199P34vOf/R2iLv91f//aX7CSr/f64AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcJ4HgPsD2Cvp1jNuXloAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 151\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZklEQVR4nO3ZywmAMBBAQSP2Yf9lpZLYgwRE3sw9n8NjLzvWmusg6/z6A3xLAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcRdW24Z97tza255nvdMgDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKI27MNtNX7LRMgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxD2ydwjC9Rsj1AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 151\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACm0lEQVR4nO3dTU4bQRBA4e7g8BcnSGEZaXz/Y81ILBOJhEAMRM2m+gJVllDy3rcvjbAevZlyu4+xjiasXWuttX64SE2P9Rjzl8n5PzF/npx/jvl9cv6htdZ6X3pqfGwjnp+ab/Ofrx/OkvN/Y/4qOf/0ITWo/4YBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAFz3dTCbJwDc3AdIv0+O+Zvk/H1rJ3kfX90n+JSc/x3z35LzdzFf3adwH0A5BgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPAeQLAzX2A6vvo0vfzT7CPULrfoPdllxof22s8PzXfxjrnq/sM6b/fEwDOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOfQA4TwC4uQ/wMTU91peYr953f5uc/x7z18n5x5gv3Q/Q+5K6H2GM7T6e/zn5/F8x7/0AyjEAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgHMfAM4TAG7uA1S/3166X6D3JfX99jG2Yzy/et//l+T8z5ivfn6p30to8/Tuh6/J+R+eAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5AsDNfYDq9+ur9wtU7weovk8v/V5B70vq+WNs8/nvdj+DJwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc+wBwngBwp7ofIHVffhvrvC//PDn/HPP75PxDzFfv66/+XkJpH6Hy+XkCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAOcJADf3Af719+nV3yuofr+/+nsD7/b5ewLAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAuQ8A9waXLerW+D6wewAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 161\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZwQmAMBAAwSj2Yf9lWUnsQYUIO/MPucdyn9vmvOYga189AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAuGP1AIwxtvPZu3m9/toGiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A418A/+OCq95QNECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADibhSoCMJwKUY1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 161\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACkUlEQVR4nO3dQW4TQRBA0WqwSUiMMFsW9v2P5VmwjSMCODios+m+QNVIlvj/7UuTRD+96fK49X7qIaxNRES046fUdD/9HfP3yfnLmN8m569j/iE5/zsiorVDS433pY/np+Zj/vO14yY5/zbmd8n5lw+pQf03DADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgmtfBbJ4AcHMfIH2fPOa/JOd/RqxyH1/dZyjtE0Q7fk/O/xjzd8n51zGf3sfwBIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4BzHwDOEwBu7gN8Tk33058xX90nqD6/dJ/e2iH1foLel/l+gurn+6v7AOm/nycAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPsAcJ4AcGt9X8DH5Py/Mf8tOf805quf76++32CfGu/LeY3nuw+gNAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOPcB4DwB4OY+QPU+P/2++oiI1g6p+d6Xy3j+TfcBVng/QOr7EmKe3u24T86fPQHgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDg3AeA8wSAW+v7AqrvF/ianH+OiGjtkLpP732Z9+mlfYYV7vNv9n4GTwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA49wHgPAHg5j5A9fPtN3vf/Zh/TM7/GvOlfYQV3q9Q/fm3yfmrJwCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCc+wBwngBwcx+geh9d3Qeo3qdX9wmqn++v/v675PxL9fmeAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8C9A0MZ6dYidNHwAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 171\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZsQ3DMAwAQcrIHt5/LE8iF9kgCKDi73oCLB5suPZ+9pB1nV6AswQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0DcZ2Zm1v3b9H7+uAonuABxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEPf9BvrqZbkAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBeTzCMLsd79HAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 171\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACl0lEQVR4nO3dTU4bQRBA4WowEBJAZsnGvv+xPJssY2H+EjCdTfcFqpAs8d63Lw2gp950zdB63/UQ1ioiItr2KjXdd3/H/GVy/t+YP0/OH8f8dXL+NSKitU1Ljfelj+efJZ//OeYvkvPvY/4mOf+U+8H1bRgAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwDWvg9k8AeDmPkD6PnnM/0rOP4/51H18zNOrvo/wMzn/MuYfkvO/x3x1HyO9D+EJAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOc+AJwnANzcByi9Xx9te5ucP3zR838k598iIlrbpN7P732Z7+dX3++v7gOk9xk8AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAODcB4DzBICb+wDV9+ur99n3yfk/Y776fYPS9wFa29ylxvvyOJ6fmo++m/N+H0A5BgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw7gPAeQLAzX2A89R03x3HfPX9/NQ+Qu/L3Ec46T5AtO0qOf8x5qv/L2GdnN97AsAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcC5DwDnCQA39wFK9/lf8L38dXJ+HxHR2iZ1n977Mu/Tq79/9T6/+vz0PocnAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJz7AHCeAHBzH6D6fvup38+vzq+T8/sxX/2+QvXvl97H8ASAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAcx8AzhMAbu4DpP///Jiv3mdX79OrP/+p7/Nvk/OH6vM9AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAODcB4D7D4Pf6NbqOjfzAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 181\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABaElEQVR4nO3ZsQ3DMAwAQcrIHt5/LE8iF9kgCKDi73oCLB5suPZ+9pB1nV6AswQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0DcZ2Zm1v3b9H7+uAonuABxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEPf9BvrqZbkAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBZRJCMJyEKzeAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 181\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACk0lEQVR4nO3dOXIUQRBG4SqtaEN4eKP7H6vbw0MLWgApcbIvkNkRiuC9z/9jhuGpnC6NZsQSQ1gnY4wx5t15aR3L20774+L+PfeXxf3zGGPMeZileayRr999/93P76a4fzwqDfXfMAA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAmz4OZvMEgNvuA1yX1rE87bSvhRjLR+67z9Nb9wnGvPte3P/Ifff9XxT3L54AcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcN4HgPMEgNvuA3wprWN5zf3X4v4h993n8a33P+fhtDSP9U++/knx9f/mvvv5l/eeAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDeB4DzBIDb7gOUnoePWLbn4d3n2d+K+5+5/+z7BKXvR4hYt+9HKH/ff+69D6AaA4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA47wPAeQLAbfcBjkvrWN5z332eflaax/o7X7/7/QRXxf2v3LfuU8x5mKV5rJGvX/73ewLAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAeR8AzhMAbrsP8KnPw7vP83d4nt79foPaD1IsH7k/L+7fcl/+ewWeAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDeB4DzBIDb6+8FdH8//6K4f9lpf1vc3+e+/Dw+993Pr/z/5wkAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYA530AOE8AuO0+QPf34y+L++fcd/9eQfc+QPd5/k1x/5j77n2A6+L+yRMAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzvsAcP8ADhLp1n2oJGIAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 191\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZUlEQVR4nO3ZwQnDQAwAQdukj/Rf1lUi93AEDrIzf4Eeiz66Z9ZcZD2nF+AsAcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQ9zm9AD9wf/fmZrkAdQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBDnG/gPZm2PugBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBVcGCMJE3daZAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 191\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA90>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACfUlEQVR4nO3dOU4DQRBA0W5s9sUExPb9jzWOCdh30yQ1F6iyZIn/X14aCX066ZpxH2MaTVjL1lprfXOSmh7TV8yfJuc/Y36RnN/F/GVy/rW11npf99T42I54/jL5/J+Yr/79bpLzT0epQf0bBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw3etgNk8AuHkf4CI1Paa3mL9Kzr/EfOo+vs2nV32fobRP0PrmLjl/H/PVfYD0vCcAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnPsAcJ4AcPM+wMHeT4/56j7CWXL+o7XWel8fp8bH9juef+jvK7gPoBwDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADj3AeA8AeDmfYDq9+7Pk/PvMb9Kzj/u6fml+d7Xt6nxsX2I518nn/8c8+4DKMcA4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM59ADhPALh5H2CRmh7TLuar9+nV9/Or3yeo/t5B9fsA1d9LSO8TeALAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAuQ8A5wkAN+8DVN+vL32vfw/3+dX79Or3+nP/SGP6jfnqPkH6+w6eAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHDuA8B5AsDN+wDV+/DL5PxrzFf3Earv91f3Eap/v+rvBbgPoBwDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADj3AeA8AeD2tQ9Qvc8/6O8VVO7TY766D7FKzj/GfHqfwRMAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzn0AuD9MbOfWY7juwgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 201\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E80100>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZ0lEQVR4nO3ZwQmAQAwAQRX7sP+yrOTs4RAO3Zl/II8ln+xj3GMj61i9AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAuHP1Aq/Yr7m5cb+7xwe5AHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQ949voK/eNBcgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQ9/MYIwhiARhkAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 201\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACoklEQVR4nO3cW04bQRBA0W4wkBAeXsGw/2XNrMAkPBJjaH6qN1CFhJR7z39pbOmqf7pm+hjraMLatdZa6w8/UtNj/RvzV8n5fzF/npx/j/nr5PxLa631vvTU+NhGPH+XfP4p5n8m519j/iY5/3SWGtR/wwDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwArnsdzOYJADf3Aar30aX7+C/YB7hIzr/FfPo+Peb3yflDzFf3KS6T80dPADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADj3AeA8AeDmPkD6PjnmfyXnn2O++n2C0nzvS+r/j7HN/1+9z/+27zN4AsAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcC5DwDnCQA39wGq79dXvy9wn5x/jPnqfXrp9/e+3KXGx/Y7nn+bfP6fmE//f08AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOPcB4DwB4OY+QC6EsX7EfOn9+N6XXWp8bKd4fuo+vo113sdXv29Q/T5AT86PmE/vE3gCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAOcJADf3Aarv91ffz98n5w+ttdb7krpPH2Ob9+nXyee/xPy37lO0/nCenH/3BIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4BzHwDOEwBu7gNU76NvkvNPMV+9j68+f5+cP8T8ZXL+GPPV7xukvq/QxnryBIAzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4BzHwDOEwBu7gNU3++v3sen32+P+eo+Qfo+/YueX90HuE/OP3oCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwLkPAPcJg9/o1jjjPQ4AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 211\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZUlEQVR4nO3ZwQnDQAwAQdukj/Rf1lUi93AEDrIzf4Eeiz66Z9ZcZD2nF+AsAcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEPc5vQA/cH/35ma5AHUCiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQ5xv4D2Ztj7oAcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSDuBcvgCMLDZB/1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 211\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4E8B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACcklEQVR4nO3dWU4CQRRG4SrBAYeIG8D9L4vegBgHFCXly+0N3EtC4jnf+x+COamXLps+xnY0YS1ba63155vUemy/Yn+V3B9iv0juj7G/S+4/Wmut901Pzcc04vMvk5//E/vb5P6zur9IDfVvGACcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwBkAnAHAdR8Hs3kCwM33AarPo1fJ/T721efp1f06ud/F/im5f4n9dXL/Hfv0fQ5PADgDgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADjvA8B5AsDN9wGq/99/n9y/x756H6H0foPeN6nvP8Y0f//q+xWq+/R9Ak8AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOO8DwHkCwM33Ac79vvt1cr+LffV5evX3Btap+Zh2p/h87wMozQDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzvsAcJ4AcKe6D1B6333vm9TnjzHNn3/u9xNU3/ffk/sR+/T39wSAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOAOA8z4AnCcA3Hwf4Gzvq4/9Q3L/Fvvq8/RVcr+P/TK5/z3353sCwBkAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwHkfAM4TAG6+D1B9nl993331PkL1/QDV/VVyf4h99T5E+v0OngBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAGcAcAYAZwBw3geA8wSAO9V9gOrz9EVyf4x99f/rq7+XUP29gerf7zG5f/UEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgPM+ANwfTYTn1r7TcFQAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target Q matrix: step 221\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EE20>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAABZ0lEQVR4nO3ZwQmAMBQFQRX7sP+yrCT2EISgO3MPvMOSy9/HuMdG1rF6AGsJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngDgBxAkgTgBxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEECeAOAHECSBOAHECiBNAnADiBBAngLhz9YBX7Nfcu3G/u+OD/ABxAogTQJwA4gQQJ4A4AcQJIE4AcQKIE0CcAOIEEPePa6Cr3rQHIQUIwnQ2/dwAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q_matrix of initial state, after training: step 221\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1CCA4E4EA30>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAACkUlEQVR4nO3dQVIVQRAA0W4BAUEJLvC9/7FmLkCgICpIuam+QNWsyHz7ikEjozdT039GbDGEdT7GGGN+vypNx/Y75y+K8685/6k4/57zN8X55zHGmPM0S+OxRz7/svj8Pzl/XZx/6c7X/uP1YRgAnAHAGQCcAcAZAJwBwBkAnAHAGQCcAcAZAJwBwE1fB7N5AsCtfYDu++juPsHn4vzfnO/uI9wX5x9y/ltx/kfOd//9X4rzvzwB4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4NwHgPMEgFv7AN330bfF+aeD5lv7CHOeSt/3R+zr+/7uPkT3foHy8z0B4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4NwHgPMEgFv7AN330eXv03P+rjj/mPPd9/Gt+xHmPJXuB4jY1/0AX4vP/5nz7gOoxgDgDADOAOAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzn0AOE8AuKPuB2jtE8x5Oi+Nx/6Wz+/uI3Tnu/cTzNJ47JHP9/cCVGMAcAYAZwBwBgBnAHAGAGcAcAYAZwBwBgBnAHAGAOc+AJwnANzaB7gpTcf2nPPd+wW638eX3qePdfr1//6L4vxrznf3EUr7FCO2N08AOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgDMAOPcB4DwB4I76vYDuPkH3foLu87vv47v7AKXfGxixrd8bOCvO//MEgDMAOAOAMwA4A4AzADgDgDMAOAOAMwA4A4AzADgDgHMfAM4TAG7tA7Tuuz/g+/5aiLG95/x1cf4l57vv82+L808Hzd8V5x89AeAMAM4A4AwAzgDgDADOAOAMAM4A4AwAzgDgDADOAODcB4D7D9Ww6NYbUnEjAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "model = Model()\n",
    "learning_rate = 0.3\n",
    "batch_size = 32\n",
    "n_epochs = 256 # On aura pas joué tous les coups mais OK\n",
    "max_history = 64\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# (Action, Reward) container for each action\n",
    "history = []\n",
    "\n",
    "# Populating the history with every possible move\n",
    "for i_epoch in range(n_epochs):\n",
    "    # Reset the board\n",
    "    initial_s, _ = board.reset()\n",
    "    initial_s = torch.tensor(initial_s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Play a random action\n",
    "    a = torch.randint(0, 65, (1,)).item()\n",
    "    encoded_a = encode_action(a)\n",
    "\n",
    "    # Perform the action in the game\n",
    "    s, signal = board.step(encoded_a)\n",
    "\n",
    "    # Process the signal\n",
    "    r = reward_from_signal(signal, board)\n",
    "\n",
    "    # Add the record to history\n",
    "    history.append((a, r))\n",
    "\n",
    "    # Continue playing if we don't have enough moves to learn in the history\n",
    "    if len(history) <= batch_size:\n",
    "        continue\n",
    "\n",
    "    # Keep history the wanted size removing first element in list\n",
    "        if len(history) > max_history:\n",
    "            history.pop(0)\n",
    "\n",
    "    # Add the reward and action for each game in history\n",
    "    # NOTE: ça overwrite si y'a plusieurs valeurs, donc batch_size devrait être < 65\n",
    "    # TODO: moyenne des valeurs ? pas d'overwrite ?\n",
    "    targets = torch.zeros(batch_size, 65)\n",
    "    inputs = torch.zeros(batch_size, 2, 8, 8)\n",
    "    for i_batch, (a, r) in enumerate(random.sample(history, batch_size)):\n",
    "        # Q Learning matrix for the initial state\n",
    "        # NOTE: Je pense qu'il faudrait partir de la prédiction du modèle\n",
    "        # et y ajouter les rewards, les cases où il y a 0 peuvent peut-être\n",
    "        # poser problème\n",
    "        target_q = torch.zeros((65,), dtype=torch.float)#model(initial_s).view(-1)# torch.zeros((65,), dtype=torch.float)\n",
    "        target_q[a] = r\n",
    "        targets[i_batch] = target_q\n",
    "        inputs[i_batch] = initial_s\n",
    "\n",
    "    # Toggle on train mode\n",
    "    model.train()\n",
    "\n",
    "    # for i_batch in range(4):\n",
    "    # Compute loss on the difference between model output and target_q\n",
    "    inputs = torch.tensor(inputs)\n",
    "    targets = torch.tensor(targets)\n",
    "    q_pred = model(inputs)        \n",
    "    q_pred = torch.clip(q_pred, -3, 3)\n",
    "    loss = criterion(targets, q_pred)\n",
    "\n",
    "    # Let the optimizer do the backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i_epoch - batch_size) % 10 == 0:\n",
    "        print(f\"Target Q matrix: step {i_epoch - batch_size + 1}\")\n",
    "        render_q(target_q.tolist())\n",
    "\n",
    "        print(f\"Q_matrix of initial state, after training: step {i_epoch - batch_size + 1}\")\n",
    "        q_pred = model(torch.tensor(initial_s, dtype=torch.float))\n",
    "        render_q(q_pred.tolist()[0])"
   ]
  },
  {
   "source": [
    "## Etape 4 : mouvements futurs et epsilon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.9\n",
      "Epoch 1 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 2 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 3 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 4 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 5 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 6 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 7 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 8 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 9 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 10 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 11 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 12 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.89\n",
      "Epoch 13 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 14 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 15 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 16 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 17 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 18 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 19 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 20 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 21 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 22 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 23 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 24 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.88\n",
      "Epoch 25 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 26 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 27 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 28 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 29 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 30 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 31 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 32 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 33 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 34 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 35 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 36 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.87\n",
      "Epoch 37 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.86\n",
      "Epoch 38 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.86\n",
      "Epoch 39 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.86\n",
      "Epoch 40 \t reward: 1.5 \t length 2 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 41 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 42 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 43 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 44 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 45 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 46 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 47 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 48 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.86\n",
      "Epoch 49 \t reward: 1.5 \t length 2 \t mean_length 1.04 \t epsilon 0.86\n",
      "Epoch 50 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 51 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 52 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 53 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 54 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 55 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 56 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 57 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 58 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 59 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 60 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 61 \t reward: 1.5 \t length 2 \t mean_length 1.04 \t epsilon 0.85\n",
      "Epoch 62 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.85\n",
      "Epoch 63 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 64 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 65 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 66 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 67 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 68 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 69 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 70 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 71 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 72 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 73 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.84\n",
      "Epoch 74 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.84\n",
      "Epoch 75 \t reward: 1.5 \t length 2 \t mean_length 1.04 \t epsilon 0.83\n",
      "Epoch 76 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.83\n",
      "Epoch 77 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.83\n",
      "Epoch 78 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.83\n",
      "Epoch 79 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.83\n",
      "Epoch 80 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.83\n",
      "Epoch 81 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.83\n",
      "Epoch 82 \t reward: 1.5 \t length 2 \t mean_length 1.05 \t epsilon 0.83\n",
      "Epoch 83 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.83\n",
      "Epoch 84 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.83\n",
      "Epoch 85 \t reward: 1.5 \t length 2 \t mean_length 1.06 \t epsilon 0.83\n",
      "Epoch 86 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.83\n",
      "Epoch 87 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.83\n",
      "Epoch 88 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.83\n",
      "Epoch 89 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 90 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 91 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 92 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 93 \t reward: 1.5 \t length 2 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 94 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 95 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 96 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 97 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 98 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 99 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 100 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.82\n",
      "Epoch 101 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.82\n",
      "Epoch 102 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 103 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 104 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 105 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 106 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 107 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 108 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 109 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 110 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 111 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 112 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 113 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.81\n",
      "Epoch 114 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.81\n",
      "Epoch 115 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.8\n",
      "Epoch 116 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.8\n",
      "Epoch 117 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.8\n",
      "Epoch 118 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.8\n",
      "Epoch 119 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.8\n",
      "Epoch 120 \t reward: 1.5 \t length 2 \t mean_length 1.07 \t epsilon 0.8\n",
      "Epoch 121 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.8\n",
      "Epoch 122 \t reward: -0.5 \t length 1 \t mean_length 1.07 \t epsilon 0.8\n",
      "Epoch 123 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.8\n",
      "Epoch 124 \t reward: -0.5 \t length 1 \t mean_length 1.07 \t epsilon 0.8\n",
      "Epoch 125 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.8\n",
      "Epoch 126 \t reward: -0.5 \t length 1 \t mean_length 1.07 \t epsilon 0.8\n",
      "Epoch 127 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.8\n",
      "Epoch 128 \t reward: -0.5 \t length 1 \t mean_length 1.07 \t epsilon 0.8\n",
      "Epoch 129 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.79\n",
      "Q_matrix of initial state, after training: step 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671A60>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADWklEQVR4nO2dv4vPcRzHny9dYVKKcurzWbAoxSDTnVInk8HgIhJGgzoUxUBRfpTBiER0BoNJd6XcTTJQyoLl/SkUpUyYXpbX5x94jp7Px/7ouTx6Lff+fi4yW8LIMgEAER0VQeYQ5X8l/UkAQPSXGB/ZLtf+MXL/Qe1fJfcv1P48uT9b+z/J/bXlT5P+0gpKNP8NDkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiBP+c7A2vgDijO8Blhk5c5gqf470b5V/k/TPAACi/8z4yLap9mfJ/fnyX5D+PgBA9OcZH9mulX+H9E/5AojjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOH4PII4vgDgTAIDoqd/3I9skAER0i5Sew0z5A+l3AIDoHzM+sh0u/zvprweAiO4PpeewqvapeWRD7X8h9zf6AojjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOH4PII4vgDjj9wHWMXLm8AMAEP1Laj3bnvK3kf778j+R/mYAiOhuUHoOZ2uf+r4Aso3fF9hL7i+UT/+/Bl8AcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDH7wHE8QUQZ/w+wBXKznax/Kekf7D8naT/BgAiuluUnsNc+UukP13+VtL/AACI/gnjI9uh8neR/mtfAHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQx+8BxPEFEGf8PsARRs4cHgEAov9IrWfbUvucnvVvBqJv5H5fPvX7fGQbf5//i9JzWFP7M+T+YvnnSP+6L4A4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOP3AOL4Aogzfh9gnrKzzZa/gfS/AUBEt4PSc3hb+0fJ/Yfl7yb9V+Uvk/4UAER0zyg9hwO1v5Lc/+sLII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDi+D2AOL4A4ozfB9jOyJnDu/KPk/798leT/m8AQPT3GB/ZTpR/l/RPlr+f9J8DQER3m9JzOF3+Aunv9QUQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHH8HkCcf9Ee/9Yb80NpAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42\n",
      "Epoch 130 \t reward: -0.5 \t length 1 \t mean_length 1.07 \t epsilon 0.79\n",
      "Epoch 131 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 132 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.79\n",
      "Epoch 133 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 134 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.79\n",
      "Epoch 135 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 136 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.79\n",
      "Epoch 137 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 138 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.79\n",
      "Epoch 139 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 140 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.79\n",
      "Epoch 141 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.79\n",
      "Epoch 142 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.78\n",
      "Epoch 143 \t reward: 1.5 \t length 2 \t mean_length 1.06 \t epsilon 0.78\n",
      "Epoch 144 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.78\n",
      "Epoch 145 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.78\n",
      "Epoch 146 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.78\n",
      "Epoch 147 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.78\n",
      "Epoch 148 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.78\n",
      "Epoch 149 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.78\n",
      "Epoch 150 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.78\n",
      "Epoch 151 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.78\n",
      "Epoch 152 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.78\n",
      "Epoch 153 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.78\n",
      "Epoch 154 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.78\n",
      "Epoch 155 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.78\n",
      "Epoch 156 \t reward: 1.5 \t length 2 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 157 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 158 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 159 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 160 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 161 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 162 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 163 \t reward: 1.5 \t length 2 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 164 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 165 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 166 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 167 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 168 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.77\n",
      "Epoch 169 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.77\n",
      "Epoch 170 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 171 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 172 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 173 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 174 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 175 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 176 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 177 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 178 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 179 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 180 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 181 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.76\n",
      "Epoch 182 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 183 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.76\n",
      "Epoch 184 \t reward: 1.5 \t length 2 \t mean_length 1.05 \t epsilon 0.76\n",
      "Epoch 185 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 186 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 187 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 188 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 189 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 190 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 191 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 192 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 193 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 194 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 195 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 196 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 197 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.75\n",
      "Epoch 198 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.75\n",
      "Epoch 199 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 200 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 201 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 202 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 203 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 204 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 205 \t reward: 1.5 \t length 2 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 206 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 207 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 208 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 209 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 210 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 211 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 212 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.74\n",
      "Epoch 213 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.74\n",
      "Epoch 214 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.73\n",
      "Epoch 215 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.73\n",
      "Epoch 216 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.73\n",
      "Epoch 217 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.73\n",
      "Epoch 218 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.73\n",
      "Epoch 219 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.73\n",
      "Epoch 220 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.73\n",
      "Epoch 221 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.73\n",
      "Epoch 222 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.73\n",
      "Epoch 223 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.73\n",
      "Epoch 224 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.73\n",
      "Epoch 225 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.73\n",
      "Epoch 226 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.73\n",
      "Epoch 227 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.73\n",
      "Epoch 228 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.73\n",
      "Epoch 229 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Q_matrix of initial state, after training: step 101\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671A60>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADSElEQVR4nO2dsauOcRiG70eHbEox6X0lDMpASbEoJJJJLKazGKTOoJBOkoQyKBksZ7I4mSQSykJJMSgDkvfNRCmbHPkZPN8/8DC5r2u/ur/h6lm+X98XrQ1NYMuUJEV0CxW5tXFx+p+L/kpJUvSnK77acDH3Lxf3T+b+xeL+6dx/V9xfl/tfivsr0t9d9B8uKonw30AA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmBN8HewNF8CcyXuA0hVobYz0XxT9LenfKfoHJEnRDxVfbehzf19x/176H4v+aklS9JcqvtpwKv1rRf84F8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc3gOYwwUwZ0qSFP3bkt2G9ZIU0dX0Nir9n0V/8vlv1j7AcCT9M0X/giRFdD9KehuX5H5pXu3PM4i/+X0HLoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5vAcwhwtgzuT79CjZk+sR/d2ivz/97UX/afqviv4mSYronpX0Nm7L/WXF/W+5v6O4/yT98u87cAHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4T2AOVwAcybvAa6U7DacSP950d8qSRHdqpLexk/pPyj6e9L/2/9LWFv030uSop+v+GrDofQPFv3bXABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzeA9gDhfAnClJiuhmKnJr41VJ/+L/Br4X95fm/tXi/kz6u4r+I0mK6PaW9Dbez/2dxf3H6c8W/fNcAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHN4D2AOF8Ccye8DXC/ZbTiW/vKi/1WSIro1Jb2NH3L/cHH/Vvqbi/7L9B8W/d2SFNG9Lult3Jj7G4r7b7gA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5vAewBwugDmT3wc4VJFbG+fTP1v0z0mSoq+F2IZf6d8o+kfTP1/0Z9OfLvpzkhTRzZX0Nk6nv1D0F3MBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOE9gDm/Ab+U/tZOTizcAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17\n",
      "Epoch 230 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 231 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 232 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 233 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 234 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 235 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 236 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 237 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 238 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 239 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 240 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 241 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 242 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.72\n",
      "Epoch 243 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.72\n",
      "Epoch 244 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 245 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 246 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 247 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 248 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 249 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 250 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 251 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 252 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 253 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 254 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 255 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.71\n",
      "Epoch 256 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.71\n",
      "Epoch 257 \t reward: 1.5 \t length 2 \t mean_length 1.02 \t epsilon 0.71\n",
      "Epoch 258 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.71\n",
      "Epoch 259 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.71\n",
      "Epoch 260 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 261 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 262 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 263 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 264 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 265 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 266 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 267 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 268 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 269 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 270 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 271 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 272 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 273 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.7\n",
      "Epoch 274 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.7\n",
      "Epoch 275 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.69\n",
      "Epoch 276 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.69\n",
      "Epoch 277 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.69\n",
      "Epoch 278 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.69\n",
      "Epoch 279 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.69\n",
      "Epoch 280 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.69\n",
      "Epoch 281 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.69\n",
      "Epoch 282 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.69\n",
      "Epoch 283 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.69\n",
      "Epoch 284 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.69\n",
      "Epoch 285 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 286 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 287 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 288 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 289 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 290 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.69\n",
      "Epoch 291 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 292 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 293 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 294 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 295 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 296 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 297 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 298 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 299 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 300 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 301 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 302 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 303 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 304 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 305 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 306 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.68\n",
      "Epoch 307 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 308 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 309 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 310 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 311 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 312 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 313 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 314 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 315 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 316 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 317 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 318 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 319 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 320 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 321 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 322 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 323 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.67\n",
      "Epoch 324 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 325 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 326 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 327 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 328 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 329 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Q_matrix of initial state, after training: step 201\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADOklEQVR4nO2dq4tVcRRGv984jggKmnyEe6PBIDYtJhEmCIPBF4KYJxjUZDapwTBZBNHRIAOGATFZtInBYDwn+EgKCuJj/Fn2/Qc+J/mt1RcbDotdzr7ntt6HLohlXpJam/xw5N7HbeU/Mv0zkqQ2ve746sONmr9qzj9b82+a86/V/O/m/O01/4M5f1/5i6a/PmeJ8N9AAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYTTeB2cDRsgnNk9gLUFeh9b+T9Nf6H8j6a/V5LUpu8dX33YX/O3m/O/l//b9Ocl/fM9gtp0xfSX2QDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhcA8QDhsgnNn76NeW3YfDktTa5JOl93FP+Rumv0WS1Kb3HV99uFD+EdN/JW3KPYA1Xn1Qzff0PrIB0iGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwuEeIBw2QDiz99G7LbsPn8t/avony18y/bXyX5r+UWlTvo9wyvSflH/M9F+U/8f059gA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4XAPEA4bIJzZPcAdy+7D5fLfmP4hSWptsmbpfVwq/6vp7yzffp9e/lbT/yVJatPHjq8+nC7/kunfZQOEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEwz1AOGyAcOYlqbXJLUfufbwqSWrTd9b0Phyo+T/N+Qs1/6I5/175J0z/WfmHTP9N+cdN/3n5103/BhsgHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHO4BwmEDhDP7PsBty+7DlfL3mf4HSWptssvS+/il5p835z8o/6Dpvy1/3fQXJam1yYal93FLzbf+b0B9eMEGCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCId7gHDYAOHMvg+w7Mi9jyvlr5r+WUlSm+5wfPXhW/krpr9cvv37+vKt56c+zJ7fQ0vv47ny7XsCNkA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA43AOE8xe6gv7W0CJdXgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23\n",
      "Epoch 330 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 331 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 332 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 333 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 334 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 335 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 336 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 337 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 338 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 339 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.66\n",
      "Epoch 340 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 341 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 342 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 343 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 344 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 345 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 346 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 347 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 348 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 349 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 350 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 351 \t reward: -0.5 \t length 1 \t mean_length 1.0 \t epsilon 0.65\n",
      "Epoch 352 \t reward: 1.5 \t length 2 \t mean_length 1.01 \t epsilon 0.65\n",
      "Epoch 353 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.65\n",
      "Epoch 354 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.65\n",
      "Epoch 355 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.65\n",
      "Epoch 356 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.65\n",
      "Epoch 357 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 358 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 359 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 360 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 361 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 362 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 363 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 364 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 365 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 366 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 367 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 368 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 369 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 370 \t reward: -0.5 \t length 1 \t mean_length 1.01 \t epsilon 0.64\n",
      "Epoch 371 \t reward: -0.5 \t length 1 \t mean_length 1.02 \t epsilon 0.64\n",
      "Epoch 372 \t reward: 1.5 \t length 2 \t mean_length 1.03 \t epsilon 0.64\n",
      "Epoch 373 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.64\n",
      "Epoch 374 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.63\n",
      "Epoch 375 \t reward: 1.5 \t length 2 \t mean_length 1.04 \t epsilon 0.63\n",
      "Epoch 376 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.63\n",
      "Epoch 377 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.63\n",
      "Epoch 378 \t reward: -0.5 \t length 1 \t mean_length 1.03 \t epsilon 0.63\n",
      "Epoch 379 \t reward: -0.5 \t length 1 \t mean_length 1.04 \t epsilon 0.63\n",
      "Epoch 380 \t reward: 1.5 \t length 2 \t mean_length 1.05 \t epsilon 0.63\n",
      "Epoch 381 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.63\n",
      "Epoch 382 \t reward: -0.5 \t length 1 \t mean_length 1.05 \t epsilon 0.63\n",
      "Epoch 383 \t reward: -0.5 \t length 1 \t mean_length 1.06 \t epsilon 0.63\n",
      "Epoch 384 \t reward: 1.5 \t length 2 \t mean_length 1.07 \t epsilon 0.63\n",
      "Epoch 385 \t reward: -0.5 \t length 1 \t mean_length 1.08 \t epsilon 0.63\n",
      "Epoch 386 \t reward: 1.5 \t length 2 \t mean_length 1.09 \t epsilon 0.63\n",
      "Epoch 387 \t reward: 1.5 \t length 2 \t mean_length 1.1 \t epsilon 0.63\n",
      "Epoch 388 \t reward: -0.5 \t length 1 \t mean_length 1.09 \t epsilon 0.63\n",
      "Epoch 389 \t reward: 1.5 \t length 2 \t mean_length 1.1 \t epsilon 0.63\n",
      "Epoch 390 \t reward: -0.5 \t length 1 \t mean_length 1.09 \t epsilon 0.63\n",
      "Epoch 391 \t reward: -0.5 \t length 1 \t mean_length 1.1 \t epsilon 0.63\n",
      "Epoch 392 \t reward: 1.5 \t length 2 \t mean_length 1.11 \t epsilon 0.62\n",
      "Epoch 393 \t reward: -0.5 \t length 1 \t mean_length 1.12 \t epsilon 0.62\n",
      "Epoch 394 \t reward: -0.5 \t length 1 \t mean_length 1.11 \t epsilon 0.62\n",
      "Epoch 395 \t reward: -0.5 \t length 1 \t mean_length 1.12 \t epsilon 0.62\n",
      "Epoch 396 \t reward: 1.5 \t length 2 \t mean_length 1.13 \t epsilon 0.62\n",
      "Epoch 397 \t reward: -0.5 \t length 1 \t mean_length 1.13 \t epsilon 0.62\n",
      "Epoch 398 \t reward: -0.5 \t length 1 \t mean_length 1.13 \t epsilon 0.62\n",
      "Epoch 399 \t reward: -0.5 \t length 1 \t mean_length 1.13 \t epsilon 0.62\n",
      "Epoch 400 \t reward: 1.5 \t length 2 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 401 \t reward: -0.5 \t length 1 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 402 \t reward: -0.5 \t length 1 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 403 \t reward: 1.5 \t length 2 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 404 \t reward: -0.5 \t length 1 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 405 \t reward: 1.5 \t length 2 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 406 \t reward: -0.5 \t length 1 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 407 \t reward: -0.5 \t length 1 \t mean_length 1.15 \t epsilon 0.62\n",
      "Epoch 408 \t reward: 1.5 \t length 2 \t mean_length 1.17 \t epsilon 0.62\n",
      "Epoch 409 \t reward: -0.5 \t length 1 \t mean_length 1.18 \t epsilon 0.62\n",
      "Epoch 410 \t reward: -0.5 \t length 1 \t mean_length 1.17 \t epsilon 0.61\n",
      "Epoch 411 \t reward: -0.5 \t length 1 \t mean_length 1.18 \t epsilon 0.61\n",
      "Epoch 412 \t reward: 1.5 \t length 2 \t mean_length 1.19 \t epsilon 0.61\n",
      "Epoch 413 \t reward: 1.5 \t length 2 \t mean_length 1.2 \t epsilon 0.61\n",
      "Epoch 414 \t reward: -0.5 \t length 1 \t mean_length 1.19 \t epsilon 0.61\n",
      "Epoch 415 \t reward: 1.5 \t length 2 \t mean_length 1.2 \t epsilon 0.61\n",
      "Epoch 416 \t reward: 1.5 \t length 2 \t mean_length 1.21 \t epsilon 0.61\n",
      "Epoch 417 \t reward: -0.5 \t length 1 \t mean_length 1.22 \t epsilon 0.61\n",
      "Epoch 418 \t reward: 1.5 \t length 2 \t mean_length 1.23 \t epsilon 0.61\n",
      "Epoch 419 \t reward: 1.5 \t length 2 \t mean_length 1.24 \t epsilon 0.61\n",
      "Epoch 420 \t reward: -0.5 \t length 1 \t mean_length 1.23 \t epsilon 0.61\n",
      "Epoch 421 \t reward: 1.5 \t length 2 \t mean_length 1.24 \t epsilon 0.61\n",
      "Epoch 422 \t reward: -0.5 \t length 1 \t mean_length 1.23 \t epsilon 0.61\n",
      "Epoch 423 \t reward: -0.5 \t length 1 \t mean_length 1.24 \t epsilon 0.61\n",
      "Epoch 424 \t reward: 1.5 \t length 2 \t mean_length 1.25 \t epsilon 0.61\n",
      "Epoch 425 \t reward: -0.5 \t length 1 \t mean_length 1.26 \t epsilon 0.61\n",
      "Epoch 426 \t reward: 1.5 \t length 2 \t mean_length 1.27 \t epsilon 0.61\n",
      "Epoch 427 \t reward: -0.5 \t length 1 \t mean_length 1.28 \t epsilon 0.61\n",
      "Epoch 428 \t reward: -0.5 \t length 1 \t mean_length 1.27 \t epsilon 0.6\n",
      "Epoch 429 \t reward: -0.5 \t length 1 \t mean_length 1.28 \t epsilon 0.6\n",
      "Q_matrix of initial state, after training: step 301\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671C70>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADP0lEQVR4nO2dsW9NcRiG3580ZiRi0JzbqTqKgQSxa6ojm6EDEYmGWEgYSFiEtIkIg8HGWFK7IGEQY3XqOalBJJhFfAbf/Qe+mrzPsz95lyffcn65t0X0IbBlQpJa61YrcsQwm/7Oov9dktRGNyq+or+e+0+K+6dzf6m4v5j7H4v7+3N/s7g/mf580V/ZVhLhv4EAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMKfxOdgbLoA54/cAtypyxHA1/QtFfzn9b0V/lySpjb5WfEW/O/3Jor8pSa11ayU9hpncv1vcv5T+w6J/lgtgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDu8BzOECmDMhSWqj1yU7+qOS1Fp3sqTH8Cz90vf8iGH8Pf9pxVf0p9KfKPq/JKm1ri/pMYxyvzSv+DvbWvequH+MC2AOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO7wHM4QKYM34PMFWyo99I/2XRP57+maL/KP23Rf+wJLXWbZT0GKbSr83HoPQPFP0P6X8u+nu5AOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAObwHsAcLoA54/cAD0p29OfS/1T090lSa93vkh7DtvQfF/2F9N8V/UOSpDbaUfEV/Y/0t/r7BqX/a1D0y1wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc3gPYA4XwJwJaevf09VG66X16Kdz/0Vxfy73Dxb336c/W/RX0z9f9O//o/1rRf8mF8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc3gOYwwUwZ/z7ALdLdvRX0p8u+uvpby/6P9NfKPqP058p+mvpl94zKPo5SWqt2yzpMUzm/nxxf4ULYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA7vAczhApgz/n2AExU5Ynie/puif0SS1EZ7Kr6i/5L+Vv/v4HLRv5P+YtFfkqTWuoslPYZ76ZffE3ABzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOE9gDl/AC/q+9YXJdqaAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671C70>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADTUlEQVR4nO2cr49VVxhF92kmRTTBUSA09wbUpFRgEIAbRSG0hlZWQJPCmDEkiDFjRpBgMPxIaEVlWwMEWjUOKmoQlIyiuTdtyoAjQYA5mO/9Azso9lp+ZT+x8pl3clvvUxfEsiRJrQ0PHLn3+VT5o+lPkqQ2bji++rRR+z+b+9/V/jVzf632n5j7X9T+f+b+gfK/Mv27H1kifDAQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOE0/g7OhgsQzuI9wKYj9z6vl79q+tfLf236n0iS2rjj+OrT3vI/M/1/Jam14X9L7/P+2r9q7l8q/5bp/8AFCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIf3AOFwAcJZkiS18aFl9+mEJLU2nLX0Pv9W/gvT/1SS1MZfHF99+rb8j03/rfRe3gNY8+r1eYU2bJn7K1yAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcHgPEA4XIJzFewDvD+k+Lb73f9/0T5f/venfLv+R6R+XpNaGfyy9zwfL9+b7rPIPm/7f5b8y/d1cgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHB4DxAOFyCcxXuA65bdp9Xyt01/WZJaG15aep/3lG/9/t7n1fL/Mv2jkqQ27nZ89elV+b+a/jflr5n+NS5AOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOLwHCIcLEM6SJLU2/OTIvc/nJEltfGat9+lQ7d8x97+u/WPm/p/lnzT9P8q/aPo3yv/S9H8vf930N7kA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4fAeIBwuQDiL7wNcsew+XS5/2fS3y99l+m/KP2/6P5b/uek/Lf+B6Z+SpNaG55be5321f8bcv8cFCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIf3AOFwAcJZfB9gxZF7n7fKf2z6RyRJbdzr+OrTTvk3Tf9C+ZdN/0r59vf6Jam1YcPS+7xR/mT6IxcgHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHN4DhPMONyL71nRpx8kAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6718B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADVUlEQVR4nO2csYuPcRzH3x9+g6hLV9INnielmFC6QVFnQBbdgunKIsVguekGYrjpFgMlizJhuSzC4IoySGGilJ7nhkvq0hUZ8DHc5/kH3rd5v1/7q88zvPosz6dvZHYJI8sIABDtiLKz+w0AEc0qpWc/Xv4i6U+Xv4X0f5U/RfpL5Z8k/acAgGh3MT6yW675B8j57zdRg81/gwMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOKEfwdr4w0gznAPsJ2ys/te/mHSfw0AEc0NSs/+avlrpD8GAIh2jPGR3VrN5/TsUf4y6a/fEUQ7zn1At+oNII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDi+B5AHG8AcUYAENHsYeTM/jMAINoJanp2KzWfCjGz/1vzd5Lzv5Z/i/QvA0BEM0Pp2d8vf4r0lwAA0R5hfGT3yhtAHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMTxPYA43gDiDPcA5xg5s39Q/mPSP13+G9KfLH+S9NfnRruN8ZHdj5q/mZz/p+afJ+ffq/kfyPn7vQHEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBzfA4jjDSDOcA9wkZEz+zvlXyf9awCAaBkdyA7lXyL92wAQ0eyl9Ow/lf+W9A8BAKI9zvjI7nn5W0n/pzeAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjewBxvAHEGQEAouVCyG54r3+F9CfK30f6H8t/QfrHACCiuU/p2c+U/4j0z5R/gfTvlv+F9Hd7A4jjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOL4HEMcbQJzhHuAEZWf3rPxZ0l8o/yjpvyx/o/cM86Q/V/4p0n8CABEN5Wf2g79A+rPeAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgju8BxPEGEGe4B7hJ2dldAYCIZpHSs5+u+TvI+d/KP0j678qfI/15AIhozlJ69g/Lp7ZwZh8ANvS+gjeAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjewBx/gGQbQLlmcIgYwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n",
      "Epoch 430 \t reward: -0.5 \t length 1 \t mean_length 1.27 \t epsilon 0.6\n",
      "Epoch 431 \t reward: 1.5 \t length 2 \t mean_length 1.28 \t epsilon 0.6\n",
      "Epoch 432 \t reward: -0.5 \t length 1 \t mean_length 1.27 \t epsilon 0.6\n",
      "Epoch 433 \t reward: -0.5 \t length 1 \t mean_length 1.28 \t epsilon 0.6\n",
      "Epoch 434 \t reward: 1.5 \t length 2 \t mean_length 1.29 \t epsilon 0.6\n",
      "Epoch 435 \t reward: -0.5 \t length 1 \t mean_length 1.3 \t epsilon 0.6\n",
      "Epoch 436 \t reward: 1.5 \t length 2 \t mean_length 1.31 \t epsilon 0.6\n",
      "Epoch 437 \t reward: -0.5 \t length 1 \t mean_length 1.32 \t epsilon 0.6\n",
      "Epoch 438 \t reward: 1.5 \t length 2 \t mean_length 1.33 \t epsilon 0.6\n",
      "Epoch 439 \t reward: 1.5 \t length 2 \t mean_length 1.34 \t epsilon 0.6\n",
      "Epoch 440 \t reward: -0.5 \t length 1 \t mean_length 1.33 \t epsilon 0.6\n",
      "Epoch 441 \t reward: 1.5 \t length 2 \t mean_length 1.34 \t epsilon 0.6\n",
      "Epoch 442 \t reward: 1.5 \t length 2 \t mean_length 1.35 \t epsilon 0.6\n",
      "Epoch 443 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.6\n",
      "Epoch 444 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.6\n",
      "Epoch 445 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.6\n",
      "Epoch 446 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 447 \t reward: 1.5 \t length 2 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 448 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 449 \t reward: 1.5 \t length 2 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 450 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 451 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 452 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 453 \t reward: 4.0 \t length 3 \t mean_length 1.34 \t epsilon 0.59\n",
      "Epoch 454 \t reward: 1.5 \t length 2 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 455 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 456 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 457 \t reward: 1.5 \t length 2 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 458 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 459 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 460 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 461 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 462 \t reward: -0.5 \t length 1 \t mean_length 1.35 \t epsilon 0.59\n",
      "Epoch 463 \t reward: -0.5 \t length 1 \t mean_length 1.36 \t epsilon 0.59\n",
      "Epoch 464 \t reward: 1.5 \t length 2 \t mean_length 1.37 \t epsilon 0.59\n",
      "Epoch 465 \t reward: -0.5 \t length 1 \t mean_length 1.38 \t epsilon 0.58\n",
      "Epoch 466 \t reward: 1.5 \t length 2 \t mean_length 1.39 \t epsilon 0.58\n",
      "Epoch 467 \t reward: -0.5 \t length 1 \t mean_length 1.4 \t epsilon 0.58\n",
      "Epoch 468 \t reward: -0.5 \t length 1 \t mean_length 1.39 \t epsilon 0.58\n",
      "Epoch 469 \t reward: -0.5 \t length 1 \t mean_length 1.4 \t epsilon 0.58\n",
      "Epoch 470 \t reward: 1.5 \t length 2 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 471 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.58\n",
      "Epoch 472 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 473 \t reward: -0.5 \t length 1 \t mean_length 1.4 \t epsilon 0.58\n",
      "Epoch 474 \t reward: 1.5 \t length 2 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 475 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.58\n",
      "Epoch 476 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 477 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.58\n",
      "Epoch 478 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 479 \t reward: 1.5 \t length 2 \t mean_length 1.42 \t epsilon 0.58\n",
      "Epoch 480 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 481 \t reward: -0.5 \t length 1 \t mean_length 1.4 \t epsilon 0.58\n",
      "Epoch 482 \t reward: 1.5 \t length 2 \t mean_length 1.41 \t epsilon 0.58\n",
      "Epoch 483 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.58\n",
      "Epoch 484 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.57\n",
      "Epoch 485 \t reward: -0.5 \t length 1 \t mean_length 1.4 \t epsilon 0.57\n",
      "Epoch 486 \t reward: 1.5 \t length 2 \t mean_length 1.41 \t epsilon 0.57\n",
      "Epoch 487 \t reward: 1.5 \t length 2 \t mean_length 1.4 \t epsilon 0.57\n",
      "Epoch 488 \t reward: 1.5 \t length 2 \t mean_length 1.41 \t epsilon 0.57\n",
      "Epoch 489 \t reward: 1.5 \t length 2 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 490 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.57\n",
      "Epoch 491 \t reward: 1.5 \t length 2 \t mean_length 1.44 \t epsilon 0.57\n",
      "Epoch 492 \t reward: -0.5 \t length 1 \t mean_length 1.43 \t epsilon 0.57\n",
      "Epoch 493 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 494 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.57\n",
      "Epoch 495 \t reward: -0.5 \t length 1 \t mean_length 1.44 \t epsilon 0.57\n",
      "Epoch 496 \t reward: -0.5 \t length 1 \t mean_length 1.43 \t epsilon 0.57\n",
      "Epoch 497 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 498 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.57\n",
      "Epoch 499 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 500 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.57\n",
      "Epoch 501 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 502 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.57\n",
      "Epoch 503 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.57\n",
      "Epoch 504 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 505 \t reward: -0.5 \t length 1 \t mean_length 1.44 \t epsilon 0.56\n",
      "Epoch 506 \t reward: -0.5 \t length 1 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 507 \t reward: 1.5 \t length 2 \t mean_length 1.44 \t epsilon 0.56\n",
      "Epoch 508 \t reward: -0.5 \t length 1 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 509 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.56\n",
      "Epoch 510 \t reward: -0.5 \t length 1 \t mean_length 1.41 \t epsilon 0.56\n",
      "Epoch 511 \t reward: 1.5 \t length 2 \t mean_length 1.42 \t epsilon 0.56\n",
      "Epoch 512 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 513 \t reward: 1.5 \t length 2 \t mean_length 1.42 \t epsilon 0.56\n",
      "Epoch 514 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 515 \t reward: 1.5 \t length 2 \t mean_length 1.44 \t epsilon 0.56\n",
      "Epoch 516 \t reward: -0.5 \t length 1 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 517 \t reward: -0.5 \t length 1 \t mean_length 1.42 \t epsilon 0.56\n",
      "Epoch 518 \t reward: 1.5 \t length 2 \t mean_length 1.43 \t epsilon 0.56\n",
      "Epoch 519 \t reward: 1.5 \t length 2 \t mean_length 1.42 \t epsilon 0.56\n",
      "Epoch 520 \t reward: 4.0 \t length 3 \t mean_length 1.45 \t epsilon 0.56\n",
      "Epoch 521 \t reward: 1.5 \t length 2 \t mean_length 1.46 \t epsilon 0.56\n",
      "Epoch 522 \t reward: 1.5 \t length 2 \t mean_length 1.47 \t epsilon 0.56\n",
      "Epoch 523 \t reward: -0.5 \t length 1 \t mean_length 1.48 \t epsilon 0.56\n",
      "Epoch 524 \t reward: 1.5 \t length 2 \t mean_length 1.49 \t epsilon 0.55\n",
      "Epoch 525 \t reward: -0.5 \t length 1 \t mean_length 1.48 \t epsilon 0.55\n",
      "Epoch 526 \t reward: -0.5 \t length 1 \t mean_length 1.47 \t epsilon 0.55\n",
      "Epoch 527 \t reward: -0.5 \t length 1 \t mean_length 1.46 \t epsilon 0.55\n",
      "Epoch 528 \t reward: 1.5 \t length 2 \t mean_length 1.47 \t epsilon 0.55\n",
      "Epoch 529 \t reward: 1.5 \t length 2 \t mean_length 1.48 \t epsilon 0.55\n",
      "Q_matrix of initial state, after training: step 401\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6718B0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADU0lEQVR4nO2dr8uWZxiGz1s+/BEMghNEeF4UEYMLM7gmGEwmfzMMWtwUYVsRUQYijI0PiwripkWDiDo1mQzC2gwzbGGIKM8DImyDBcOm5bZc7z9w2nYeRz84y8FV3pv3ab2PXRDLgiS1Nqxz5N6nV5KkNttgrffxRfmLpn9Kklob9lt6n+7W/o/m/he1/5m5f6v2X5v7a8vfa/r3llgi/G8ggHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMJp/BycDRcgnAVJUpstt+w+/lf+KtP/R5JaG25Yep8O1/7f5v7q8reZ/hNJam04Zel9Wqz9C+b+1+X/YPrHuADhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDh8B4gHC5AOPP3AI8tu487yl9j+n9KUmvDTUvv06Hav2/u76n9n8397eVfNP2vJElt5uhSH1X7+8z9n7gA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4fAeIBwuQDjz9wAfW3Yffyv/kenvLP+M6X9X/i+m/6kktTact/Q+nSz/V9PfWv4y039b/lnTP8cFCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIf3AOFwAcKZvwe4btl9PFL+aPozSWpt8PQ+qfaXmvvvan+/uX+39jea+8/Lv2P6B8q3vlegPi5yAcIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMLhPUA4XIBwFiSpteG4I/c+XZEktdkf1nofN9f+JnP/We1/0PcK1Ga7TP9h+ZdM/8vyd5v+g/K/Nf1vuADhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDh8B4gHC5AOPP/B7B/Ty7/E9N/Wv5Hpv9X+Z+b/tXyt5j+7+U/NP1dktTacMLS+3S59o+Y+9e5AOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOHwHiAcLkA48/8HWOHIvU//ln/b9A9KktpsveOrjy/Lv2b6R8s/bfrfl3/S9M9LUmvDSkvv05vyre8N9D4d4AKEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEw3uAcN4Dq9X31kpJI5cAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671BE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADTElEQVR4nO2du2pVURRF55KAYiNIooJwboiNlQT0A4KFkEYFrX2AItgYbIziAx9obCRpBFHwUSuoTcBC8gEKwcoml5wNghoRbESrZbPuD8zSOUc/mM1gNXdzbmT2CSPLGABEdJOMnNnWAQAxmKLWsx+Wf4/0LwNARHeD0rPdrP2H5P752r9F7l+v/R/k/nj5x0j/1SZKNP8NDkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiBP+OVgbXwBxxgAAMdhC2dn/KX876f8EgIjuPaVnO1j7G+T+RPn7Sf8jAER0y5Sebbb2F8n9ufIfk/5ZXwBxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMfvAcTxBRBn9B5ghbKznyl/gvQ3ACCi+0zp2fbW/hty/0jt/yX3N5e/QvozAIAYMDqQPWr/DLn/xBdAHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMTxewBxfAHEGb0H2EfZ2X8q/x3pHyr/KunfKf8D6R8AgIjuLaVnO1w+9b3/zDZe/jbS/1X+M9I/5QsgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4PYA4vgDijN4DvKDs7E+U30i/A4CIjtOzZmOwldz/XfvUe4TMNnqPsIfcXyv/JekfL3+e9Bd8AcRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHL8HEMcXQJwxAIjo7jNyZrsEAIjBkFrPfqr2d5L732p/N7n/pfxZ0l8uf5H058o/Svqvy79N+td8AcRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHL8HEMcXQJzR9wEWKDv7+fKnSX+1/F2k/7X8c6T/qPxp0l8tf5n0ZwEgolui9GwXav8kuf/cF0AcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxPF7AHF8AcQZfR8gGDmzZfnfSX8HACAGU4yP7IflPyX90+VfIf275V8k/QcAENFR/zeQ2dbKXyf9SV8AcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDH7wHE+QdKRPfW+Iv+IwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671940>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADUUlEQVR4nO2dzYuOcRSG7yNZjSkLNSnPU8rC1kfCQokixVqErZKSKAsrC0WTkrJFNGtKFCkLNPnYWij1+ylNWaiZWc3mWDjvP3DPrNz3tb/mvFNXZ/Oe530isyWMLOsBADFOUXa2ZQCIGLZQevZf5b8i/aPl3yb9a+UzOjI7yr9K+nf+/YHxMPcB2puaf4Cc/34dNdj8NzgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByBO+OtgbbwBxFkPrP77fMQ4Q03PtlD+ftL/AAARwwtKz368/Mekf7b8X6S/pfzTpP8UABDjPsZHto/eAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgju8BxPEGEGdyD7CLkTP7l/J3k/7n8neQ/jcAQIzkA/6t1/xpcv5i+XtI/1P5F0j/AQAgxq2Mj2w/vQHEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBzfA4jjDSDO5H0BI2VnawAQMZyi9OxzNX8jOX+p/NXeA2yg9OwrNf88Of9hzd9Jzv9a86nPj2wr3gDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII7vAcTxBhBncg+wmbKz/QbW5Pl+ajyyoXzqHgHZ5gAgYrhC6dlnyz9G+i8BADFuZ3xk+17zqXuKzL7kDSCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4vgeQBxvAHEm7wuYYeTMvgAAiPEMNT3bkzXyb5L+DQCIGC5RevZ75d8n/YvlnyP9R+XPkv4VbwBxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMf3AOJ4A4gz+X2AQ5Sd7S0ARAzbKD37j5p/gpz/vPxp0l8s/yTpPyt/ivSXASBiOELp2V+Xf5n073oDiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4vgcQxxtAnMk9APV7/8j2DQAiBur5/Mx+o+ZzLwzI1srfS/rz5W8i/T8AEDG8o/TsB8u/RfrXy6f+/8w+7w0gjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4HkCcvy1wBeUkTiPhAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "Epoch 530 \t reward: 1.5 \t length 2 \t mean_length 1.49 \t epsilon 0.55\n",
      "Epoch 531 \t reward: -0.5 \t length 1 \t mean_length 1.5 \t epsilon 0.55\n",
      "Epoch 532 \t reward: -0.5 \t length 1 \t mean_length 1.49 \t epsilon 0.55\n",
      "Epoch 533 \t reward: 1.5 \t length 2 \t mean_length 1.5 \t epsilon 0.55\n",
      "Epoch 534 \t reward: -0.5 \t length 1 \t mean_length 1.49 \t epsilon 0.55\n",
      "Epoch 535 \t reward: -0.5 \t length 1 \t mean_length 1.48 \t epsilon 0.55\n",
      "Epoch 536 \t reward: -0.5 \t length 1 \t mean_length 1.47 \t epsilon 0.55\n",
      "Epoch 537 \t reward: 1.5 \t length 2 \t mean_length 1.46 \t epsilon 0.55\n",
      "Epoch 538 \t reward: 1.5 \t length 2 \t mean_length 1.47 \t epsilon 0.55\n",
      "Epoch 539 \t reward: 4.0 \t length 3 \t mean_length 1.46 \t epsilon 0.55\n",
      "Epoch 540 \t reward: -0.5 \t length 1 \t mean_length 1.45 \t epsilon 0.55\n",
      "Epoch 541 \t reward: -0.5 \t length 1 \t mean_length 1.46 \t epsilon 0.55\n",
      "Epoch 542 \t reward: 1.5 \t length 2 \t mean_length 1.47 \t epsilon 0.55\n",
      "Epoch 543 \t reward: -0.5 \t length 1 \t mean_length 1.46 \t epsilon 0.55\n",
      "Epoch 544 \t reward: 1.5 \t length 2 \t mean_length 1.47 \t epsilon 0.54\n",
      "Epoch 545 \t reward: -0.5 \t length 1 \t mean_length 1.48 \t epsilon 0.54\n",
      "Epoch 546 \t reward: 1.5 \t length 2 \t mean_length 1.49 \t epsilon 0.54\n",
      "Epoch 547 \t reward: 1.5 \t length 2 \t mean_length 1.5 \t epsilon 0.54\n",
      "Epoch 548 \t reward: 1.5 \t length 2 \t mean_length 1.5 \t epsilon 0.54\n",
      "Epoch 549 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.54\n",
      "Epoch 550 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.54\n",
      "Epoch 551 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 552 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 553 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.54\n",
      "Epoch 554 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 555 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 556 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.54\n",
      "Epoch 557 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 558 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.54\n",
      "Epoch 559 \t reward: 4.0 \t length 3 \t mean_length 1.56 \t epsilon 0.54\n",
      "Epoch 560 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.54\n",
      "Epoch 561 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.54\n",
      "Epoch 562 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.54\n",
      "Epoch 563 \t reward: 1.5 \t length 2 \t mean_length 1.6 \t epsilon 0.54\n",
      "Epoch 564 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.54\n",
      "Epoch 565 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 566 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 567 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 568 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 569 \t reward: 1.5 \t length 2 \t mean_length 1.6 \t epsilon 0.53\n",
      "Epoch 570 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 571 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 572 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 573 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.53\n",
      "Epoch 574 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 575 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 576 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.53\n",
      "Epoch 577 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.53\n",
      "Epoch 578 \t reward: 4.0 \t length 3 \t mean_length 1.6 \t epsilon 0.53\n",
      "Epoch 579 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.53\n",
      "Epoch 580 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.53\n",
      "Epoch 581 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.53\n",
      "Epoch 582 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.53\n",
      "Epoch 583 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.53\n",
      "Epoch 584 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.53\n",
      "Epoch 585 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.53\n",
      "Epoch 586 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 587 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 588 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 589 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 590 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 591 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 592 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 593 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 594 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 595 \t reward: 4.0 \t length 3 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 596 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 597 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 598 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 599 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 600 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 601 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 602 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 603 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.52\n",
      "Epoch 604 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 605 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 606 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.52\n",
      "Epoch 607 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 608 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 609 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 610 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 611 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.51\n",
      "Epoch 612 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 613 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 614 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 615 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 616 \t reward: 4.0 \t length 3 \t mean_length 1.66 \t epsilon 0.51\n",
      "Epoch 617 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.51\n",
      "Epoch 618 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.51\n",
      "Epoch 619 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.51\n",
      "Epoch 620 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.51\n",
      "Epoch 621 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 622 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 623 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 624 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 625 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 626 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.51\n",
      "Epoch 627 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 628 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.51\n",
      "Epoch 629 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.5\n",
      "Q_matrix of initial state, after training: step 501\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671EE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADSUlEQVR4nO2cPYtWVxhF95GJhYJgEIyFd1CwMYLgB0RsTAoVRQgI09hYBjONlWNhZeFYpTFiOY3NgBAICWqhaYJCPiCgNoLh3hQqhASEsVDx2DzvH9ile61+sZv1Ps17uK33sQtimZMktfktlt3Hf8s/Yvq/lP+d6Z+XpNaGDZbep9e1v2Luny1/zvTflf/c9LeVv2D6q+ssET4aCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwGn8HZ8MFCGf2HmCHZffx7/J3m/4TSWptOGrpfbpb+/+Z+5+Wf9L0f5Kk1oaNlt6ntdq/Zu4vlv+96X/LBQiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiH9wDhcAHCmb0HuGvZfTxa/hem/1CSWhu+tPQ+3a/9H839U7V/y9w/Xf5+0/9DktTmHV3qo8r3fsh9fM8FCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIf3AOFwAcKZvQc4ZNl9fFD+PdP/qvxl018q/zfTPyhJrQ2fW3qfHpd/2/SPS5La/CeOrz6+rf2t5v5LLkA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4vAcIhwsQzuw9wE3L7uOZ8l+Y/meS1Nrg6X1S7e8y95+Wv97035R/wPR/L3/V9BfKv2T6l7kA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4fAeIBwuQDhzktTasN2Re5/+kSS1+UfWeh/3lL/J9F9JUmvDPkvv05+1f8zcv1P+D6b/dfkLpr9a/lXTv8AFCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIf3AOFwAcKZfR/gomX38Ur5h03/1/J3mv6z8s+Z/vXy95r+X+X/bPonym+m38tfNP1rXIBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBweA8QDhcgnNl7gI2W3cc1SWpt+MbS+3Sj9veY+4/KXzH9s+Xb39svf8n0l8vfbPr/S1JrwxlL79NNLkA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4vAcI5wOc1u/W+NIxDwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671580>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADUklEQVR4nO2dscuVZRyG70e/NYfw00F7D4gQJg4qKFggSIIQLkYitISDEoKLg4NYVDg4uAQiOkSLIIotEQiKIJigYA1RIkTwvumgnzjomj0uv/MP3G7e17Vf3Mt1fst5OKf1PnZBLAuSpDZbtOw+LpW/0/RvlX/G9I9JUmvDB5bep79q/4K5f6j2m7nfa/+5uf9u+ftN//IyS4S3BgIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggnMbXwdlwAcKZvwdYZ9l9/Kf8jab/pyS1Nhyw9D5dqv2n5v6q8veY/jVJam3YZul9ulf735v7R8s/b/qHuQDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDh8B4gHC5AOPP3ANctu4+7y99u+nclqbXhoKX36Yfa/8Xc/6T2l8z9xfL3mv7PkqQ2c3Spjyp/uem/4gKEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEw3uAcLgA4czfA3xo2X38tfybpr+r/NOmf7z830x/iyS1Nuyw9D7dKf+x6a+RJLXZguOrj//V/nvm/r9cgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHB4DxAOFyCc+XuAS5bdxwPlPzH91ZLU2uDpfVLtv9H/FajNvA9CH/8vf7Pp/17+VdP/tPyTpv8dFyAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAc3gOEwwUIZ0GSWhs2OHLv0wNJUpv9ba33cX35i6a/JEmtDWstvU+Pav9jc/9G+T+Z/r7yPzP9K+WfMv0TXIBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBweA8QDhcgnPnvA3xl2X38tvyPTP92+e+b/sPyj5j+2fK3mv798q+b/m5Jam1Yael9elb7X5r757gA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4fAeIBwuQDjz9wArLLuPLySpteFHS+/TF7W/ydz/o/yLpv95+V+b/jflHzP9M+W/Y/ovJam14YSl9+kUFyAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAc3gOE8xpstPHWU+Cg5gAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671610>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADQUlEQVR4nO2dzYvNcRSHP1/G68hSSv2uhSg2MkV5SfYSSdnYKomVpbHA0oqkbG2URLKXvBQ1sqHIwu9XSpaaMV47Fs79B87Myud59k/n3no6m3v63hbRh8CWCUlSG60t2dF/laTWuo0lPYaP6V8q+tPp3y/6h9Ov6IoYlP500f/3vdtob+0D9E9z/vbi/NdLSoPhv4EAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMKfxc7A3bABzxvcAy0t29D/TX1X059NfX/Q/S1Jr3UxJj2Eq/XNF/0r6c0V/Mv2TRf+mJKmNtlR8Rf+ODWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHMmZCk1rqdFTlieJn+jqL/Kv1dRf+FpAW/b7DQe4bWum0lPYY36R8r+nckLeiegg1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzBm/D7CmZEc/K0mtdQdLegwP058o+r8lSW20ruIr+i/pl+Yr+vH8A0X/kbQo9xAri/O/swHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOeN7gNUlO/pvktRat6+kx/Ak55fGK3qlf6joP5Ck1rqzJT2Gq+kfLfp3JS3G+wQbivM/sQHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOeP/Cyj9Hh0xzEuS2uhIaXr099I/XvRvp3+66F+XpNa6kh8xjP0zRf9a+nuK/rP0bxX9E2wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMIcNYM74fYCpkh39TPrLiv6v9HcX/efpLy36f9LfWvTfpr+p6H+QpNa6CyU9hovpTxf9S2wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMIcNYM74HmBzyY7+vSS11p0q6THcyPlrivNn01/oPcNk0Z+TpNa6xyU9hv3pXy765yVJbbSi4iv6H2wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMOcvIiUB5XUzTGUAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "Epoch 630 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 631 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 632 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 633 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 634 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 635 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 636 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 637 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 638 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 639 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 640 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 641 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.5\n",
      "Epoch 642 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 643 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 644 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 645 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.5\n",
      "Epoch 646 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 647 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 648 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 649 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.5\n",
      "Epoch 650 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.5\n",
      "Epoch 651 \t reward: 1.5 \t length 2 \t mean_length 1.6 \t epsilon 0.5\n",
      "Epoch 652 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 653 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 654 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 655 \t reward: 4.0 \t length 3 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 656 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 657 \t reward: 4.0 \t length 3 \t mean_length 1.6 \t epsilon 0.49\n",
      "Epoch 658 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 659 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.49\n",
      "Epoch 660 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 661 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 662 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 663 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 664 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 665 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 666 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 667 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 668 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 669 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 670 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 671 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.49\n",
      "Epoch 672 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 673 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 674 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.49\n",
      "Epoch 675 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 676 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 677 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.48\n",
      "Epoch 678 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.48\n",
      "Epoch 679 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 680 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 681 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 682 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 683 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 684 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 685 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 686 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 687 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 688 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 689 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.48\n",
      "Epoch 690 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 691 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.48\n",
      "Epoch 692 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.48\n",
      "Epoch 693 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.48\n",
      "Epoch 694 \t reward: -0.5 \t length 1 \t mean_length 1.5 \t epsilon 0.48\n",
      "Epoch 695 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.48\n",
      "Epoch 696 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.48\n",
      "Epoch 697 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.48\n",
      "Epoch 698 \t reward: -0.5 \t length 1 \t mean_length 1.5 \t epsilon 0.47\n",
      "Epoch 699 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 700 \t reward: 4.0 \t length 3 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 701 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 702 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 703 \t reward: 4.0 \t length 3 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 704 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 705 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 706 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 707 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 708 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 709 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.47\n",
      "Epoch 710 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 711 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 712 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 713 \t reward: 4.0 \t length 3 \t mean_length 1.56 \t epsilon 0.47\n",
      "Epoch 714 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.47\n",
      "Epoch 715 \t reward: -0.5 \t length 1 \t mean_length 1.56 \t epsilon 0.47\n",
      "Epoch 716 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 717 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 718 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 719 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 720 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.47\n",
      "Epoch 721 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.47\n",
      "Epoch 722 \t reward: 4.0 \t length 3 \t mean_length 1.56 \t epsilon 0.46\n",
      "Epoch 723 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.46\n",
      "Epoch 724 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 725 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 726 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 727 \t reward: 4.0 \t length 3 \t mean_length 1.56 \t epsilon 0.46\n",
      "Epoch 728 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 729 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.46\n",
      "Q_matrix of initial state, after training: step 601\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671580>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADK0lEQVR4nO2dsYvPcRyH3x9dKUmUK3R9rxiUTa5QlE4MMiHKFZORpAyGm24wKInRRJ0iTLqBXIpCnWzKQH2+XahTJCl19bG8v//A6zav59mfPsvTe/m9vv1Ka7UF2DISERFlfLtkt/ox/ROi/zD9m6J/If3Vov83/VnRn0p/o+h/T39R9MfSPy3691ZJIvw3EIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmFH4O9oYLYM6wB9gl2a2+S3+f6L+KiCil2yrprf+c7/8Q39+Q/hnRv5v+OtH/lf5K9xCyzwUwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhz2AOVwAc4Y9wJxkt3ok/SOiPxcRUUq3RdJb/yXffyy+fyzffyS+fzz9UdFfioiIMq7oEa1G+mtF/zcXwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBz2AOZwAcwZ9gCTkt3qfPrzoj+Z/kq/j38r+rsjIkrpRiS99cvpS3uK1vphT7FG8aPVP+nL/5fABTCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHPYA5XABzhj3Afclu9VT6X0V/c0REKZ2mtz7y/b3i+6/T3yT639I/IPov0n8g+ifTnxH9aS6AOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOewBzOECmDPsAaTv46PV5fQXRH8i/THRX4yIKKU7J+mtv53vHxLff5b+S9Hfn/6U6M+mf130L3EBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzGEPYA4XwJxhD3BRslu9kf5B0X+e/g7R/5D+edG/lf5O0X+f/hPRP5r+etH/mf5l0b/GBTCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHPYA5XABzhj3AqGS3uhQRUUp3WNJb/zTfnxDfX0j/juifTX9a9GfSvyL6V9PfJvqfIiJK6fZIeuvfcAHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMYQ9gzj/Tg+vWuGIaXgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADL0lEQVR4nO2dTYtOcRiH7z8jLGZj4aV0HkqxEBtDkWYxdrOSBeXlC5BpymyJ7ahpxBfwUixk9ezMYhLF2JAFpTgn5WVhMwuE/jb3+QK/Z+d3Xfuru05X9+bcp1NqbWuALWMREVEGeyS7tm/TPyH6D9O/IfoX0x8T/T/p3xb9c+lvEv3v6X8T/c3pnxL9+2skEf4bCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCq+DvWEDmNPfA0xIdm1X0j8i+k8jIkppdkt67d7l/FHfp58V/TvpbxT9H+kviP5s+jdF/wIbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5jT3wMMJbu20+lPi/4wIqKUZpek1+59zn8kzj+e81+I8w+mv030P0dERBkoekRtI/1x0V9lA5hDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPYA5bABz+nuAY5Jd28fpL4n+VPqLoj+T/nPRPxQRUUqzTtJr9zv9V6K/PyIiymCD4kdtf6a/XvR/sQHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOf09wAPJru3J9D+J/vaIiFIaTa9d5Pyj4vwn6W8R/a/pT4r+cvqjPv9ron+ZDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHM6e8B1kp2bf+m/1r096W/Q/Q/RkSU0pyX9NrdyvlT4vyl9JdFfzL906J/L/150Z9jA5hDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPYA5bABz+nuAS5Jd2+vpj/q/gb2i/yb9GdFfTP+A6L9Mfyj60+mPi/5q+rOiv8AGMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId7AHPYAOb09wBbJbu2XyIiSmmk79tr7frv2yfE+Svp3xX9M+lfEf2r6c+J/nz6O0X/Q0REKc1hSa/dMzaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmPMP6PPr1v5a/lAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671CA0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADP0lEQVR4nO2dsWoVURRF94kBRQwqtjIDIYKgougPpLCIljYKtqZIY6l+glrapIitoI2lprDIDyiKCkJCYAZbUYmIFubanPcDZ6zca/WLA++td5q5c1+0NjSBLfOSpOiPlOw2/JCkiO50SW/jp/Tniv5++utFfy39e0X/fvrXiv5zSVL0qxVfbdjI+Q+L8++UPnj4fyAAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzAkeB3vDBjBndh5gsWS3YVeSIrqzJb2NH3L+geL8Pzm/prdR6S8U/b3094v+XPqbRX9FkhT9qYqvNmyzAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhPIA5bABz5iUportYkVsb30iSoj9amt6G7zn/ZnH+k5w/6X6Df3A/QpT0Nrb0Dxf9n5Kk6I9XfLXhKxvAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHM4DmMMGMGd2P0DxBfthlKbflx/Rld5vb23cTn/q/w0cLPq/JUnR9xVfbRhy/vXi/Gc5v3y/AhvAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHM4DmMMGMGd2P8Ck5+ER3aOif1uSFLXH6WqD0r9a9F9IUkR3uaS38VX6y0V/K/2lor+T/oWi/5YNYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA7nAcxhA5gzOw+wWJFbG3clSdHfKk1vw+P0V4r+ZvrLRX9Lmv5+fkR3pei/lCRFX/shtmF2v8F6cf4aG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAczgOYwwYwZ/Z/AVOfxx8q+r/SP1f036dfer9ebdhJ/0zR/5j+iaL/RZIiuo2S3sbV9HeK/hIbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzOA5jDBjBndh7gfMluwztJiugelPQ23s35C8X5e+lfKvqv0z9Z9D9LUkT3raS38Vj6N4r+U0mTvj82gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDmcBzDnL5DFBOU891f1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "Epoch 730 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 731 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 732 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 733 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 734 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 735 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 736 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 737 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 738 \t reward: 1.5 \t length 2 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 739 \t reward: 4.0 \t length 3 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 740 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 741 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 742 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 743 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.46\n",
      "Epoch 744 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 745 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 746 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.46\n",
      "Epoch 747 \t reward: -0.5 \t length 1 \t mean_length 1.54 \t epsilon 0.45\n",
      "Epoch 748 \t reward: -0.5 \t length 1 \t mean_length 1.52 \t epsilon 0.45\n",
      "Epoch 749 \t reward: 1.5 \t length 2 \t mean_length 1.52 \t epsilon 0.45\n",
      "Epoch 750 \t reward: 4.0 \t length 3 \t mean_length 1.54 \t epsilon 0.45\n",
      "Epoch 751 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.45\n",
      "Epoch 752 \t reward: 1.5 \t length 2 \t mean_length 1.56 \t epsilon 0.45\n",
      "Epoch 753 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.45\n",
      "Epoch 754 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.45\n",
      "Epoch 755 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.45\n",
      "Epoch 756 \t reward: -0.5 \t length 1 \t mean_length 1.58 \t epsilon 0.45\n",
      "Epoch 757 \t reward: 1.5 \t length 2 \t mean_length 1.58 \t epsilon 0.45\n",
      "Epoch 758 \t reward: 4.0 \t length 3 \t mean_length 1.6 \t epsilon 0.45\n",
      "Epoch 759 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 760 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 761 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.45\n",
      "Epoch 762 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 763 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.45\n",
      "Epoch 764 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 765 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 766 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 767 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 768 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 769 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 770 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.45\n",
      "Epoch 771 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.45\n",
      "Epoch 772 \t reward: -0.5 \t length 1 \t mean_length 1.6 \t epsilon 0.44\n",
      "Epoch 773 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 774 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 775 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 776 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 777 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 778 \t reward: 1.5 \t length 2 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 779 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.44\n",
      "Epoch 780 \t reward: 4.0 \t length 3 \t mean_length 1.64 \t epsilon 0.44\n",
      "Epoch 781 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 782 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 783 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 784 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.44\n",
      "Epoch 785 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 786 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 787 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 788 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 789 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 790 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 791 \t reward: 4.0 \t length 3 \t mean_length 1.68 \t epsilon 0.44\n",
      "Epoch 792 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 793 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.44\n",
      "Epoch 794 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 795 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.44\n",
      "Epoch 796 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 797 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.44\n",
      "Epoch 798 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 799 \t reward: 4.0 \t length 3 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 800 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 801 \t reward: 4.0 \t length 3 \t mean_length 1.64 \t epsilon 0.43\n",
      "Epoch 802 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.43\n",
      "Epoch 803 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 804 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.43\n",
      "Epoch 805 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 806 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 807 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 808 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 809 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 810 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 811 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 812 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 813 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 814 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 815 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 816 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.43\n",
      "Epoch 817 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 818 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 819 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 820 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 821 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 822 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.43\n",
      "Epoch 823 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 824 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.43\n",
      "Epoch 825 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 826 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 827 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 828 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 829 \t reward: 4.0 \t length 3 \t mean_length 1.7 \t epsilon 0.42\n",
      "Q_matrix of initial state, after training: step 701\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671EE0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADNklEQVR4nO2cvcuOYRiHz0smycfCoucxGLAQiTK9A/kc5GMwPRkUipRBkUGUQelVKIOeyeAjg88Y3kmRyLtgMLjuLCwvkvUyvOf9D/xsfsexH509T0fncp9dpbXaAmyZGxERZbhRslt9nf5I9MfpXxf9o+kvFP2f6d8T/X3prxD9z+l/Ff1l6Y9EfzxHEuG/gQDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwp/A52Bs2gDn9PcCEZLc6lf4O0X8SEVHKYLGkt24m5/8W589P/4ToT6Y/EP0u/cuifyr9q6J/nA1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzOnvAR5Kdqu7098j+g8iIkoZDCW9dTXn3xbnH8z5Y3H+KP1/vWdQ9Ig2+/OjDJeI/nc2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjCnvwfYJtmtPkt/SvQn0r8p+ofTfyP6G9KfJ/p/IiJKGdyV9Nbtz/mLxPk/0pfuEaLVGTaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMGMKe/B7gv2a3uTf+b6C+NiChFfG6/zT63H2W4XZz/NP1Vov8x/a2i/zz9O6J/IP1Lon+aDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHM6e8BFkh2q7/SfyX6m9JfLfofIiJKGZyT9Nadz/lbxPkv0n8n+uvSH4n+OP0bon+EDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHM6e8Bjkl2q9fS3yn6j9NfK/rv0z8p+lfSXy/6b9N/JPq70hcfSKhd+mdF/wIbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5jT3wMsl+xWv0RElDJYI+mtm875m8X5L9O/JfqH0pfeF4hW+/cFzoj+xfSl/y9anY6IKGWwUtJb94kNYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOb8BfXI6tbfwZjgAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671820>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJklEQVR4nO2dv6uPYRiH70cWykD50RleKcUgEWKR4ctwRIoSRWxKRJ0kk0ySTh05nbIRRYoSOQPfQRZCJAOl9L7DyY9iUIyP5X7/gc/ZfK5rv7qXq3t576e31NrWAFvmRkREWb5Fsmv7Iv0jon8j/SnRP57+PNH/m/4d0T+Q/grR/5L+D9FfnP4h0b81RxLhv4EAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMKfwOdgbNoA5/T3AQLJrO0x/VPSnIyJKaZZKeu2+5fyf4vxF6Z8S/SvpLxH97+lfFP1z6U+K/gk2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjCnvwd4KNm13Z3+XtG/HxFRSrNW0mv3PuffFucfzPnPxPnb0p/tPYOiR9Q20l8m+l/ZAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc/h5gp2TX9nH6Q9EfpH9N9I+l/0r0N6U/X/T/RESU0kjza+36+QvE+b/TXyj6v9gA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5nAPYA4bwJz+HuCeZNd2X/ozoj8SEVFKo+m1i5y/R5z/IP1Vov8p/e2i/zT9u6K/P335fwNsAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHO4BzCHDWBOfw8w2/fp70R/XfrrRf9tREQpzbik124s5w/E+cP0X4v+xvSPiv719K+K/kk2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjCnvwc4Ldm1nUh/l+g/Sn+D6L9Jf0z0x9PfLPov058W/dH0R0R/Jv2zon+JDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHM6e8BVkp2bT9HRJTS7JD02j3J+VvF+c/Tvyn6h9O/IPrn0z8j+pfTXyP6HyIiSmlWS3rtPrIBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewJx/EE/q1ol4woMAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADRklEQVR4nO2dO4+NYRRGn9etQUOMS+T7InFJJi6JRikKhIKKH6AyiVIpEYlSKRmVH2AqCoJClBqJSyQuiXxfxGWExkxD2Jp9/sA+Ks9a/co+xTq7OTvvaRFDCGxZJUlq/fqSHcMPSWqtO1jSY3ya/sai/y3920X/dPrXiv7F9M8W/VuSpNafqPiK4V7Ov1qcf2lFaTD8NxCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5jR+DvaGDWDO5B5gpmTHsChJrXW7S3qMb9LfVvQ/pr9Y9GfSX1f0l9Kv6IoYlf5U9wxq/dbaBxg+sQHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOaskqbVutiJHjK8kSa1fW5oew3LOP1+cfyPnry7O/5X+hqL/XZJa63aU9Bjfpz/V+wjTvO/ABjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz2ADm/Kv3AY6W9Bgfpr+36L+UJLV+ZcVXDL/TX1P0f6ZfugdQDJN7gLmSHuN8zq99kWP4wwYwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwh3sAc9gA5kzeB5j2vfwrRf+yJKn1FV2KQekfK/oPJKm17mRJj/Fu+seL/v30Nxf9L+kfKPrP2ADmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmcA9gDhvAnMk9wPaKHDF+kCS1/kxpegwL6Z8q+nfSP1L0H0lSa13p80eMC+mX7hEixsk9wpai/zn9+aI/xwYwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwh3sAc9gA5kz+L+BQyY7hSfpri/5y+ruK/tv0NxX9r+nvLPrv0u+K/ihN93t++o+L/mE2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjBncg+wr2TH8EKSWuuul/QYL+T80v8VKIal9PcX/efpl97rVwyT9/pflfQYZ9M/V/RvSpJav6fiK4bXbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcw5y/FLQTlu3roJwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "Epoch 830 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 831 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 832 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 833 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 834 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 835 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 836 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 837 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 838 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 839 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.42\n",
      "Epoch 840 \t reward: 4.0 \t length 3 \t mean_length 1.7 \t epsilon 0.42\n",
      "Epoch 841 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.42\n",
      "Epoch 842 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.42\n",
      "Epoch 843 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.42\n",
      "Epoch 844 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.42\n",
      "Epoch 845 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.42\n",
      "Epoch 846 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.42\n",
      "Epoch 847 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.42\n",
      "Epoch 848 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.42\n",
      "Epoch 849 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.42\n",
      "Epoch 850 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.42\n",
      "Epoch 851 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.42\n",
      "Epoch 852 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.41\n",
      "Epoch 853 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.41\n",
      "Epoch 854 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 855 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 856 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 857 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.41\n",
      "Epoch 858 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 859 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.41\n",
      "Epoch 860 \t reward: 4.0 \t length 3 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 861 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 862 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.41\n",
      "Epoch 863 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.41\n",
      "Epoch 864 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.41\n",
      "Epoch 865 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 866 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 867 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 868 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 869 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 870 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 871 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 872 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 873 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.41\n",
      "Epoch 874 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 875 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 876 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 877 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.41\n",
      "Epoch 878 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 879 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.41\n",
      "Epoch 880 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.4\n",
      "Epoch 881 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.4\n",
      "Epoch 882 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.4\n",
      "Epoch 883 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.4\n",
      "Epoch 884 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.4\n",
      "Epoch 885 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 886 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 887 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 888 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.4\n",
      "Epoch 889 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.4\n",
      "Epoch 890 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 891 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 892 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.4\n",
      "Epoch 893 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.4\n",
      "Epoch 894 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.4\n",
      "Epoch 895 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 896 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 897 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.4\n",
      "Epoch 898 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 899 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 900 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.4\n",
      "Epoch 901 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 902 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 903 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 904 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.4\n",
      "Epoch 905 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 906 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 907 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.4\n",
      "Epoch 908 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.4\n",
      "Epoch 909 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.39\n",
      "Epoch 910 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.39\n",
      "Epoch 911 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.39\n",
      "Epoch 912 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 913 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 914 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 915 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 916 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 917 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 918 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 919 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 920 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 921 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 922 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 923 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 924 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 925 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 926 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 927 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 928 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 929 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.39\n",
      "Q_matrix of initial state, after training: step 801\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671970>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADI0lEQVR4nO2csYvPcRiA34/8A5QMV7+bDSK5RSxMTuqKQWLhMt6lxEZnIyVGHQvJQClhYjlZTiKD+futG6Tcf3Afy/v7B94zeZ5nf3qXp3f5vt9P633oIVh2RkREmz1RsvvwIf3lov8g/cdF/3L6e4v+r/TfFf359OeK/nr6G0V/Jv3For+6oyTKf4MBwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgND8Hs3EDwJneA5wu2X14k/65ov8iIqK1SSnE3setnL9VnL8j/VtFfyX9A0X/e/r3iv619B8W/SU3ABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHA8R4AjhsAzvQe4G3J7sOp9C8U/WcREa1NdpX0Pm7m/GfF+Rdy/v3i/Kvp7y76fyIios1W9Ig+RPqToj+6AeAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjvcAcNwAcKb3AAsluw+v018r+sfSf170z6f/tegfSr90jxB92IyIaG1S+j+/93Ep5+8pzv+d/kzR33ADwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAc7wHguAHg/Kv3ATaL/q6IiNaKv7f3MXL+2eL8l+kfKfqf0z9V9N+m/6ron0n/QdFfdgPAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOAYABzvAeC4AeBM7wG2+3/6p6J/NP3DRf9LRERrk5WS3sdbOf9kcf779H8U/f3pXyn6j9J/WvQvugHgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI73AHDcAHCm9wCLJbsPq+kvFP3X6W/rHiDa7PWifzf9uaK/nv67oj+f/sGi/y39O0X/hhsAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOB4DwDHDQBneg+wr2T34WdERGuT0oP/vY9jzj9enP8x/SdF/1L6t4v+zfRL7xNEH6bvExwr+msREa1NZkp6HzfcAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAx3sAOH8BNl/q1naIRAYAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671520>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJklEQVR4nO2cr2tWcRSHz9cfIKvyahDumFVBVBYsikGDCDNMRJBpMIgsvGntTWtLbxhiMOgQRFxwIAYNosUwVAStjnvBoC/WIYh8DZ77D3xm8vM8/bmnPJxyD99Sa1sDbNkTERFl+rxk1/ZF+ouiv5r+XdG/lf5u0f+d/proL6R/RPQ/p78t+lPpXxf9B7skEf4bCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCr+DvWEDmNPfA1yS7No+Tf+K6D+OiCilmZL02m3n/B/i/P3pj0R/Of2B6E/SXxb9UfryPQUbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5jT3wM8l+zaXkh/QfTXIiJKaU5Jeu3e5vxH4vyrOV/Taxfpz4r+5t8PTEvzo7aR/ozob7EBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDn9PcBlya7tk/Rfi/6Z9B+K/rX0P4n+0fQPiv63iIhSmp+SXrt9OV96HyFq27+PcEj0v7IBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDn/6n2AiegPIiJKaX5Jeu325vzb4vw76R8T/Y/pnxP9l+lviP5c+mPRH7IBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDn9PcABya7t9/S/iP7h9OdEfyMiopTmmaTX7mLOPyvOf5X+O9E/mf4N0b+f/qroL7IBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDn9PcBQsms7Tn9e9NfT3+n/+CXRX0n/tOi/SX9T9GfTnxH9rfRHor/MBjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz2ADm9PcAJyS7tu8jIkppbkp67e7l/J2+t78u+vPpr4j+UvpD0R+nf1z0P0RElNIMJL12EzaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmPMH6Cfq1v5yAh0AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671610>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADP0lEQVR4nO2dsauOYRiHf7dOxMKgJM77EsmgLExOGSgxOMooq5TMymAwKLOUrDIqx+CkGNQxOYsySMT7HpIysBwxuA2e7x+4vzP5Xdd+fc83XN3Lc3/PF5lDCmyZkSRFv75k5/BbkiK6IyU9xxfNP1v0HzR/qejPNX+l6M82/2LRvyNJin5vxVcO79r5J4rnP1lXOhj+GwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc4LrYG+YAOas1T7A9pKe45fmby3635r/tujva/7Bov+q+RVdmaOaf6Xo3/z3Af2m2hcYVpkA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5rAPYA4TwJwZSYroShfamWO70O5rIeXwp51/unj+o+aX7sMzx1VJUvSlfQblMNlnmPZ9hMNF/6UkKfqNFV85/GQCmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM+gDlMAHMm7wOU75MlKaI7WtJzfN78Q0V/WdLU7xso+i1F/3vz9xf9N5IU0V0q6Tnebv6Gov+LCWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO+wDmMAHMmbwPMFuRM8eV5p8r+vclSdFXdCkHNX+u6C9J07/XH9GdLPqLktbifYUDxfNfMwHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMYR/AHCaAOZP3Aaa6j1b0x4v+0+afKfoPm3+q6D+WpIjufEnP8V7zp/19/3zRX2j+QtGfZwKYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz6AOUwAcyb7ALUf6OcwNH/a9/r3FP33khTR7SzpOX5q528rnv+1+aX/O1AOy5IU0d0t6TleaP6ton+ZCWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO+wDmMAHMmewD7CjZOXyWpIjuRknP8Wo7f9p9gl1F/2PzNxf9H5IU0T0r6Tkea/71on9NkhT97oqvHD4wAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMxhH8CcvzQ4BeV3JNLMAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 930 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 931 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 932 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 933 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 934 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 935 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 936 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.39\n",
      "Epoch 937 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 938 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.39\n",
      "Epoch 939 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 940 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 941 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 942 \t reward: 4.0 \t length 3 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 943 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 944 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 945 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 946 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 947 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 948 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 949 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 950 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 951 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 952 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.38\n",
      "Epoch 953 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 954 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 955 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 956 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 957 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 958 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 959 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 960 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.38\n",
      "Epoch 961 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.38\n",
      "Epoch 962 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 963 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 964 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 965 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 966 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.38\n",
      "Epoch 967 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.38\n",
      "Epoch 968 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.38\n",
      "Epoch 969 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.37\n",
      "Epoch 970 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 971 \t reward: 4.0 \t length 3 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 972 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 973 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 974 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 975 \t reward: 4.0 \t length 3 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 976 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 977 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 978 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 979 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.37\n",
      "Epoch 980 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 981 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 982 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 983 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 984 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 985 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.37\n",
      "Epoch 986 \t reward: 4.0 \t length 3 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 987 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 988 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 989 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.37\n",
      "Epoch 990 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 991 \t reward: 4.0 \t length 3 \t mean_length 1.66 \t epsilon 0.37\n",
      "Epoch 992 \t reward: 4.0 \t length 3 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 993 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.37\n",
      "Epoch 994 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.37\n",
      "Epoch 995 \t reward: 4.0 \t length 3 \t mean_length 1.66 \t epsilon 0.37\n",
      "Epoch 996 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.37\n",
      "Epoch 997 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.37\n",
      "Epoch 998 \t reward: -0.5 \t length 1 \t mean_length 1.62 \t epsilon 0.37\n",
      "Epoch 999 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.37\n",
      "Epoch 1000 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.37\n",
      "Epoch 1001 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.36\n",
      "Epoch 1002 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.36\n",
      "Epoch 1003 \t reward: -0.5 \t length 1 \t mean_length 1.64 \t epsilon 0.36\n",
      "Epoch 1004 \t reward: 1.5 \t length 2 \t mean_length 1.64 \t epsilon 0.36\n",
      "Epoch 1005 \t reward: 1.5 \t length 2 \t mean_length 1.66 \t epsilon 0.36\n",
      "Epoch 1006 \t reward: 4.0 \t length 3 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1007 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1008 \t reward: -0.5 \t length 1 \t mean_length 1.66 \t epsilon 0.36\n",
      "Epoch 1009 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1010 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1011 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1012 \t reward: 1.5 \t length 2 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1013 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.36\n",
      "Epoch 1014 \t reward: -0.5 \t length 1 \t mean_length 1.68 \t epsilon 0.36\n",
      "Epoch 1015 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.36\n",
      "Epoch 1016 \t reward: 1.5 \t length 2 \t mean_length 1.7 \t epsilon 0.36\n",
      "Epoch 1017 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.36\n",
      "Epoch 1018 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.36\n",
      "Epoch 1019 \t reward: 4.0 \t length 3 \t mean_length 1.72 \t epsilon 0.36\n",
      "Epoch 1020 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.36\n",
      "Epoch 1021 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.36\n",
      "Epoch 1022 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.36\n",
      "Epoch 1023 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.36\n",
      "Epoch 1024 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.36\n",
      "Epoch 1025 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.36\n",
      "Epoch 1026 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.36\n",
      "Epoch 1027 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.36\n",
      "Epoch 1028 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.36\n",
      "Epoch 1029 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.36\n",
      "Q_matrix of initial state, after training: step 901\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671610>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADHUlEQVR4nO2dv6vNcRyH319kFFkYHHUNShbdLCaUIj/STcxCikhRKCWlUJToKiEz6SY/ckthskgWKQP1PQYWkVH0sbzPP/A+m+d59qdPp57ey3l1Ttda30KwzIuIiG75lpLd+mfpnyz6l9K/XfT3p7+y6H9M/2XR35D+5qI/m/7noj+R/oGif2tOSZT/BgOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcDp/DqYjRcAzmgPsKtkt/5B+nuL/t305xb9v+mX9Gh9pH+56J9If33Rf5X+haJ/Ov0rRf+4FwCOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4LgHgOMFgDPaA8yU7NZPpX+w6N+MiOi6wbyS3oZ/8v07xff35ftHiu9fT39B0f8VERHd8t8VP1o/P/0VRf+TFwCOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4LgHgOMFgDPaA0yV7NbPpP+66K9L/17R35P+26I/mf6Sov8tIqLrBsdLehteyfeXFt//mv5E0f/sBYBjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOO4B4HgB4Iz2ALMlu/Wb0/9e9BdHRHTdoKa3YeT74/7fwdai/zT9HUX/Ufrj/j7DdNE/7AWAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADjuAeB4AeCM9gDLSnbrv6T/pOhvS3+y6L+NiOi6wdGS3obX8v1x9wDvi/7q9A8X/en0y3sCLwAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwHEPAMcLAGe0B9hdslt/P/3tRf9x+muK/rv0TxX9i+mvLfpv0n9Y9Hemv7Hov0j/atE/5gWAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADjuAeB4AeCM9gCrSnbrP0REdN1gYUlvw5/5/qbi+8/Tv1H0D6V/tuifS/980T+T/lifv+sGi0p6G/7wAsAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHPcAcP4Bq1bp1oF1o5IAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671E50>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADK0lEQVR4nO2crWtQYRSHz+u0GgwiCHcggqvKkmgYbmARhkEQ5kfacAguKAgGZxAEDRNkMpMfA8EgA4uwyYJiGloniPBeEMRgsMp8Lef+A7/o73n6w4HLwyn38JbWaguwZXdERJTRacludS39G6J/P/0noj+b/i7R/5v+M9G/lP646G+lvyP6I+lfEP0X2oeD/wYCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwJzC72Bv2ADmDPcAFyW71efpXxb9pxERpXR7Jb31v3P+L3H+vvQfiP719PeL/s/0b4n+3fRXRH+ODWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHMGe4B1iW71an0r4r+o4iIUrpjkt76Tzl/VZw/k/OlLdhaX9I/K/qvIyKijCp6RKuR/pjob7MBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzOEewBw2gDnDPcB5yW71ZfofRP9E+q9E/1z6X0X/cPqHRP9bREQpnfTef2v98N7/HnH+n/S1g4JWKxvAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHO4BzGEDmDPcA2xIdquT6f8Q/QMREaV0mt76yPl3xPm30z8u+h/TPyX679J/I/pn0n8s+lfYAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc4R5A/CFf+/S/i/7B9OdEfyUiopRuU9JbP5HzJ8T5m+lvif54+rOi/yT9ZdGfZwOYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz2AOWwAc4Z7gGuS3erD9GdEfzX906L/Nv2bon8v/UnR30j/i+gfSX9M9LfTXxT9RTaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMGMGe4Bzgp2a2+j4gopVuQ9NYv5fwpcf56+muiP53+kugvpD8v+svpHxX9z+mPiP4OG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHM+Qe8S+nW5J7X3AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671820>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADRUlEQVR4nO2dTYtOcRiHf39GTUpqbEZ0zoLUJFlOXhZmo2xFKT6EKKSslFDkQ1CKbJXNWHhpltI0JRbnRGwoTWlK3BZzP1/gnln5Xdf+ev6nnqt7c+5zTosYQmDLlCSp9VtKdgx/Jam1br6kx7iU/smi/yL9D0X/QPrfi/6u9O8U/auSpNbPVnzF8C3PP108/1ntj4f/BgIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAnMbtYG+YAOZM9gGmS3YMa5LUWjdX0mNcSf9Q0X+f/mLRX0h/o/sMFV0Ro9K/XPTvrf9Av7V2AcMfJoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA57AOYwwQwZ7PeD7C9pMf4K/2zRf9J+jNF/4ckqfV7Kr5i+JLnny+e/yj940X/lSSp9dsqvmL4zQQwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwh30Ac5gA5kz2AcrPl0ubcj//WNF/LWnD16/W7yj6q+nvL/ofJam17kpJj/Fu+qV9hojxCxPAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHPYBzGECmDMlSa11hytyxPgu/WtF/7YkqfUVXYpB6R8t+m8kqbXuRkmP8Wb6F4r+w/T3Ff1P6R8s+stMAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHPYBzCHCWDOZB9guiJHjGuSpNbPl06PYSn9U0X/efoniv5LSWqtO1PSY3ya/vWifyv9haK/mP7bon+ECWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO+wDmMAHMmXwvYG/JjuFz+juL/s/0u6I/SlJr3VxJj3Elz58pnv8j/dLz+YphWZJa6x6X9BjPpX+/6F9iAphDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPoA5TABzJvsAu0t2DF8lqbXuQUmP8WL6s0X/mySp9SVfMUz8HUV/VZJa6xZLeowL6d8u+uvfadjAPgUTwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBz2Acz5BwBQCOXBa2piAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n",
      "Epoch 1030 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.36\n",
      "Epoch 1031 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.36\n",
      "Epoch 1032 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.36\n",
      "Epoch 1033 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1034 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1035 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1036 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1037 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1038 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1039 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1040 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1041 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1042 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1043 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.35\n",
      "Epoch 1044 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1045 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1046 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1047 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1048 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1049 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1050 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.35\n",
      "Epoch 1051 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1052 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1053 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1054 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1055 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1056 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1057 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1058 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1059 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1060 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1061 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1062 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.35\n",
      "Epoch 1063 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1064 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1065 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.35\n",
      "Epoch 1066 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.35\n",
      "Epoch 1067 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1068 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1069 \t reward: 1.5 \t length 2 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1070 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1071 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1072 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1073 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1074 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1075 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1076 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1077 \t reward: 1.5 \t length 2 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1078 \t reward: 1.5 \t length 2 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1079 \t reward: -0.5 \t length 1 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1080 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1081 \t reward: 1.5 \t length 2 \t mean_length 1.88 \t epsilon 0.34\n",
      "Epoch 1082 \t reward: -0.5 \t length 1 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1083 \t reward: 1.5 \t length 2 \t mean_length 1.88 \t epsilon 0.34\n",
      "Epoch 1084 \t reward: -0.5 \t length 1 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1085 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1086 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.34\n",
      "Epoch 1087 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.34\n",
      "Epoch 1088 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1089 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1090 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.34\n",
      "Epoch 1091 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.34\n",
      "Epoch 1092 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.34\n",
      "Epoch 1093 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1094 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1095 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.34\n",
      "Epoch 1096 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1097 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.34\n",
      "Epoch 1098 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1099 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.34\n",
      "Epoch 1100 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1101 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.34\n",
      "Epoch 1102 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1103 \t reward: 4.0 \t length 3 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1104 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1105 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1106 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1107 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1108 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1109 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1110 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1111 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1112 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1113 \t reward: 4.0 \t length 3 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1114 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1115 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1116 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1117 \t reward: 4.0 \t length 3 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1118 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1119 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1120 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1121 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1122 \t reward: -0.5 \t length 1 \t mean_length 1.74 \t epsilon 0.33\n",
      "Epoch 1123 \t reward: -0.5 \t length 1 \t mean_length 1.72 \t epsilon 0.33\n",
      "Epoch 1124 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.33\n",
      "Epoch 1125 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.33\n",
      "Epoch 1126 \t reward: -0.5 \t length 1 \t mean_length 1.7 \t epsilon 0.33\n",
      "Epoch 1127 \t reward: 1.5 \t length 2 \t mean_length 1.72 \t epsilon 0.33\n",
      "Epoch 1128 \t reward: 4.0 \t length 3 \t mean_length 1.74 \t epsilon 0.33\n",
      "Epoch 1129 \t reward: 1.5 \t length 2 \t mean_length 1.74 \t epsilon 0.33\n",
      "Q_matrix of initial state, after training: step 1001\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671760>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJ0lEQVR4nO2asYvPcRyH3x8ZTjIYDJbfUVIGF4OS+k0UcQOJcuU4xaaLy2DRZTHo6LqNcpw6RWI4oph+JWWgMygpvt/FYDBIbvtY3t9/4GXzep796bM8vZbPu9Ta1ABbVkdERBk+KNm1eZ7+VdG/kv4t0T+X/h7Rf5P+W9Hfnf4J0X+Q/rLoj6R/VvRvr5JE+G8gAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwpfAd7wwKY090DnJTs2txP/4zo30l/vej/TF/SozaR/rzoT6Q/JvqL6V8X/UvpT4v+NAtgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rAA5nT3APckuzan0p8U/dn014n+r/RviP7FiIhSen1Jr+0g/SHRX4mIiDL8R/GjNmvS3yT631gAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMIcFMKe7Bzgu2bV5mP470d+V/iPRP5b+QPT76W8W/a8REaX09kp6bV/n+xvF97+nv130P7IA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5nAPYA4LYE53D/BKsmuzL/0for8hIqKUnqbXNvL9Q+L7z9I/Lfp30x8X/YX0H4v+0fRnRX+SBTCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBzWABzunuArZJdm8/pz4n++fS3if6niIhSeoclvbZP8/0D4vsv0n8v+jvTvyD6N9NfEv1RFsAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQHM6e4B+pJdm0H6o6K/lP6I6C+n/6//6TtE/0P6C6I/nv6Y6C+mPy/6EyyAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMCmNPdA2yR7Np8iYgopTck6bVdyff3i++/TP+a6F9Of0r0Z9KfEf2p9I+I/pOIiFJ6ayW9tr9ZAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHO4BzDnL+7k6NbaSp3bAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671820>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADL0lEQVR4nO2cQYtOcRSHz19TIgt8gPuuULxNSrJSFgglWcyCshDKmKKMhcVkoVlYGEXNjEIWisUsJIWwUFaS0jQUVvd+ACxESv0tnPsFfrPze579c89dPJ3NPe9bam1rgC0jERFRBoclu7YP058S/en0Z0R/MiKilGa7pNfubc6/Is6/mP5W0X+fvqRHbSP9MdFfWKFNhv8FAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMCcwudgb9gA5vT3AOOSXdv59I+J/r30R0T/T/qLoj+a/k3RP53+GtH/kf5Z0b+R/lXRv8AGMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId7AHPYAOb09wCPJbu2B9OfFP2ZiIhSmqGk124p518X55/L+Zpeu0h/VvQn/j1g8FN7gXZ1+ptF/yMbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5jT3wOclOza3k7/lejvSv++6B9Nf0n0h+nvEP03ERGlNL8kvXar0t8p+q8jIqIMNip+1PYzG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOY098DvJTs2u5O/5Pob4qIKKX5Lem1W5nzl/t//8u6B4gy2CP6L9J/JPqH0p8X/XE2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjCnvwcYlezaLqb/TfTXpX9N9M9HRJTSdJJeuybnbxPnv0v/iegfSP+46N9N/5LoX2YDmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHP6e4BTkl3bW+mfEP076R8R/QfpT4j+bPr7Rf9p+l9Ff336W0T/Q/pToj/NBjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz2ADm9PcAeyW7ts8jIkppvkh67Tbk/H3i/GfpL/f3+XOifyb9MdFfSH8o+kvprxX972wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMOcvls7q1jl0G+8AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671580>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADUUlEQVR4nO2dwYuNYRTGn2OuFcpENAvfFwslKWkyK7FUilgoZWWN7CyMlbGwE9ZWSlkQpSzJajRJScqC3s9iMtEorMw4Fs79Bx5Wnue3/933q/vrbO55vxuZLWFkGQFARLeGkTOHX+XPkP58+cdJ/0H5i6Q/Vf530l9f/nXSvwAAiH6S8ZFtuc4/Qp7/mPrizf+DAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4oR/DtbGE0CcEQAg+gnKzrYKABHdbkrP4U35u0j/bfkLpD9d/iHSf1o+oyNzQPmzpD/35wP64B6gpSeAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjfQBxPAHEGe8DcCFkG78fYBOl5/Cl/GOk/7D8baT/EQAQ/RTjI9tinX+SPP9e+YdJ/wkAIPq1jI9sPz0BxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAc7wOI4wkgzngfYETZ2VYAIKI7Rek53C3/IOk/K596/sxhBQAQ/QbGR7Zv5e8g/fcAENFdofQcLpe/nfQ/eAKI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDjeBxDHE0CcEQBEdDOMnDnMl3+R9K8BAKJndCAbyt9D+q8BIKK7Rek5nC3/DOnfLn8n6b8rfwvpL3kCiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA43gcQxxNAnPH7ASYoO9tq+ftI/2X5B0j/efnTpL8AABHdCUrP4X75c6Q/W/5R0n9U/ivS3+sJII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDieB9AHE8Accb7ANT9cmRbKn8d6f8of4r0FwEgoqP2ETKH8T7C3/5fAHW/H9nG9/vvUHoOp8u/QfrnPQHEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBzvA4jjCSDOeB9gM2Vn+wwAEd1NSs/hXPmTpL8M4F/sM2wk/a8AENG9oPQc9pd/lfQvAQCi38r4yPbJE0AcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxPE+gDi/AcQzCeV1avl6AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 1130 \t reward: 10.5 \t length 5 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1131 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.33\n",
      "Epoch 1132 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1133 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.33\n",
      "Epoch 1134 \t reward: -0.5 \t length 1 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1135 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1136 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1137 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.33\n",
      "Epoch 1138 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.32\n",
      "Epoch 1139 \t reward: 1.5 \t length 2 \t mean_length 1.76 \t epsilon 0.32\n",
      "Epoch 1140 \t reward: 4.0 \t length 3 \t mean_length 1.78 \t epsilon 0.32\n",
      "Epoch 1141 \t reward: 4.0 \t length 3 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1142 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1143 \t reward: 4.0 \t length 3 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1144 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1145 \t reward: -0.5 \t length 1 \t mean_length 1.78 \t epsilon 0.32\n",
      "Epoch 1146 \t reward: 4.0 \t length 3 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1147 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1148 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.32\n",
      "Epoch 1149 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.32\n",
      "Epoch 1150 \t reward: 7.0 \t length 4 \t mean_length 1.88 \t epsilon 0.32\n",
      "Epoch 1151 \t reward: 1.5 \t length 2 \t mean_length 1.88 \t epsilon 0.32\n",
      "Epoch 1152 \t reward: -0.5 \t length 1 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1153 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1154 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1155 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1156 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1157 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.32\n",
      "Epoch 1158 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.32\n",
      "Epoch 1159 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.32\n",
      "Epoch 1160 \t reward: -0.5 \t length 1 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1161 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1162 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1163 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1164 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1165 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1166 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1167 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1168 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1169 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.32\n",
      "Epoch 1170 \t reward: 1.5 \t length 2 \t mean_length 1.78 \t epsilon 0.32\n",
      "Epoch 1171 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1172 \t reward: 1.5 \t length 2 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1173 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.32\n",
      "Epoch 1174 \t reward: -0.5 \t length 1 \t mean_length 1.8 \t epsilon 0.32\n",
      "Epoch 1175 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.31\n",
      "Epoch 1176 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.31\n",
      "Epoch 1177 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.31\n",
      "Epoch 1178 \t reward: 1.5 \t length 2 \t mean_length 1.82 \t epsilon 0.31\n",
      "Epoch 1179 \t reward: 4.0 \t length 3 \t mean_length 1.82 \t epsilon 0.31\n",
      "Epoch 1180 \t reward: 4.0 \t length 3 \t mean_length 1.84 \t epsilon 0.31\n",
      "Epoch 1181 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.31\n",
      "Epoch 1182 \t reward: 1.5 \t length 2 \t mean_length 1.84 \t epsilon 0.31\n",
      "Epoch 1183 \t reward: 1.5 \t length 2 \t mean_length 1.86 \t epsilon 0.31\n",
      "Epoch 1184 \t reward: -0.5 \t length 1 \t mean_length 1.84 \t epsilon 0.31\n",
      "Epoch 1185 \t reward: -0.5 \t length 1 \t mean_length 1.86 \t epsilon 0.31\n",
      "Epoch 1186 \t reward: 4.0 \t length 3 \t mean_length 1.88 \t epsilon 0.31\n",
      "Epoch 1187 \t reward: 4.0 \t length 3 \t mean_length 1.88 \t epsilon 0.31\n",
      "Epoch 1188 \t reward: 1.5 \t length 2 \t mean_length 1.88 \t epsilon 0.31\n",
      "Epoch 1189 \t reward: 4.0 \t length 3 \t mean_length 1.9 \t epsilon 0.31\n",
      "Epoch 1190 \t reward: 1.5 \t length 2 \t mean_length 1.9 \t epsilon 0.31\n",
      "Epoch 1191 \t reward: 4.0 \t length 3 \t mean_length 1.92 \t epsilon 0.31\n",
      "Epoch 1192 \t reward: 7.0 \t length 4 \t mean_length 1.96 \t epsilon 0.31\n",
      "Epoch 1193 \t reward: 4.0 \t length 3 \t mean_length 1.98 \t epsilon 0.31\n",
      "Epoch 1194 \t reward: 1.5 \t length 2 \t mean_length 1.98 \t epsilon 0.31\n",
      "Epoch 1195 \t reward: 1.5 \t length 2 \t mean_length 1.98 \t epsilon 0.31\n",
      "Epoch 1196 \t reward: -0.5 \t length 1 \t mean_length 1.96 \t epsilon 0.31\n",
      "Epoch 1197 \t reward: -0.5 \t length 1 \t mean_length 1.98 \t epsilon 0.31\n",
      "Epoch 1198 \t reward: 4.0 \t length 3 \t mean_length 2.0 \t epsilon 0.31\n",
      "Epoch 1199 \t reward: 4.0 \t length 3 \t mean_length 2.02 \t epsilon 0.31\n",
      "Epoch 1200 \t reward: 4.0 \t length 3 \t mean_length 2.03 \t epsilon 0.31\n",
      "Epoch 1201 \t reward: 4.0 \t length 3 \t mean_length 2.06 \t epsilon 0.31\n",
      "Epoch 1202 \t reward: 4.0 \t length 3 \t mean_length 2.07 \t epsilon 0.31\n",
      "Epoch 1203 \t reward: -0.5 \t length 1 \t mean_length 2.08 \t epsilon 0.31\n",
      "Epoch 1204 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.31\n",
      "Epoch 1205 \t reward: -0.5 \t length 1 \t mean_length 2.08 \t epsilon 0.31\n",
      "Epoch 1206 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.31\n",
      "Epoch 1207 \t reward: 4.0 \t length 3 \t mean_length 2.08 \t epsilon 0.31\n",
      "Epoch 1208 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.31\n",
      "Epoch 1209 \t reward: 4.0 \t length 3 \t mean_length 2.1 \t epsilon 0.31\n",
      "Epoch 1210 \t reward: 4.0 \t length 3 \t mean_length 2.11 \t epsilon 0.31\n",
      "Epoch 1211 \t reward: 1.5 \t length 2 \t mean_length 2.14 \t epsilon 0.31\n",
      "Epoch 1212 \t reward: -0.5 \t length 1 \t mean_length 2.11 \t epsilon 0.31\n",
      "Epoch 1213 \t reward: 1.5 \t length 2 \t mean_length 2.12 \t epsilon 0.3\n",
      "Epoch 1214 \t reward: 1.5 \t length 2 \t mean_length 2.11 \t epsilon 0.3\n",
      "Epoch 1215 \t reward: 1.5 \t length 2 \t mean_length 2.12 \t epsilon 0.3\n",
      "Epoch 1216 \t reward: 4.0 \t length 3 \t mean_length 2.13 \t epsilon 0.3\n",
      "Epoch 1217 \t reward: -0.5 \t length 1 \t mean_length 2.14 \t epsilon 0.3\n",
      "Epoch 1218 \t reward: 1.5 \t length 2 \t mean_length 2.13 \t epsilon 0.3\n",
      "Epoch 1219 \t reward: 1.5 \t length 2 \t mean_length 2.16 \t epsilon 0.3\n",
      "Epoch 1220 \t reward: 10.5 \t length 5 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1221 \t reward: 1.5 \t length 2 \t mean_length 2.22 \t epsilon 0.3\n",
      "Epoch 1222 \t reward: -0.5 \t length 1 \t mean_length 2.19 \t epsilon 0.3\n",
      "Epoch 1223 \t reward: -0.5 \t length 1 \t mean_length 2.22 \t epsilon 0.3\n",
      "Epoch 1224 \t reward: 1.5 \t length 2 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1225 \t reward: -0.5 \t length 1 \t mean_length 2.2 \t epsilon 0.3\n",
      "Epoch 1226 \t reward: 1.5 \t length 2 \t mean_length 2.19 \t epsilon 0.3\n",
      "Epoch 1227 \t reward: 4.0 \t length 3 \t mean_length 2.22 \t epsilon 0.3\n",
      "Epoch 1228 \t reward: 4.0 \t length 3 \t mean_length 2.23 \t epsilon 0.3\n",
      "Epoch 1229 \t reward: 4.0 \t length 3 \t mean_length 2.22 \t epsilon 0.3\n",
      "Q_matrix of initial state, after training: step 1101\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671970>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADKUlEQVR4nO2dsYvOcRzHP189FrEYDNJzKWI0UG44ZbGIUkp3iiuKG2QhJ2VQcmKR4U5RR7lLKUUWi3LDKQYjUfr+ksFgIQv1Nfj8/oE3k/frtb/6PsOr9/J8ep7SWm0BtgwiIqKMjEl2q0vp3xT9U+k/EP1D6R8V/bvpfxL9DelfF/0z6T8U/YPpT4j+wgpJhP8GAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMCcwtfB3rAA5vT3AEcku9V76e8V/afp7xT9l+lLerQa6T8W/f3pXxP9s+nfEv0T6Y+L/iILYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAOb09wBXJbvVc+lfEP3L6e8W/efpnxT9uYiIUoabJb1179MfiP6viIgoI18VP1pdm34R/cYCmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDksgDn9PcAxyW71TvrvRH9L+ouiP57+guhPpL9D9F9FRJQy3CrprXub768R3/+W/gHRf8QCmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDksgDn9PcALyW51V/qfRX99REQpQ01vXeT7q8T3f6R/SfQvpj8j+tPpz4r+VPrnRf8KC2AOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsADm9PcA2yW71dfpbxL9DxERpQxXSnrrfqYvff7Wuv7zjyp+tLqc/pLoj6U/Lfoz6S+L/igLYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAOYMIiJKGUr/P99a92c9/v779HWi/yX9w6J/P33p9/6j1f73/k+L/o3050V/Mv03or+NBTCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBzWABz/tU9wEbp9VY/pr9P9J+kPyn68+nvEf1n6d8W/ePpT4n+bEREKcPVkt667yyAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmPMbXFjq1vVeZmAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671520>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADJ0lEQVR4nO2czYtPcRSHzxcbO1lN0R0bbGiSlSwQ0jRFoalZUZJJykuDpSwxeSlpJMVqakJR0ySEhawkscHGvVG/leys9LVw7j/wmZ3P8+yfzl08nc0995Za2xpgy7KIiCjDeyS7tk/TPyP6V9M/L/qXIiJKad5Keu225Px94vzH6W8Q/U/pS3rUNtLfKPofl2iT4X+BAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCn8DrYGzaAOf09wCnJru319DeJ/vv0h0R/kP410T+d/k3RPxERUUpzUNJr9yDn7xfnP0r/iOjfZQOYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz2AOWwAc/p7gHuSXdvD6S/2+/7lkl673zl/Upw/k/M1vXaR/kD0/91BlOEP2gO0I+mvEP1fbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gTn8PcFyya3sr/XnRH0t/WvSn0n8t+tvSHxf9uYiIUhrpfX6t3Uj6n0V/fUQs6v8MbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gTn8PMCfZtR1Pf1b0J9JfJfo/0r8j+kfTXyf6X9LfIfov058R/cn0L4v+OTaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMGMKe/B9gu2bV9lf430V+T/oLoj0ZElNLcl/TaHcr5K8X5P9OfEv3p9CdEfzb9A6L/kA1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzOnvATZLdm3fpX9S9G+kf0z0b6e/W/Sfpb9X9J+k/0f0l6a/VfTfpH9W9K+wAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhHsAcNoA5/T3ALsmu7fOIiFKaF5Jeu505f1Scv5D+vOiPpX9R9C+kv1r0v6c/JPqD9NeK/lc2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jzF3LP6tapRP3KAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978FC3FAC0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADMUlEQVR4nO2dsauPcRSHP19dlK4kRaT3jUUiCxksYrBICjOlWwa7xCADyW5QN8WMkiwGshjEIpKF3jcRJclNXW4dwz33Hzj3Tj7Psz+/8w5PZ3nP7/drEUMIbJmQJLW+leyMp7XuaEmP8UH6Z4v+jfSHot+nX9EVMSr9O0X/5PwH9JO1Bxhmcv6x4vz7y0qD4b+BAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCn8TrYGzaAOUt1D7C3pMf4Iv19Rf95+m+K/s70y+/T058r+hPpXyn6F9NfWfRn2QDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmcA9gDhvAnKW6B9ha0mP8kP5ifx9gY9H/Iklq/fqKrxi+5fwDxflP0z9V9G9Lklo/UfEVwxwbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5iz8P301RU5YvyV/umifyv9w0X/Ufqle4aIcX77tX5FxVcMf9LfXPQ/SVJr3VRJj3E6/f1F/xkbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5izcA9wpCJHjA/Tv170z0mSWl/RpRiUfu0DYhgkqbVuuqTHOJX+5aJ/Kf0tRf9j+suL/l82gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjBnSf4vQK3fVvTfp7+j6L9Nf3vRfydJrXUHS3qMT9K/WfTPpH+o6D9O/2XR38MGMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId7AHPYAOYs3ANMluwYZtJfVfR/p7+26P+QpNa63SU9xlc5f2Vx/mz6m4r+Z0lqrbtX0mM8nv7don+CDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHMWbgHWFOyY/gpSa11F0p6jFfT31D0v0pa9PMv9p6hte51SY9xV/rXiv55SVLr11V8xfCdDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDm/APcnwrlSPzUJQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 1230 \t reward: 1.5 \t length 2 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1231 \t reward: 1.5 \t length 2 \t mean_length 2.16 \t epsilon 0.3\n",
      "Epoch 1232 \t reward: 1.5 \t length 2 \t mean_length 2.15 \t epsilon 0.3\n",
      "Epoch 1233 \t reward: 1.5 \t length 2 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1234 \t reward: -0.5 \t length 1 \t mean_length 2.15 \t epsilon 0.3\n",
      "Epoch 1235 \t reward: 1.5 \t length 2 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1236 \t reward: 1.5 \t length 2 \t mean_length 2.17 \t epsilon 0.3\n",
      "Epoch 1237 \t reward: 1.5 \t length 2 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1238 \t reward: 4.0 \t length 3 \t mean_length 2.19 \t epsilon 0.3\n",
      "Epoch 1239 \t reward: 1.5 \t length 2 \t mean_length 2.2 \t epsilon 0.3\n",
      "Epoch 1240 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1241 \t reward: 1.5 \t length 2 \t mean_length 2.2 \t epsilon 0.3\n",
      "Epoch 1242 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1243 \t reward: 4.0 \t length 3 \t mean_length 2.22 \t epsilon 0.3\n",
      "Epoch 1244 \t reward: 1.5 \t length 2 \t mean_length 2.21 \t epsilon 0.3\n",
      "Epoch 1245 \t reward: -0.5 \t length 1 \t mean_length 2.22 \t epsilon 0.3\n",
      "Epoch 1246 \t reward: -0.5 \t length 1 \t mean_length 2.19 \t epsilon 0.3\n",
      "Epoch 1247 \t reward: -0.5 \t length 1 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1248 \t reward: 4.0 \t length 3 \t mean_length 2.19 \t epsilon 0.3\n",
      "Epoch 1249 \t reward: -0.5 \t length 1 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1250 \t reward: 1.5 \t length 2 \t mean_length 2.17 \t epsilon 0.3\n",
      "Epoch 1251 \t reward: 4.0 \t length 3 \t mean_length 2.14 \t epsilon 0.3\n",
      "Epoch 1252 \t reward: 4.0 \t length 3 \t mean_length 2.15 \t epsilon 0.3\n",
      "Epoch 1253 \t reward: 4.0 \t length 3 \t mean_length 2.18 \t epsilon 0.3\n",
      "Epoch 1254 \t reward: 1.5 \t length 2 \t mean_length 2.17 \t epsilon 0.29\n",
      "Epoch 1255 \t reward: 4.0 \t length 3 \t mean_length 2.18 \t epsilon 0.29\n",
      "Epoch 1256 \t reward: -0.5 \t length 1 \t mean_length 2.15 \t epsilon 0.29\n",
      "Epoch 1257 \t reward: -0.5 \t length 1 \t mean_length 2.16 \t epsilon 0.29\n",
      "Epoch 1258 \t reward: 4.0 \t length 3 \t mean_length 2.17 \t epsilon 0.29\n",
      "Epoch 1259 \t reward: 4.0 \t length 3 \t mean_length 2.2 \t epsilon 0.29\n",
      "Epoch 1260 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.29\n",
      "Epoch 1261 \t reward: 1.5 \t length 2 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1262 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1263 \t reward: -0.5 \t length 1 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1264 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1265 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1266 \t reward: -0.5 \t length 1 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1267 \t reward: 1.5 \t length 2 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1268 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1269 \t reward: -0.5 \t length 1 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1270 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1271 \t reward: -0.5 \t length 1 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1272 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1273 \t reward: 1.5 \t length 2 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1274 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1275 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1276 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.29\n",
      "Epoch 1277 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1278 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1279 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1280 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.29\n",
      "Epoch 1281 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1282 \t reward: -0.5 \t length 1 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1283 \t reward: 1.5 \t length 2 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1284 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1285 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.29\n",
      "Epoch 1286 \t reward: -0.5 \t length 1 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1287 \t reward: 1.5 \t length 2 \t mean_length 2.22 \t epsilon 0.29\n",
      "Epoch 1288 \t reward: 4.0 \t length 3 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1289 \t reward: 1.5 \t length 2 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1290 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1291 \t reward: -0.5 \t length 1 \t mean_length 2.24 \t epsilon 0.29\n",
      "Epoch 1292 \t reward: 1.5 \t length 2 \t mean_length 2.23 \t epsilon 0.29\n",
      "Epoch 1293 \t reward: 4.0 \t length 3 \t mean_length 2.2 \t epsilon 0.29\n",
      "Epoch 1294 \t reward: -0.5 \t length 1 \t mean_length 2.17 \t epsilon 0.29\n",
      "Epoch 1295 \t reward: -0.5 \t length 1 \t mean_length 2.18 \t epsilon 0.28\n",
      "Epoch 1296 \t reward: 4.0 \t length 3 \t mean_length 2.19 \t epsilon 0.28\n",
      "Epoch 1297 \t reward: 1.5 \t length 2 \t mean_length 2.22 \t epsilon 0.28\n",
      "Epoch 1298 \t reward: 1.5 \t length 2 \t mean_length 2.21 \t epsilon 0.28\n",
      "Epoch 1299 \t reward: 1.5 \t length 2 \t mean_length 2.2 \t epsilon 0.28\n",
      "Epoch 1300 \t reward: -0.5 \t length 1 \t mean_length 2.17 \t epsilon 0.28\n",
      "Epoch 1301 \t reward: 4.0 \t length 3 \t mean_length 2.16 \t epsilon 0.28\n",
      "Epoch 1302 \t reward: 4.0 \t length 3 \t mean_length 2.17 \t epsilon 0.28\n",
      "Epoch 1303 \t reward: -0.5 \t length 1 \t mean_length 2.16 \t epsilon 0.28\n",
      "Epoch 1304 \t reward: 4.0 \t length 3 \t mean_length 2.17 \t epsilon 0.28\n",
      "Epoch 1305 \t reward: 1.5 \t length 2 \t mean_length 2.18 \t epsilon 0.28\n",
      "Epoch 1306 \t reward: -0.5 \t length 1 \t mean_length 2.15 \t epsilon 0.28\n",
      "Epoch 1307 \t reward: -0.5 \t length 1 \t mean_length 2.16 \t epsilon 0.28\n",
      "Epoch 1308 \t reward: 4.0 \t length 3 \t mean_length 2.17 \t epsilon 0.28\n",
      "Epoch 1309 \t reward: 4.0 \t length 3 \t mean_length 2.18 \t epsilon 0.28\n",
      "Epoch 1310 \t reward: -0.5 \t length 1 \t mean_length 2.15 \t epsilon 0.28\n",
      "Epoch 1311 \t reward: 4.0 \t length 3 \t mean_length 2.14 \t epsilon 0.28\n",
      "Epoch 1312 \t reward: 1.5 \t length 2 \t mean_length 2.13 \t epsilon 0.28\n",
      "Epoch 1313 \t reward: -0.5 \t length 1 \t mean_length 2.16 \t epsilon 0.28\n",
      "Epoch 1314 \t reward: 1.5 \t length 2 \t mean_length 2.15 \t epsilon 0.28\n",
      "Epoch 1315 \t reward: 1.5 \t length 2 \t mean_length 2.16 \t epsilon 0.28\n",
      "Epoch 1316 \t reward: -0.5 \t length 1 \t mean_length 2.13 \t epsilon 0.28\n",
      "Epoch 1317 \t reward: -0.5 \t length 1 \t mean_length 2.12 \t epsilon 0.28\n",
      "Epoch 1318 \t reward: 4.0 \t length 3 \t mean_length 2.13 \t epsilon 0.28\n",
      "Epoch 1319 \t reward: 4.0 \t length 3 \t mean_length 2.14 \t epsilon 0.28\n",
      "Epoch 1320 \t reward: 1.5 \t length 2 \t mean_length 2.13 \t epsilon 0.28\n",
      "Epoch 1321 \t reward: 1.5 \t length 2 \t mean_length 2.08 \t epsilon 0.28\n",
      "Epoch 1322 \t reward: -0.5 \t length 1 \t mean_length 2.05 \t epsilon 0.28\n",
      "Epoch 1323 \t reward: -0.5 \t length 1 \t mean_length 2.08 \t epsilon 0.28\n",
      "Epoch 1324 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.28\n",
      "Epoch 1325 \t reward: 4.0 \t length 3 \t mean_length 2.08 \t epsilon 0.28\n",
      "Epoch 1326 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.28\n",
      "Epoch 1327 \t reward: 1.5 \t length 2 \t mean_length 2.08 \t epsilon 0.28\n",
      "Epoch 1328 \t reward: 1.5 \t length 2 \t mean_length 2.07 \t epsilon 0.28\n",
      "Epoch 1329 \t reward: -0.5 \t length 1 \t mean_length 2.06 \t epsilon 0.28\n",
      "Q_matrix of initial state, after training: step 1201\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADMklEQVR4nO2cT4tOYRiH72d6/ctX4D0LWykRJTYWFDWUzYyFxaQU2bChSKLYsBGlNItZmNko3qJY2JAiUrK1eA5fQf5N81i4zxf4zc7vuvbX+5xO13tvnrtTWqstwJZRRESUbq9kt/o6/QXRP5H+oujPpn9R9G+kvyz6w/ubiP50+vOiP5f+tOhPpiQR/hsIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMK18HeMAHMGe6zT0p2qw/SPyD6L9I/JPrP0pf0aDXSfyT6x9Jf7T7AfdE/lb78/pkA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5rAPYA4TwJxhH+CCZLd6M/3ron8p/RnRX0r/oOg/j4goZbxV0lv/Of0Nov8jIiJK90Hxo9Udef5IPH+ZCWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO+wDmMAHMGfYBTkt2q/fS/yL6W9J/IvpH0r8t+ufS3yf6ryIiShlvk/TWf0pf+iO21q9ERETpzih+tHqXCWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO+wDmMAHMGfYB3kh2q3vSr6LfRUSUMtb01kf660X/578f6Ja0B6gz6a/2e/+XRf9a+mdF/w4TwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBz2AcxhApgz7APsl+xWX0ZElDJeI+mt/5P+OtH/lf5O0X8fERGl26740erH9B+L/tH0r4r+lfTl7zMwAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMxhH8AcJoA5o4iIUsZrFbm1/ndERJRut3R6q2/TH4n+cvqHRf9p+ptE/1v6x0X/YfrvRH9X+iuiP8UEMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId9AHOYAOYM+wDSfXxr/XAfv1k6vdWv6c+J/nz606I/SX+1z78o+rPpnxf9WxERpYw3SnrrvzMBzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzGEfwJy/fJTu1ko/0s0AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADLUlEQVR4nO2cv6uOcRiH769OljOcyUI97yoDm4RO+ZFBhmMQRSQLyeQYDnU6ncLgmCQWiSgyOIMM8qNOSDYGWb9PsZjOcBapr+Hczz/wOSaf69qv9357u7qX536f0lptAbaMRUREGU1KdqtL6V8S/RvpnxH9exERpXTHJL31T3L+PnH+m/S3i/7n9CU9Wo30J0R/eZ02Gf4XCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCo+DvWEDmDPcA0xLdqsL6W8S/R/pd6Lfp39O9O+kf1b070ZElNJtlPTW/8z5u8T5H9I/JPov2ADmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmcA9gDhvAnOEe4Kpkt3ol/QuifysiopRuTNJb/yfnr+n/+aWI5wgtzxFKtyj6U6sfMFrQvkCdTr+IfmMDmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHOGe4Dzkt3q7fSfiv7R9K+L/kz6j0T/RPpHRP9ZREQp3UdJb/3O9GdEf/V3K6M9ih+tvmMDmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDlsAHOGe4CHkt3qyfTX+n6BHaL/Kf050Z9Lf5vof/lH/mXRv5b+vOjPsgHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOcM9wAHJbvVV+m9Ff2/6S6I/GRFRSjcr6a2fz/nrxfm/098t+u/TX9P7CaKMDov+czaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMGMGe4B5iQ7FaX078o+jfTPy76j9PfLPrf098v+q/TXxH98fQPiv7L9Hk/AGgQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc4R5gq2S3+jUiopTutKS3/n7OX+s9wAPRP5X+lOgvRkSU0m2Q9Nb/yvnj4vyV9LeI/jc2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jzFwGz69ZerPkbAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671B80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADR0lEQVR4nO2dT4uOcRSG76Mxg8liFhLpeWJqPoEUpSxQsvEnZWFtZSRF+QSKkoyVtYXSYCOFhVKUfIKpoeeXSBaz0PgzJsdizvsF7mblvq/91amn6z2b5/Q+kTkkjCxjABDRTTJyZlsu/zTpz5d/kfTvlL9I+tPlMzoyG8qfJ/215xY99fyRw+j5nyHnP9xADTb/DQ5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYgTfh2sjTeAOKN7gDFGzmyr5R8k/dflHyL9V+UvkP5M+ZdI/3b5K6Q/Xv510r9W/gTp//YGEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxfA8gjjeAOOv1/wB7Sf99+VdI/2b5e0j/AwAg+inGRw5LNf8oOf95+ZdJ/xYAIHrqngM5rHoDiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4vgcQxxtAnNE9wDZGzmzfyj9P+vfKP0X6j8rfSPp/AADRcz+EHP6Wv4P0vwBARHeS0rM9Lv8I6b/wBhDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcXwPII43gDije4ATjJzZnpQ/R/qzAIDoGR3IAeXvJP3PABDRPaD0bGfLv0v6F8rvSL8BAKIPxkcO6Q0gjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4HkAcbwBxRvcAmxk5s/0EAES/m5qew8fyp0l/sfwZ0l8AgIjuAKVne1P+fdI/V/5x0n9a/lvS3+8NII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDi+B5AHG8Acda+Ox/9JsrO4dc6+VtJ/zsARHT7KD3bu5o/Ts5fKZ/63gJyGH1v4RmlZztW/kvSP+wNII4DEMcBiOMAxHEA4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDi+B5AHG8AcUb3AFsoO4cfABDRzVJ6trnyd5H+JwBA9JOMjxyWy58g/d8AENF9pfRs28u/QfpXAQDRTzE+cljyBhDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxHEA4jgAcXwPIM4/cRIN5drsUFYAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 1330 \t reward: 1.5 \t length 2 \t mean_length 2.05 \t epsilon 0.28\n",
      "Epoch 1331 \t reward: 1.5 \t length 2 \t mean_length 2.06 \t epsilon 0.28\n",
      "Epoch 1332 \t reward: 1.5 \t length 2 \t mean_length 2.05 \t epsilon 0.28\n",
      "Epoch 1333 \t reward: 4.0 \t length 3 \t mean_length 2.06 \t epsilon 0.28\n",
      "Epoch 1334 \t reward: -0.5 \t length 1 \t mean_length 2.03 \t epsilon 0.28\n",
      "Epoch 1335 \t reward: 4.0 \t length 3 \t mean_length 2.06 \t epsilon 0.28\n",
      "Epoch 1336 \t reward: 1.5 \t length 2 \t mean_length 2.05 \t epsilon 0.28\n",
      "Epoch 1337 \t reward: -0.5 \t length 1 \t mean_length 2.06 \t epsilon 0.28\n",
      "Epoch 1338 \t reward: 1.5 \t length 2 \t mean_length 2.05 \t epsilon 0.28\n",
      "Epoch 1339 \t reward: 4.0 \t length 3 \t mean_length 2.04 \t epsilon 0.27\n",
      "Epoch 1340 \t reward: 4.0 \t length 3 \t mean_length 2.05 \t epsilon 0.27\n",
      "Epoch 1341 \t reward: 4.0 \t length 3 \t mean_length 2.04 \t epsilon 0.27\n",
      "Epoch 1342 \t reward: 1.5 \t length 2 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1343 \t reward: 1.5 \t length 2 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1344 \t reward: 1.5 \t length 2 \t mean_length 2.01 \t epsilon 0.27\n",
      "Epoch 1345 \t reward: -0.5 \t length 1 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1346 \t reward: 1.5 \t length 2 \t mean_length 2.01 \t epsilon 0.27\n",
      "Epoch 1347 \t reward: 4.0 \t length 3 \t mean_length 2.04 \t epsilon 0.27\n",
      "Epoch 1348 \t reward: 1.5 \t length 2 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1349 \t reward: 4.0 \t length 3 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1350 \t reward: 4.0 \t length 3 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1351 \t reward: 1.5 \t length 2 \t mean_length 2.04 \t epsilon 0.27\n",
      "Epoch 1352 \t reward: 4.0 \t length 3 \t mean_length 2.05 \t epsilon 0.27\n",
      "Epoch 1353 \t reward: 1.5 \t length 2 \t mean_length 2.04 \t epsilon 0.27\n",
      "Epoch 1354 \t reward: 4.0 \t length 3 \t mean_length 2.05 \t epsilon 0.27\n",
      "Epoch 1355 \t reward: 1.5 \t length 2 \t mean_length 2.06 \t epsilon 0.27\n",
      "Epoch 1356 \t reward: -0.5 \t length 1 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1357 \t reward: -0.5 \t length 1 \t mean_length 2.06 \t epsilon 0.27\n",
      "Epoch 1358 \t reward: -0.5 \t length 1 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1359 \t reward: 4.0 \t length 3 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1360 \t reward: 4.0 \t length 3 \t mean_length 2.03 \t epsilon 0.27\n",
      "Epoch 1361 \t reward: 1.5 \t length 2 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1362 \t reward: 1.5 \t length 2 \t mean_length 2.01 \t epsilon 0.27\n",
      "Epoch 1363 \t reward: 4.0 \t length 3 \t mean_length 2.02 \t epsilon 0.27\n",
      "Epoch 1364 \t reward: 7.0 \t length 4 \t mean_length 2.05 \t epsilon 0.27\n",
      "Epoch 1365 \t reward: 7.0 \t length 4 \t mean_length 2.06 \t epsilon 0.27\n",
      "Epoch 1366 \t reward: 4.0 \t length 3 \t mean_length 2.07 \t epsilon 0.27\n",
      "Epoch 1367 \t reward: 1.5 \t length 2 \t mean_length 2.1 \t epsilon 0.27\n",
      "Epoch 1368 \t reward: 4.0 \t length 3 \t mean_length 2.11 \t epsilon 0.27\n",
      "Epoch 1369 \t reward: 4.0 \t length 3 \t mean_length 2.12 \t epsilon 0.27\n",
      "Epoch 1370 \t reward: 4.0 \t length 3 \t mean_length 2.13 \t epsilon 0.27\n",
      "Epoch 1371 \t reward: 1.5 \t length 2 \t mean_length 2.14 \t epsilon 0.27\n",
      "Epoch 1372 \t reward: 4.0 \t length 3 \t mean_length 2.15 \t epsilon 0.27\n",
      "Epoch 1373 \t reward: 4.0 \t length 3 \t mean_length 2.16 \t epsilon 0.27\n",
      "Epoch 1374 \t reward: 4.0 \t length 3 \t mean_length 2.17 \t epsilon 0.27\n",
      "Epoch 1375 \t reward: 1.5 \t length 2 \t mean_length 2.18 \t epsilon 0.27\n",
      "Epoch 1376 \t reward: 4.0 \t length 3 \t mean_length 2.19 \t epsilon 0.27\n",
      "Epoch 1377 \t reward: 4.0 \t length 3 \t mean_length 2.18 \t epsilon 0.27\n",
      "Epoch 1378 \t reward: 4.0 \t length 3 \t mean_length 2.19 \t epsilon 0.27\n",
      "Epoch 1379 \t reward: 1.5 \t length 2 \t mean_length 2.2 \t epsilon 0.27\n",
      "Epoch 1380 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.27\n",
      "Epoch 1381 \t reward: 4.0 \t length 3 \t mean_length 2.2 \t epsilon 0.27\n",
      "Epoch 1382 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.27\n",
      "Epoch 1383 \t reward: -0.5 \t length 1 \t mean_length 2.24 \t epsilon 0.27\n",
      "Epoch 1384 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.27\n",
      "Epoch 1385 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.26\n",
      "Epoch 1386 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1387 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1388 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1389 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1390 \t reward: 1.5 \t length 2 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1391 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1392 \t reward: 1.5 \t length 2 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1393 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1394 \t reward: 1.5 \t length 2 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1395 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1396 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1397 \t reward: 1.5 \t length 2 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1398 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1399 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1400 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1401 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1402 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.26\n",
      "Epoch 1403 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1404 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1405 \t reward: 7.0 \t length 4 \t mean_length 2.27 \t epsilon 0.26\n",
      "Epoch 1406 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.26\n",
      "Epoch 1407 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.26\n",
      "Epoch 1408 \t reward: 4.0 \t length 3 \t mean_length 2.33 \t epsilon 0.26\n",
      "Epoch 1409 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.26\n",
      "Epoch 1410 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.26\n",
      "Epoch 1411 \t reward: -0.5 \t length 1 \t mean_length 2.34 \t epsilon 0.26\n",
      "Epoch 1412 \t reward: 1.5 \t length 2 \t mean_length 2.33 \t epsilon 0.26\n",
      "Epoch 1413 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.26\n",
      "Epoch 1414 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.26\n",
      "Epoch 1415 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1416 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.26\n",
      "Epoch 1417 \t reward: 4.0 \t length 3 \t mean_length 2.4 \t epsilon 0.26\n",
      "Epoch 1418 \t reward: -0.5 \t length 1 \t mean_length 2.37 \t epsilon 0.26\n",
      "Epoch 1419 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1420 \t reward: 1.5 \t length 2 \t mean_length 2.35 \t epsilon 0.26\n",
      "Epoch 1421 \t reward: -0.5 \t length 1 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1422 \t reward: -0.5 \t length 1 \t mean_length 2.33 \t epsilon 0.26\n",
      "Epoch 1423 \t reward: 1.5 \t length 2 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1424 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.26\n",
      "Epoch 1425 \t reward: -0.5 \t length 1 \t mean_length 2.38 \t epsilon 0.26\n",
      "Epoch 1426 \t reward: -0.5 \t length 1 \t mean_length 2.35 \t epsilon 0.26\n",
      "Epoch 1427 \t reward: -0.5 \t length 1 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1428 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.26\n",
      "Epoch 1429 \t reward: -0.5 \t length 1 \t mean_length 2.38 \t epsilon 0.26\n",
      "Q_matrix of initial state, after training: step 1301\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671CA0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADMUlEQVR4nO2dMYvUVxRH75NdNGBhY2Mxgx/A2IgIwgqrEVJZKAZMEcFCLewUo4KF4GrQziJaCFooGJLCKmB0wYUFERv1A8h/ChsbCyGKwrPwzhe4s5XnnP7Mm+Jwm/mx23ofegiWuYiIaOMfS3YfXqX/d9E/lP5S0T+f/tGifyf9T0V/ffr/Fv2f079X9H9Nf3vRf7muJMp3gwHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOA0fw5m4wWAM90DnCjZfbiZ/kLRX0n/l6L/IP13RX9z+heK/uX0HxX9/enPuofYXfRXvQBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMc9ABwvAJzpHuBYye7D7fSPFP376R8u+n+lv1j0lyMiWhvtK+l98jj9LUX/bUREtPGlih99uJjvzxXf/+IFgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA47gHgeAHgTPcAJ0t2H/5M/1nR35X+k6K/N/3jRf9W+juL/vOIiNZGP5X0Pvkv/fmi/zkiItr4bMWPPvzhBYBjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOO4B4HgB4Ez3AMsluw+L6b8u+tsiIlob1fQ+ifQ3Fv0P3z5g/E/tCwwH079a9H9P/1TRv5H+6aJ/3QsAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHDcA8DxAsCZ7gEOlOw+PIyIaG20oaT3ycf0W9Hv6c/09/6jjfdU/OjD0/RPFP2b6Z8r+lfSf1P0t3oB4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOAYwBwDACOewA4XgA4cxERrY1+qMi9T/6PiIg2Xii93oeVfH+mPUC08e7i+6vpbyr679OfdQ/woujvSL+kRx+8AHQMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxz0AHC8AnDXZA7Q2mi/6nyMioo3PVPzow7X0F4v+csSa7BGWiu+fT/+3on83Yrb/l+AFgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA47gHgfAU9+/HWdnUCCQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671AC0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADQklEQVR4nO2dzYtPYRTHz6PJgo0pC0r3NkpNdtbykh12XnYWg4WiiKSUNEkpiSjKArOw87LDTl6atZ2mlOm5KRZqbMZC6rGYc/+B7+x8P5/9Z84sPp3Nc7q/0lptAbZMRERE6bdJdqtf0r8h+pfT3y36HyIiSun2SHob3uf8w+L8F+kfEP3X6Ut6tLqil26DpLfh1xptMvwvEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmFJ6DvWEDmDPeA1yQ7FZvp79e9JfT3y76n9OfEf259LUH+VbzQb6fFP2l9HeI/qf0d4r+PBvAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHO4BzGEDmDPeA5yU7FYfpb+q9+xSuglJb8PfnL9FnP8t52t6GyL9l6J/aOUP9NPaP1AXcv5acf4fNoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOYwwYwZ7wHOC7ZrT5J/47on0//oeifSv+s6N9Lf5/ov42IKKX7Kult2Jq+9HsDrQ3j7w0cUfxo9TkbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5gz3gPMSXarM+lfEf3rERGldNL37lsb5nP+QXH+q/T3iv679DeL/vf0j4n+0/Svif5VNoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOYwwYwZ7wHWO17+i3Rv5j+R9HfFRFRSndf0ttwJuf34vya8yfF+Us5f784/036J0T/MRvAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHO4BzGEDmDMREVFKt16RWxuWIyKi9Jek6a3eTF/6PkC0On4fYKPo/0x/WvQX0l8U/an0j4r+s/Tviv45NoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOYwwYwZ7wHWKfIrQ2/058R/bmIiCj9acWPVh+kPyv6s+mv6p6glG5K0tuwmH4R/ZXtXfpNih+t/mADmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMM9gDn/AJ4G8dYCeZ9zAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671820>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADOklEQVR4nO2cP2+NcRiG7x+VKga0i+W8ImnEbpGwiA4GQRCJ+AZi1EVikFhqFN9AJIIgBkPFQmKxizQR77tY6s+gaJDH0Od8geeY3Ne1X/2dNFee5dw5LaIPgS1TktTaaK4iRwyr6Z8s+k/Sv1L0b6b/qejPpl/RFTEo/ftF/9zGH+h21D5A/y3fv1h8/86m0sPw30AA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmNP4OtgbLoA54z3AdEWOGNbTP1H0n6Z/vOg/S3+l6M+nv1T0F9P/UfRn0r9R9K+mX9oTRAzfuADmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmsAcwhwtgzngPMFuRI4ZP6S8U/eX0rxf9a+kfKfovJUmt21XxFf2XfH/S30eYaI+g1m2u+Ir+DxfAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHPYA5nABzBnvAUo/mB8xDOlfLvq30j9V9B+nv63of5ckta5VfI2vZ+v2FP2PktTa6ExJj+Fh+keL/gsugDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDnsAczhApgz3gNM+nv9t4v+JUlS6yq6FL3Sn/T7+HslPYbz6d8t+hfSP1D036a/pej/4gKYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwx7AHC6AOeM9wFxFjhhWJUmt21t6PfoP6e8r+u/T31/030lSa6ODJT2GN+k/Kvqn0z9b9B+k/7roH+ICmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmMMewBwugDlTkqTWTZfs6NfTnyn6P9LfXvTXJKm10eGSHsOrfH+q+P7v9HcX/c+S1NroeUmP4Vj6y0V/gQtgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDnsAc7gA5oz3AFtLdvQ/Jam10WJJj2Ep/fmivyLpX+wRJtoDtDZaK+kxbE9/qehv/N9bt7PiK/qvXABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBz2AOY8xf3lg7lgQRhYAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 1430 \t reward: -0.5 \t length 1 \t mean_length 2.35 \t epsilon 0.26\n",
      "Epoch 1431 \t reward: 1.5 \t length 2 \t mean_length 2.36 \t epsilon 0.26\n",
      "Epoch 1432 \t reward: -0.5 \t length 1 \t mean_length 2.33 \t epsilon 0.25\n",
      "Epoch 1433 \t reward: -0.5 \t length 1 \t mean_length 2.34 \t epsilon 0.25\n",
      "Epoch 1434 \t reward: 1.5 \t length 2 \t mean_length 2.33 \t epsilon 0.25\n",
      "Epoch 1435 \t reward: -0.5 \t length 1 \t mean_length 2.36 \t epsilon 0.25\n",
      "Epoch 1436 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.25\n",
      "Epoch 1437 \t reward: 4.0 \t length 3 \t mean_length 2.38 \t epsilon 0.25\n",
      "Epoch 1438 \t reward: 4.0 \t length 3 \t mean_length 2.39 \t epsilon 0.25\n",
      "Epoch 1439 \t reward: 4.0 \t length 3 \t mean_length 2.4 \t epsilon 0.25\n",
      "Epoch 1440 \t reward: 4.0 \t length 3 \t mean_length 2.41 \t epsilon 0.25\n",
      "Epoch 1441 \t reward: -0.5 \t length 1 \t mean_length 2.4 \t epsilon 0.25\n",
      "Epoch 1442 \t reward: 7.0 \t length 4 \t mean_length 2.43 \t epsilon 0.25\n",
      "Epoch 1443 \t reward: -0.5 \t length 1 \t mean_length 2.44 \t epsilon 0.25\n",
      "Epoch 1444 \t reward: 4.0 \t length 3 \t mean_length 2.45 \t epsilon 0.25\n",
      "Epoch 1445 \t reward: -0.5 \t length 1 \t mean_length 2.46 \t epsilon 0.25\n",
      "Epoch 1446 \t reward: 7.0 \t length 4 \t mean_length 2.49 \t epsilon 0.25\n",
      "Epoch 1447 \t reward: 4.0 \t length 3 \t mean_length 2.5 \t epsilon 0.25\n",
      "Epoch 1448 \t reward: 7.0 \t length 4 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1449 \t reward: 4.0 \t length 3 \t mean_length 2.54 \t epsilon 0.25\n",
      "Epoch 1450 \t reward: 1.5 \t length 2 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1451 \t reward: -0.5 \t length 1 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1452 \t reward: -0.5 \t length 1 \t mean_length 2.49 \t epsilon 0.25\n",
      "Epoch 1453 \t reward: 4.0 \t length 3 \t mean_length 2.48 \t epsilon 0.25\n",
      "Epoch 1454 \t reward: 1.5 \t length 2 \t mean_length 2.47 \t epsilon 0.25\n",
      "Epoch 1455 \t reward: 4.0 \t length 3 \t mean_length 2.46 \t epsilon 0.25\n",
      "Epoch 1456 \t reward: 4.0 \t length 3 \t mean_length 2.47 \t epsilon 0.25\n",
      "Epoch 1457 \t reward: 1.5 \t length 2 \t mean_length 2.5 \t epsilon 0.25\n",
      "Epoch 1458 \t reward: 4.0 \t length 3 \t mean_length 2.5 \t epsilon 0.25\n",
      "Epoch 1459 \t reward: 1.5 \t length 2 \t mean_length 2.54 \t epsilon 0.25\n",
      "Epoch 1460 \t reward: 1.5 \t length 2 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1461 \t reward: 4.0 \t length 3 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1462 \t reward: -0.5 \t length 1 \t mean_length 2.49 \t epsilon 0.25\n",
      "Epoch 1463 \t reward: 4.0 \t length 3 \t mean_length 2.5 \t epsilon 0.25\n",
      "Epoch 1464 \t reward: 7.0 \t length 4 \t mean_length 2.52 \t epsilon 0.25\n",
      "Epoch 1465 \t reward: 1.5 \t length 2 \t mean_length 2.5 \t epsilon 0.25\n",
      "Epoch 1466 \t reward: -0.5 \t length 1 \t mean_length 2.47 \t epsilon 0.25\n",
      "Epoch 1467 \t reward: 1.5 \t length 2 \t mean_length 2.46 \t epsilon 0.25\n",
      "Epoch 1468 \t reward: -0.5 \t length 1 \t mean_length 2.43 \t epsilon 0.25\n",
      "Epoch 1469 \t reward: 4.0 \t length 3 \t mean_length 2.42 \t epsilon 0.25\n",
      "Epoch 1470 \t reward: -0.5 \t length 1 \t mean_length 2.39 \t epsilon 0.25\n",
      "Epoch 1471 \t reward: 1.5 \t length 2 \t mean_length 2.38 \t epsilon 0.25\n",
      "Epoch 1472 \t reward: 1.5 \t length 2 \t mean_length 2.37 \t epsilon 0.25\n",
      "Epoch 1473 \t reward: 1.5 \t length 2 \t mean_length 2.36 \t epsilon 0.25\n",
      "Epoch 1474 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.25\n",
      "Epoch 1475 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.25\n",
      "Epoch 1476 \t reward: 1.5 \t length 2 \t mean_length 2.35 \t epsilon 0.25\n",
      "Epoch 1477 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.25\n",
      "Epoch 1478 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.25\n",
      "Epoch 1479 \t reward: 1.5 \t length 2 \t mean_length 2.34 \t epsilon 0.25\n",
      "Epoch 1480 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.25\n",
      "Epoch 1481 \t reward: 1.5 \t length 2 \t mean_length 2.34 \t epsilon 0.25\n",
      "Epoch 1482 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1483 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1484 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1485 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.24\n",
      "Epoch 1486 \t reward: 7.0 \t length 4 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1487 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1488 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1489 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1490 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1491 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1492 \t reward: 1.5 \t length 2 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1493 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1494 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1495 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1496 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1497 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1498 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1499 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1500 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1501 \t reward: 1.5 \t length 2 \t mean_length 2.34 \t epsilon 0.24\n",
      "Epoch 1502 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1503 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.24\n",
      "Epoch 1504 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1505 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.24\n",
      "Epoch 1506 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.24\n",
      "Epoch 1507 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.24\n",
      "Epoch 1508 \t reward: 1.5 \t length 2 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1509 \t reward: -0.5 \t length 1 \t mean_length 2.34 \t epsilon 0.24\n",
      "Epoch 1510 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1511 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1512 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1513 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1514 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1515 \t reward: 10.5 \t length 5 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1516 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.24\n",
      "Epoch 1517 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.24\n",
      "Epoch 1518 \t reward: -0.5 \t length 1 \t mean_length 2.23 \t epsilon 0.24\n",
      "Epoch 1519 \t reward: -0.5 \t length 1 \t mean_length 2.25 \t epsilon 0.24\n",
      "Epoch 1520 \t reward: 7.0 \t length 4 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1521 \t reward: 1.5 \t length 2 \t mean_length 2.29 \t epsilon 0.24\n",
      "Epoch 1522 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.24\n",
      "Epoch 1523 \t reward: -0.5 \t length 1 \t mean_length 2.34 \t epsilon 0.24\n",
      "Epoch 1524 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1525 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.24\n",
      "Epoch 1526 \t reward: 1.5 \t length 2 \t mean_length 2.33 \t epsilon 0.24\n",
      "Epoch 1527 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.24\n",
      "Epoch 1528 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.24\n",
      "Epoch 1529 \t reward: 1.5 \t length 2 \t mean_length 2.36 \t epsilon 0.24\n",
      "Q_matrix of initial state, after training: step 1401\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719A0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADKElEQVR4nO2du8vNcRjAnx8vMmCRRZ0jlGSSLGLwFrkNLiVZzC6TwYIJi8HkMlsk5TK4RTGQRW8mSSG/UxZZMAjxNXjOP/B4J5/PZ/+8zxk+77N8n87pWutbCJaJiIjohitLdutfpT9V9Nekv7vo30h/Q9F/kv6voj8z/ftFf0v6j4r+ZPozi/6vGSVR/hsMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAp/M5mI0bAM74HuBIyW79+fTXF/2n6R8u+hfSf1b016W/vejfSf9W0d+Z/sGifyn95UX/jRsAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHAMQA4BgDHAOB4DwDHDQBnfA+wrWS3/m76S4v+u/T3Fv1rERFdN5hd0tvoR/rHiv7Z9FcV/ZcREdENl1X8aP3bnD+rOP+nGwCOAcAxADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4HgPAMcNAGd8D7C/ZLf+SvqPi/7G9F8U/dXpTxb9R9Phd93gaElvo3Ppzyn63yMiohueqfjR+uNuADgGAMcA4BgAHAOAYwBwDACOAcAxADgGAMcA4BgAHAOA4z0AHDcAnPE9wNWS3fp96U8V/TUREV03qOltFOkvKPqf//6B4c3aB+h3pX+66J9If0/Rv57+paJ/0A0AxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgGAAcA4BjAHC8B4DjBoAzvgc4VLJbfzEiousG80t6G33J+YuK8z/m/NLvDbQ2upbzNxXnP0x/btH/lv6Bon85/fdFf4kbAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAxwDgeA8Axw0AZyJiWt7zN5emt/5Bzi+9p7c2Gr+nLy7O/5B+7R+h9b/T31r076X/uuivSL+kR+vdAHQMAI4BwDEAOAYAxwDgGAAcA4BjAHAMAI4BwDEAOAYAx3sAOG4AOON7gIUVubXRp/TnFf2vERHRDU9W/Gj9qfTXFv3nEdNyj/Cv3/e/o+jfjvi3ew43ABwDgGMAcAwAjgHAMQA4BgDHAOAYABwDgGMAcAwAjgHA8R4Azh/JdPHWVNouKAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671CA0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADV0lEQVR4nO2dr8uWZxiGz9t9iIJDV0QXnjeMTcHgj6YMhLFgEgYurBhccij4hQVdkAU1GBSULc1g+cI+EEyGMRjI1vwRBHUYnifoWFFRmHz4eS94vf/AafM8jn681xsOrnJf8LTexy6IZUGS1GaDZfdxKv9X0/+6/A9Mf1WSWht2W3qfbtf8E+b8i+UfMv3l8i1dfXyrt+ETS+/TozXeZHhfIIBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCaTwHZ8MGCGd+D/CdZffxp/K9kPr4pvzPTf9m+QdM/4YktTZY9wi9T6s1f4s5/5/yN5j+y/I3mf4zNkA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA43AOEwwYIZ34PsN+y+/iHJLU2rLf0Pv1X89ea81dq/g5z/r3yX5n+uvIvmP5i+dtM/0H5G03/ORsgHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHO4BwmEDhDO/B/jSsvv4W/k/mv7p8q+a/uHyvzD938s/aPrXJak183MLvT630IZdpn/n7Q/MvvH+wLjEBgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHe4Bw2ADhzO8BfrDsPp4p/6Tpn5Ok1gbrPbv3aanmf2TOf1r+O90DqM22mv6T8veY/q3yfzH9b9kA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4XAPEA4bIJz5PcBRy+7jz+W/63v6n6a/T5JaGy5Zep+O1/yd5vy7NX+vOf+vmr/dnH+//OOmf4kNEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA73AOGwAcJZkKTWho8duffpsSSpzU5Z0/t4tuavNeevlL9g+q8lSW220fHVx+flL5v+ofKPmP6V8i+Y/iIbIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBzuAcJhA4Qzvwf4zJF7nx6W/5XpX5Mktdn3jq8+ni//mOlflqTWhs2W3qd/y//U9P8u/0PTfyFJajPrnkJ9XGEDhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhMM9QDj/A4nX99a11rK5AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671580>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADQ0lEQVR4nO2cT4tPcRSHP19mGn9W/pRSfjdNbDQbFlaytSBKpETIC2AnshLZ8QKESIkUsbCVlQWbyYYm3auUwqwG08SxOb83cGbn8zz7p7N57tnc07dF9CGwZUKSWhttqsgRw9f0Dxb9Z+lfKfqX0y9FHDG09BeL/lT6T4v+IUlS69ZUfEX/M+efK86/uaI0GP4bCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzGr+DvWEDmDO+B1hfkSOGH+mfKvp30z9a9B+l/7Hob0v/atG/lH5FV8Sg9G8V/bPpTxf9OTaAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOdwDmMMGMGd8D7C5IkcMX9I/XPSfpH+t6F9M/1jRfyhJat3Wiq/oP+X8fcX5L9N/XvQPpD9V9BfZAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc8T3ATEWOGGbTv1D0r6d/pujfTn910f8lSWpd7UOI/m/6pXsKRT++pzhZ0mO4l/6Rov+YDWAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAO9wDmsAHMGd8DHK/IEcOD9Jf13r1aV9Gl6JX+xqL/TZJaGz0u6TEcSf9F0d+f/p6i/zr9tUV/gQ1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzBnfA6yryBHDvCSpdbUf+tH36U8X/bn0txf9D5LU2mhHSY/hffqviv7e9E8X/Tvpvy36u9gA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5nAPYA4bwJwJSVLrJkt29Evpl97rV/Tj9/pXFf3fktTaaHdJj+FNzl9ZnP8n/Q1F/7sktTaaLekxzKT/rujvZAOYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz2AOWwAc8b3AMv9H3+jpMdwPv0tRf+zJKl1pffyFf1C+q3ohyS1Nloq6TFMpn+/6J+QJLWu9L6Dop9nA5hDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPYA5/wAiAA/lcQxzpgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n",
      "Epoch 1530 \t reward: 1.5 \t length 2 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1531 \t reward: 4.0 \t length 3 \t mean_length 2.38 \t epsilon 0.24\n",
      "Epoch 1532 \t reward: -0.5 \t length 1 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1533 \t reward: 4.0 \t length 3 \t mean_length 2.38 \t epsilon 0.24\n",
      "Epoch 1534 \t reward: -0.5 \t length 1 \t mean_length 2.35 \t epsilon 0.24\n",
      "Epoch 1535 \t reward: -0.5 \t length 1 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1536 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.23\n",
      "Epoch 1537 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1538 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.23\n",
      "Epoch 1539 \t reward: 1.5 \t length 2 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1540 \t reward: -0.5 \t length 1 \t mean_length 2.33 \t epsilon 0.23\n",
      "Epoch 1541 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1542 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.23\n",
      "Epoch 1543 \t reward: 7.0 \t length 4 \t mean_length 2.25 \t epsilon 0.23\n",
      "Epoch 1544 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.23\n",
      "Epoch 1545 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.23\n",
      "Epoch 1546 \t reward: -0.5 \t length 1 \t mean_length 2.23 \t epsilon 0.23\n",
      "Epoch 1547 \t reward: 1.5 \t length 2 \t mean_length 2.2 \t epsilon 0.23\n",
      "Epoch 1548 \t reward: 4.0 \t length 3 \t mean_length 2.21 \t epsilon 0.23\n",
      "Epoch 1549 \t reward: 4.0 \t length 3 \t mean_length 2.18 \t epsilon 0.23\n",
      "Epoch 1550 \t reward: -0.5 \t length 1 \t mean_length 2.15 \t epsilon 0.23\n",
      "Epoch 1551 \t reward: 4.0 \t length 3 \t mean_length 2.16 \t epsilon 0.23\n",
      "Epoch 1552 \t reward: 7.0 \t length 4 \t mean_length 2.19 \t epsilon 0.23\n",
      "Epoch 1553 \t reward: 4.0 \t length 3 \t mean_length 2.22 \t epsilon 0.23\n",
      "Epoch 1554 \t reward: 1.5 \t length 2 \t mean_length 2.21 \t epsilon 0.23\n",
      "Epoch 1555 \t reward: 4.0 \t length 3 \t mean_length 2.22 \t epsilon 0.23\n",
      "Epoch 1556 \t reward: 4.0 \t length 3 \t mean_length 2.23 \t epsilon 0.23\n",
      "Epoch 1557 \t reward: -0.5 \t length 1 \t mean_length 2.22 \t epsilon 0.23\n",
      "Epoch 1558 \t reward: 4.0 \t length 3 \t mean_length 2.23 \t epsilon 0.23\n",
      "Epoch 1559 \t reward: 7.0 \t length 4 \t mean_length 2.22 \t epsilon 0.23\n",
      "Epoch 1560 \t reward: 4.0 \t length 3 \t mean_length 2.23 \t epsilon 0.23\n",
      "Epoch 1561 \t reward: 4.0 \t length 3 \t mean_length 2.24 \t epsilon 0.23\n",
      "Epoch 1562 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.23\n",
      "Epoch 1563 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.23\n",
      "Epoch 1564 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.23\n",
      "Epoch 1565 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.23\n",
      "Epoch 1566 \t reward: 7.0 \t length 4 \t mean_length 2.29 \t epsilon 0.23\n",
      "Epoch 1567 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1568 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1569 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.23\n",
      "Epoch 1570 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1571 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.23\n",
      "Epoch 1572 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.23\n",
      "Epoch 1573 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1574 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.23\n",
      "Epoch 1575 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1576 \t reward: 4.0 \t length 3 \t mean_length 2.37 \t epsilon 0.23\n",
      "Epoch 1577 \t reward: -0.5 \t length 1 \t mean_length 2.38 \t epsilon 0.23\n",
      "Epoch 1578 \t reward: 1.5 \t length 2 \t mean_length 2.37 \t epsilon 0.23\n",
      "Epoch 1579 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.23\n",
      "Epoch 1580 \t reward: -0.5 \t length 1 \t mean_length 2.33 \t epsilon 0.23\n",
      "Epoch 1581 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1582 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1583 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.23\n",
      "Epoch 1584 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1585 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.23\n",
      "Epoch 1586 \t reward: -0.5 \t length 1 \t mean_length 2.31 \t epsilon 0.23\n",
      "Epoch 1587 \t reward: 14.5 \t length 6 \t mean_length 2.27 \t epsilon 0.23\n",
      "Epoch 1588 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.23\n",
      "Epoch 1589 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.23\n",
      "Epoch 1590 \t reward: 4.0 \t length 3 \t mean_length 2.29 \t epsilon 0.22\n",
      "Epoch 1591 \t reward: 10.5 \t length 5 \t mean_length 2.31 \t epsilon 0.22\n",
      "Epoch 1592 \t reward: 4.0 \t length 3 \t mean_length 2.33 \t epsilon 0.22\n",
      "Epoch 1593 \t reward: 4.0 \t length 3 \t mean_length 2.34 \t epsilon 0.22\n",
      "Epoch 1594 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.22\n",
      "Epoch 1595 \t reward: 1.5 \t length 2 \t mean_length 2.34 \t epsilon 0.22\n",
      "Epoch 1596 \t reward: 4.0 \t length 3 \t mean_length 2.35 \t epsilon 0.22\n",
      "Epoch 1597 \t reward: 4.0 \t length 3 \t mean_length 2.36 \t epsilon 0.22\n",
      "Epoch 1598 \t reward: 1.5 \t length 2 \t mean_length 2.35 \t epsilon 0.22\n",
      "Epoch 1599 \t reward: 1.5 \t length 2 \t mean_length 2.34 \t epsilon 0.22\n",
      "Epoch 1600 \t reward: 1.5 \t length 2 \t mean_length 2.33 \t epsilon 0.22\n",
      "Epoch 1601 \t reward: 4.0 \t length 3 \t mean_length 2.31 \t epsilon 0.22\n",
      "Epoch 1602 \t reward: 4.0 \t length 3 \t mean_length 2.33 \t epsilon 0.22\n",
      "Epoch 1603 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.22\n",
      "Epoch 1604 \t reward: 1.5 \t length 2 \t mean_length 2.31 \t epsilon 0.22\n",
      "Epoch 1605 \t reward: 10.5 \t length 5 \t mean_length 2.29 \t epsilon 0.22\n",
      "Epoch 1606 \t reward: -0.5 \t length 1 \t mean_length 2.27 \t epsilon 0.22\n",
      "Epoch 1607 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.22\n",
      "Epoch 1608 \t reward: 1.5 \t length 2 \t mean_length 2.25 \t epsilon 0.22\n",
      "Epoch 1609 \t reward: 4.0 \t length 3 \t mean_length 2.25 \t epsilon 0.22\n",
      "Epoch 1610 \t reward: 4.0 \t length 3 \t mean_length 2.27 \t epsilon 0.22\n",
      "Epoch 1611 \t reward: -0.5 \t length 1 \t mean_length 2.29 \t epsilon 0.22\n",
      "Epoch 1612 \t reward: 10.5 \t length 5 \t mean_length 2.35 \t epsilon 0.22\n",
      "Epoch 1613 \t reward: -0.5 \t length 1 \t mean_length 2.38 \t epsilon 0.22\n",
      "Epoch 1614 \t reward: 4.0 \t length 3 \t mean_length 2.39 \t epsilon 0.22\n",
      "Epoch 1615 \t reward: 10.5 \t length 5 \t mean_length 2.38 \t epsilon 0.22\n",
      "Epoch 1616 \t reward: 4.0 \t length 3 \t mean_length 2.39 \t epsilon 0.22\n",
      "Epoch 1617 \t reward: 7.0 \t length 4 \t mean_length 2.42 \t epsilon 0.22\n",
      "Epoch 1618 \t reward: 4.0 \t length 3 \t mean_length 2.43 \t epsilon 0.22\n",
      "Epoch 1619 \t reward: 7.0 \t length 4 \t mean_length 2.46 \t epsilon 0.22\n",
      "Epoch 1620 \t reward: 7.0 \t length 4 \t mean_length 2.49 \t epsilon 0.22\n",
      "Epoch 1621 \t reward: 4.0 \t length 3 \t mean_length 2.46 \t epsilon 0.22\n",
      "Epoch 1622 \t reward: 7.0 \t length 4 \t mean_length 2.49 \t epsilon 0.22\n",
      "Epoch 1623 \t reward: 1.5 \t length 2 \t mean_length 2.48 \t epsilon 0.22\n",
      "Epoch 1624 \t reward: 7.0 \t length 4 \t mean_length 2.5 \t epsilon 0.22\n",
      "Epoch 1625 \t reward: -0.5 \t length 1 \t mean_length 2.5 \t epsilon 0.22\n",
      "Epoch 1626 \t reward: 7.0 \t length 4 \t mean_length 2.52 \t epsilon 0.22\n",
      "Epoch 1627 \t reward: 4.0 \t length 3 \t mean_length 2.54 \t epsilon 0.22\n",
      "Epoch 1628 \t reward: 7.0 \t length 4 \t mean_length 2.56 \t epsilon 0.22\n",
      "Epoch 1629 \t reward: 4.0 \t length 3 \t mean_length 2.56 \t epsilon 0.22\n",
      "Q_matrix of initial state, after training: step 1501\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719D0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADQUlEQVR4nO2dS6uNYRiG75eNnEIOKbVWBjJgYO6QwsDEXJH8CokMRPIrJMrcxIBdcpgbMJCBvq/UbiPklONr4Fl/4DZzX9f8Ws9ude1n8j1rrdb70AWxzElSa5PmyL2Pf+Np0/fW9D6sr/lLzPm/a/5Wc/5C+ZauPqj8G6Z/svy3pr9RklqbrLX0Pn603nj4fyCAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwmk8Ds6GDRDOnCSpTS9bdh/Olb/L9J+Vf8X0z5Z/3vQvSf/2PL3m7zDnv6j5q835n8tfZvo/2ADhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhcA8QDhsgnNk9wBbL7sOiJLU2WWPpffxU/pzp/yx/Yvpj+d9Mf0X5x03/VvnbTP9V+ctN/zsbIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBzuAcJhA4Qzuwc4atl9uFP+XdM/Uv5j099b/h7Tf1L+PtN/JEmtTX5aeh9nv9ew0vS/SpLa9J7jqw+H2QDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhcA8QDhsgnNk9wGnL7sO18udN/5AktWZ9vF+9jyp/q+kv/H2B6UPvDxj2l3/K9K+Xf8D0H5R/0/RPsAHCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDC4R4gHDZAOLN7gEuW3YfzktTaZL2l9/F9zT9pzr9R86+a88/U/EPm/Pmav9uc/7Tm7zTnPy//tukfYwOEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEwz1AOGyAcGbfV7/ZkXsfX5e/yvS/lL/J9N+Uv8L0v5W/1PR/lb/B9N9Jktr0vuOrDwfLf2n629kA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4XAPEA4bIJzZPYD1hf29j2P5W0x/UZLUphcdX324UL71PF59eCdJrU3WWXofP9R87x+pD7/LP2b6tyWptclaS+/jRzZAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAONwDhPMHqc3/1hFBVvsAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671E50>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADRklEQVR4nO2dsauOcRiG74dPSAYhJd43J4WSkiiLDauyy2YwGGRCEiadwWCwya6s2CyKpKRQOvq9pIQMJ3FyeAznef+B+zO572u/zvMNV8/yPd/vRGZLGFkmABDRrWTkzGEBABD9A2p6tiM1fz05/2v5W0j/AwAg+rOMj2yz5R8k/cflUzqyLenRdZSew7CMm2z+FxyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxAn/HWwNt4A4kwAANFfoOxsVwEgoqNCyhz+1Pzj5Py7NT/I+Vn+dtJ/W/5U9wgR3U7Sf13+GtL/7g0gjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4HkAcbwBxxnuA1ZSd7QcARHTbKD2Hd+VvJv2P5e8j/WflL5L++L4C9b5A5jBb/iHSf1T+WtKf9wYQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHF8DyCON4A44z3ADsrO9qb806R/s/xLpH+5/BnSnyt/D+m/AIAI6rl+ZA4ofwPpf1n6A/0V7gO0i94A4jgAcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCO7wHE8QYQZ7wH2ErZ2d6Xf4L07wBARLeX0nN4Xj79Xj6Af/H5p/1/CdTv+5FtvvzrpH/OG0AcByCOAxDHAYjjAMRxAOI4AHEcgDgOQBwHII4DEMcBiOMAxPE9gDjeAOKM9wBnKDvbjfKnfV/gCekfAICI7jyl53Ct5u8n5z+t+cfI+ffKX0n6CwCA6A8zPrI99AYQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOI4AHF8DyCON4A4EwCI6DYxcubwqfwVpP+r/Kney4/o1pH+t/Kn+j4+oltF+j8BANEfZXxku1/+KdK/5Q0gjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDHAYjjAMRxAOL4HkAcbwBxxnuAnpEzh1b+btJ/CQCI/iTjI9vt8neR/isAiOg2UnoOn8ufIf258iekvwgAiH454yPbb28AcRyAOA5AHAcgjgMQxwGI4wDEcQDiOABxHIA4DkAcByCOAxDH9wDi/AX8dwXltQurGQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671970>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADNklEQVR4nO2dv8uOYRiGz5uvWPxaLPI+EolMBpvYGCnZTJTBgIhRRiIMBsVkk/qMbGQzmEQieR5ZLH58C4Xbcr3/wJnJeRz70b0cXct79ryt97ELYlmQpNZm2xy59+lN+btN/3n550z/avmfTH9D+a9Nf3v5903/iCSpDascX31cqvdPmu/fWmY9DP8NBBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4jZ+Ds+EChLMgSWqDF0If/0hSa7ODlt6nh+UfMP1H5b80/Z3l3zL9k+U7unqfVP4T099Xvr3n4AKEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEwx4gHC5AOPPvA6x35N6nz+VfNv0L5d8x/ePlnzf9K5KkNmx1fPXxbb1/xHz/fvmvTH9H+dtN/zUXIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBz2AOFwAcKZ7wF2OXLv04vyz5r+tfKPmv698teZ/pfy15j+N0n/Yk+w33z/cfn29xW4AOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOGwBwiHCxDOfA9wypF7n26Wf8P0T0uS2uDoUh9VvrUHUB/ne4Dblt6nE+Uvmv6h8g+Z/mL5a03/KxcgHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHPYA4XABwpnvAVY5cu/TkiSpDRut1/v4sfzNpv++/C2m/06SWputtvQ+fS//uenvLv+M6V8v/4HpH+YChEMA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhMMeIBwuQDgLkqQ2eCH08U/5K0z/Z/nLTf+3JLU222PpfXpW/gbT/yRJaoO1p1Afl+r9p+b7e8t/ZPoHuADhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhsAcIhwsQznwPsGDZffwlSa3NLll6ny6Wv83030iS2rDS8dXHH/X+JvP9D+V/Nv315d81/WOSpDasdXz1kf8LSIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHAIIBwCCIc9QDh/Abw4EeXboEIIAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n",
      "Epoch 1630 \t reward: 4.0 \t length 3 \t mean_length 2.56 \t epsilon 0.22\n",
      "Epoch 1631 \t reward: 7.0 \t length 4 \t mean_length 2.58 \t epsilon 0.22\n",
      "Epoch 1632 \t reward: 4.0 \t length 3 \t mean_length 2.58 \t epsilon 0.22\n",
      "Epoch 1633 \t reward: -0.5 \t length 1 \t mean_length 2.62 \t epsilon 0.22\n",
      "Epoch 1634 \t reward: 1.5 \t length 2 \t mean_length 2.6 \t epsilon 0.22\n",
      "Epoch 1635 \t reward: 4.0 \t length 3 \t mean_length 2.64 \t epsilon 0.22\n",
      "Epoch 1636 \t reward: 7.0 \t length 4 \t mean_length 2.66 \t epsilon 0.22\n",
      "Epoch 1637 \t reward: 4.0 \t length 3 \t mean_length 2.66 \t epsilon 0.22\n",
      "Epoch 1638 \t reward: 4.0 \t length 3 \t mean_length 2.66 \t epsilon 0.22\n",
      "Epoch 1639 \t reward: 7.0 \t length 4 \t mean_length 2.66 \t epsilon 0.22\n",
      "Epoch 1640 \t reward: -0.5 \t length 1 \t mean_length 2.62 \t epsilon 0.22\n",
      "Epoch 1641 \t reward: 1.5 \t length 2 \t mean_length 2.66 \t epsilon 0.22\n",
      "Epoch 1642 \t reward: 7.0 \t length 4 \t mean_length 2.68 \t epsilon 0.22\n",
      "Epoch 1643 \t reward: 4.0 \t length 3 \t mean_length 2.72 \t epsilon 0.22\n",
      "Epoch 1644 \t reward: 1.5 \t length 2 \t mean_length 2.7 \t epsilon 0.22\n",
      "Epoch 1645 \t reward: 4.0 \t length 3 \t mean_length 2.7 \t epsilon 0.22\n",
      "Epoch 1646 \t reward: 4.0 \t length 3 \t mean_length 2.7 \t epsilon 0.22\n",
      "Epoch 1647 \t reward: 4.0 \t length 3 \t mean_length 2.74 \t epsilon 0.22\n",
      "Epoch 1648 \t reward: 10.5 \t length 5 \t mean_length 2.78 \t epsilon 0.22\n",
      "Epoch 1649 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.21\n",
      "Epoch 1650 \t reward: 7.0 \t length 4 \t mean_length 2.8 \t epsilon 0.21\n",
      "Epoch 1651 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1652 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1653 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.21\n",
      "Epoch 1654 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.21\n",
      "Epoch 1655 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1656 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1657 \t reward: -0.5 \t length 1 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1658 \t reward: 10.5 \t length 5 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1659 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1660 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1661 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1662 \t reward: 7.0 \t length 4 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1663 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1664 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1665 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1666 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1667 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1668 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.21\n",
      "Epoch 1669 \t reward: 7.0 \t length 4 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1670 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1671 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1672 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1673 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1674 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1675 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1676 \t reward: 1.5 \t length 2 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1677 \t reward: 1.5 \t length 2 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1678 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1679 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1680 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1681 \t reward: 1.5 \t length 2 \t mean_length 2.9 \t epsilon 0.21\n",
      "Epoch 1682 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1683 \t reward: 7.0 \t length 4 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1684 \t reward: -0.5 \t length 1 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1685 \t reward: 7.0 \t length 4 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1686 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1687 \t reward: 4.0 \t length 3 \t mean_length 2.92 \t epsilon 0.21\n",
      "Epoch 1688 \t reward: 4.0 \t length 3 \t mean_length 2.92 \t epsilon 0.21\n",
      "Epoch 1689 \t reward: -0.5 \t length 1 \t mean_length 2.92 \t epsilon 0.21\n",
      "Epoch 1690 \t reward: -0.5 \t length 1 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1691 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1692 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1693 \t reward: 7.0 \t length 4 \t mean_length 2.88 \t epsilon 0.21\n",
      "Epoch 1694 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1695 \t reward: 7.0 \t length 4 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1696 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1697 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1698 \t reward: 1.5 \t length 2 \t mean_length 2.84 \t epsilon 0.21\n",
      "Epoch 1699 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.21\n",
      "Epoch 1700 \t reward: 14.5 \t length 6 \t mean_length 2.92 \t epsilon 0.21\n",
      "Epoch 1701 \t reward: 4.0 \t length 3 \t mean_length 2.94 \t epsilon 0.21\n",
      "Epoch 1702 \t reward: 14.5 \t length 6 \t mean_length 3.0 \t epsilon 0.21\n",
      "Epoch 1703 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.21\n",
      "Epoch 1704 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.21\n",
      "Epoch 1705 \t reward: 1.5 \t length 2 \t mean_length 3.02 \t epsilon 0.21\n",
      "Epoch 1706 \t reward: 7.0 \t length 4 \t mean_length 3.03 \t epsilon 0.21\n",
      "Epoch 1707 \t reward: 7.0 \t length 4 \t mean_length 3.08 \t epsilon 0.21\n",
      "Epoch 1708 \t reward: 10.5 \t length 5 \t mean_length 3.11 \t epsilon 0.21\n",
      "Epoch 1709 \t reward: 7.0 \t length 4 \t mean_length 3.14 \t epsilon 0.21\n",
      "Epoch 1710 \t reward: 4.0 \t length 3 \t mean_length 3.13 \t epsilon 0.21\n",
      "Epoch 1711 \t reward: -0.5 \t length 1 \t mean_length 3.14 \t epsilon 0.2\n",
      "Epoch 1712 \t reward: 4.0 \t length 3 \t mean_length 3.13 \t epsilon 0.2\n",
      "Epoch 1713 \t reward: 7.0 \t length 4 \t mean_length 3.1 \t epsilon 0.2\n",
      "Epoch 1714 \t reward: 1.5 \t length 2 \t mean_length 3.07 \t epsilon 0.2\n",
      "Epoch 1715 \t reward: 4.0 \t length 3 \t mean_length 3.08 \t epsilon 0.2\n",
      "Epoch 1716 \t reward: -0.5 \t length 1 \t mean_length 3.03 \t epsilon 0.2\n",
      "Epoch 1717 \t reward: 19.0 \t length 7 \t mean_length 3.04 \t epsilon 0.2\n",
      "Epoch 1718 \t reward: -0.5 \t length 1 \t mean_length 3.0 \t epsilon 0.2\n",
      "Epoch 1719 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.2\n",
      "Epoch 1720 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.2\n",
      "Epoch 1721 \t reward: -0.5 \t length 1 \t mean_length 2.98 \t epsilon 0.2\n",
      "Epoch 1722 \t reward: 14.5 \t length 6 \t mean_length 3.03 \t epsilon 0.2\n",
      "Epoch 1723 \t reward: 1.5 \t length 2 \t mean_length 3.02 \t epsilon 0.2\n",
      "Epoch 1724 \t reward: 1.5 \t length 2 \t mean_length 3.0 \t epsilon 0.2\n",
      "Epoch 1725 \t reward: 4.0 \t length 3 \t mean_length 2.98 \t epsilon 0.2\n",
      "Epoch 1726 \t reward: 1.5 \t length 2 \t mean_length 2.96 \t epsilon 0.2\n",
      "Epoch 1727 \t reward: -0.5 \t length 1 \t mean_length 2.94 \t epsilon 0.2\n",
      "Epoch 1728 \t reward: 1.5 \t length 2 \t mean_length 2.92 \t epsilon 0.2\n",
      "Epoch 1729 \t reward: -0.5 \t length 1 \t mean_length 2.9 \t epsilon 0.2\n",
      "Q_matrix of initial state, after training: step 1601\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671F10>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADTElEQVR4nO2cz4tOcRSHP19NmMmPySiS3puVUpPGDqWsyMJuIhsLCzt2yl+g7NhZWNiI7CzESilmZ9KUstK9SZTR+NEMmhwL5/0Hjp3P8+yf97yLp7O5594W0YfAlglJam00VZEjhlVJUutelaZHfzDnby7O/5HzjxbnP0//dtG/kP65on83/c9Ff4cktTaaLOkxrG0oDYb/BgIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAnMbjYG/YAOZMSJJa96RkR38i/dNF/2H6N4v+pfTPFP37ktTaaFNJj+Fn+geK/uv0Z4r+cvpbi/43NoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOYwwYwZ/x9gFIIEcPv9KeL/kr6e4v+u/SPFP0X6X8s+rvSP1z0F9KfLvor6ZfvGdgA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5nAPYA4bwJzx9wGOl+zon6Zfmx690r9R9C+nX3oer+gX0p8o+uuS1Nroa0mPYVv6G4v+L0lS6x5XfEV/kg1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzBnfA+wv2dG/Sb/0fr2iH79fX9NjUPo7i/6nvz/Qna/9gf5O+vNF/0H6F4v+rfSXiv4sG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOYM74HeFSyoz8lSa2NtpT0GL6nX/7effqLRX9OktS6mYqv6Jdz/tni/Hvpl75PEDGsS5Ja96DiK/p5NoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOYwwYwZ0KSWhttr8gRw5f054r+Yvr7iv7b9P/p/6t1pXsGRT++ZzhUnP8y518tzr+W/pWif50NYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAcwZ3wPMVuSIYSn9PUX/vSSpdbsrvqL/kPOni/NX0p8q+quSpNaV5iv6lfSPFf1nktTaaLKkx7DGBjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHewBz/gASyAPlfYVM+wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6719A0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADK0lEQVR4nO2cTYuOcRSHz9+Md2WamFCeW8qGpkzZ2NqzUco3sJdIWSmR7H0DpWzY285GjZrYKLnvKTQ0jfLOdGzO/QV+z87vuvZX/57m6mzuM6dl9hlgy2xERGuTeUXOHDYiIqJ196TXs79e7x8S3/9Y/oLor5c/I/pb5e8V/W8REdE6RY/IPur9w+L7H7ZpL8P/AgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgTuNzsDdMAHNmIyKidY8lO/tL5UvfwyP78Xv4edF/FjH9PkNrk4noD+VfFv1H5S+K/mr5c6K/yQQwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwh30Ac5gA5oz3Aab6//bWJqdE/1X5Z0V/ufyLov+kfEWPzCHKl/YZModxn2FJ9FfKl/9+TABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBz2Acwhwlgzngf4KBkZ/+p/Luif6P8C6L/tPwjov8+IqK1yR5Jz+F7+VuiP1P+nOhvRkRE624qfmR/hwlgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvsA5jABzBnvA0z1PTxad1V6PfsH9f5R8f218neJ/s/yp72PMJUfrZN+f2S/Vv5D0b/CBDCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHfQBzmADmjPcB7kt29tfKl+79R/bL5Uv3/iP78d7/c0nP4Vz5TfSz/Nuif6v8BdFfj4iI1p1W/Mj+JRPAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHPYBzGECmDPeB9inyJnD1/LPiP6L8k+I/pvyj4n+u/K3i/6f8hdFf7X8naL/q/xZ0f/LBDCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHfQBzmADmjPsAS4qcOayUf1L0X5e/Q/R/l79f9L+Uf0D0P5d/XPTflr9b9H9ERETr5hU/st9gAphDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPoA5/wAC3A/lwx8HiwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671AC0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADK0lEQVR4nO2dvcuOcRiGz5/ekuQjMSieZyQpJUVGomQyiEG9GUQxMcokIxNFBikDGUxSxChKSkmM9+0tA8lHkpKf5Xr+gdPmPI796Oqpo2t5rvu+W+9DF8QyJ0mtTTY5cu/jm/Inpj+Wf9j0b5f/0PT3ln/f9PeXf830j0uS2nSN46sPH2v+EXP+rUXWYPhvIIBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDCafwdnA0bIJw5SVKbLrHsPvyUpNYmuy29j4/L32L6r8p/Yvq7yj9n+ufLd3T1Pqr8i6Z/pvyVpv+FDRAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhEEA4BBAO9wDhsAHCmb0fYJsj9z6+KP+K6Z8s/7rpHyv/gumfLX/e9G+Wf9z0r5X/wPT3lb/a9D+xAcIhgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMLhHiAcNkA4s3uADY7c+/iu/AOmf6/8zab/uvy1pv+h/B2m/6z89ab/vvydpv+0fPv3swHCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDC4R4gHDZAOLN7APv78+UfMv07kqQ2dXSpDyp/uel/k6TWJjcsvY9Hy79s+qfK32P6j8pfavo/2ADhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4RBAOAQQDgGEQwDhcA8QDhsgnDlJUps2y55tjzZdZ/oL5U9Mfyzfej5efZi9H+Bf3y+wYPrryj9h+lfLP236l9gA4RBAOAQQDgGEQwDhEEA4BBAOAYRDAOEQQDgEEA4BhEMA4XAPEA4bIJzZPcCcZffhtyS1Nllm6X38Xv5q0/9U/rzp3yx/o+m/lSS16WLHVx9+1fzb5vzD5V8y/dNsgHAIIBwCCIcAwiGAcAggHAIIhwDCIYBwCCAcAgiHAMIhgHC4BwiHDRDO7HsBqxy59/Fz+QdN/275203/uSSpTb2Q+/Cn5m8z578o/6Xpby3/jOlflCS16QrHVx++sgHCIYBwCCAcAgiHAMIhgHAIIBwCCIcAwiGAcAggHAIIhwDC4R4gnL8soxXlKiZiUAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "Epoch 1730 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.2\n",
      "Epoch 1731 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.2\n",
      "Epoch 1732 \t reward: 1.5 \t length 2 \t mean_length 2.84 \t epsilon 0.2\n",
      "Epoch 1733 \t reward: 7.0 \t length 4 \t mean_length 2.84 \t epsilon 0.2\n",
      "Epoch 1734 \t reward: -0.5 \t length 1 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1735 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1736 \t reward: 1.5 \t length 2 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1737 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1738 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1739 \t reward: 10.5 \t length 5 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1740 \t reward: -0.5 \t length 1 \t mean_length 2.74 \t epsilon 0.2\n",
      "Epoch 1741 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1742 \t reward: 7.0 \t length 4 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1743 \t reward: 1.5 \t length 2 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1744 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1745 \t reward: 10.5 \t length 5 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1746 \t reward: 7.0 \t length 4 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1747 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1748 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1749 \t reward: -0.5 \t length 1 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1750 \t reward: 7.0 \t length 4 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1751 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1752 \t reward: 7.0 \t length 4 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1753 \t reward: 4.0 \t length 3 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1754 \t reward: 4.0 \t length 3 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1755 \t reward: 10.5 \t length 5 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1756 \t reward: 7.0 \t length 4 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1757 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1758 \t reward: 1.5 \t length 2 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1759 \t reward: 10.5 \t length 5 \t mean_length 2.76 \t epsilon 0.2\n",
      "Epoch 1760 \t reward: 10.5 \t length 5 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1761 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1762 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1763 \t reward: 7.0 \t length 4 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1764 \t reward: -0.5 \t length 1 \t mean_length 2.76 \t epsilon 0.2\n",
      "Epoch 1765 \t reward: 7.0 \t length 4 \t mean_length 2.78 \t epsilon 0.2\n",
      "Epoch 1766 \t reward: 10.5 \t length 5 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1767 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1768 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1769 \t reward: 7.0 \t length 4 \t mean_length 2.84 \t epsilon 0.2\n",
      "Epoch 1770 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1771 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1772 \t reward: 1.5 \t length 2 \t mean_length 2.8 \t epsilon 0.2\n",
      "Epoch 1773 \t reward: 7.0 \t length 4 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1774 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1775 \t reward: -0.5 \t length 1 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1776 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.2\n",
      "Epoch 1777 \t reward: 7.0 \t length 4 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1778 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1779 \t reward: 10.5 \t length 5 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1780 \t reward: 1.5 \t length 2 \t mean_length 2.82 \t epsilon 0.19\n",
      "Epoch 1781 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.19\n",
      "Epoch 1782 \t reward: 7.0 \t length 4 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1783 \t reward: 7.0 \t length 4 \t mean_length 2.88 \t epsilon 0.19\n",
      "Epoch 1784 \t reward: 1.5 \t length 2 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1785 \t reward: 4.0 \t length 3 \t mean_length 2.9 \t epsilon 0.19\n",
      "Epoch 1786 \t reward: 1.5 \t length 2 \t mean_length 2.88 \t epsilon 0.19\n",
      "Epoch 1787 \t reward: 4.0 \t length 3 \t mean_length 2.88 \t epsilon 0.19\n",
      "Epoch 1788 \t reward: 19.0 \t length 7 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1789 \t reward: 7.0 \t length 4 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1790 \t reward: 10.5 \t length 5 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1791 \t reward: 7.0 \t length 4 \t mean_length 3.04 \t epsilon 0.19\n",
      "Epoch 1792 \t reward: 4.0 \t length 3 \t mean_length 3.03 \t epsilon 0.19\n",
      "Epoch 1793 \t reward: 1.5 \t length 2 \t mean_length 3.04 \t epsilon 0.19\n",
      "Epoch 1794 \t reward: 7.0 \t length 4 \t mean_length 3.05 \t epsilon 0.19\n",
      "Epoch 1795 \t reward: 4.0 \t length 3 \t mean_length 3.08 \t epsilon 0.19\n",
      "Epoch 1796 \t reward: 4.0 \t length 3 \t mean_length 3.07 \t epsilon 0.19\n",
      "Epoch 1797 \t reward: 1.5 \t length 2 \t mean_length 3.08 \t epsilon 0.19\n",
      "Epoch 1798 \t reward: 4.0 \t length 3 \t mean_length 3.07 \t epsilon 0.19\n",
      "Epoch 1799 \t reward: 4.0 \t length 3 \t mean_length 3.1 \t epsilon 0.19\n",
      "Epoch 1800 \t reward: -0.5 \t length 1 \t mean_length 3.05 \t epsilon 0.19\n",
      "Epoch 1801 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1802 \t reward: -0.5 \t length 1 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1803 \t reward: 7.0 \t length 4 \t mean_length 2.9 \t epsilon 0.19\n",
      "Epoch 1804 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1805 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1806 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1807 \t reward: -0.5 \t length 1 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1808 \t reward: 10.5 \t length 5 \t mean_length 2.88 \t epsilon 0.19\n",
      "Epoch 1809 \t reward: -0.5 \t length 1 \t mean_length 2.84 \t epsilon 0.19\n",
      "Epoch 1810 \t reward: 7.0 \t length 4 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1811 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.19\n",
      "Epoch 1812 \t reward: -0.5 \t length 1 \t mean_length 2.82 \t epsilon 0.19\n",
      "Epoch 1813 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.19\n",
      "Epoch 1814 \t reward: 14.5 \t length 6 \t mean_length 2.88 \t epsilon 0.19\n",
      "Epoch 1815 \t reward: 1.5 \t length 2 \t mean_length 2.9 \t epsilon 0.19\n",
      "Epoch 1816 \t reward: 4.0 \t length 3 \t mean_length 2.9 \t epsilon 0.19\n",
      "Epoch 1817 \t reward: -0.5 \t length 1 \t mean_length 2.94 \t epsilon 0.19\n",
      "Epoch 1818 \t reward: 10.5 \t length 5 \t mean_length 2.98 \t epsilon 0.19\n",
      "Epoch 1819 \t reward: 19.0 \t length 7 \t mean_length 3.02 \t epsilon 0.19\n",
      "Epoch 1820 \t reward: 1.5 \t length 2 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1821 \t reward: 7.0 \t length 4 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1822 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1823 \t reward: 7.0 \t length 4 \t mean_length 2.94 \t epsilon 0.19\n",
      "Epoch 1824 \t reward: 4.0 \t length 3 \t mean_length 2.94 \t epsilon 0.19\n",
      "Epoch 1825 \t reward: 4.0 \t length 3 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1826 \t reward: 1.5 \t length 2 \t mean_length 2.94 \t epsilon 0.19\n",
      "Epoch 1827 \t reward: 7.0 \t length 4 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1828 \t reward: 1.5 \t length 2 \t mean_length 2.94 \t epsilon 0.19\n",
      "Epoch 1829 \t reward: -0.5 \t length 1 \t mean_length 2.96 \t epsilon 0.19\n",
      "Q_matrix of initial state, after training: step 1701\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671970>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADMElEQVR4nO2dT4vNcRSHz5dBgxBGoe5NWKBMFvIClD+RInsbFlbKC1FWFmzsRYn8KS9AFhqFBdLvVygzhDAhjsU99w18xsrnefZPZ7o9czZz7ndaZpcBtkxERLQ2WKvImf3HiIhow0vS9OzOlt9EP8vfLvovyj8p+tfKPyL6t8u/LvonIiJaG+yU9OyfLZIGw38DAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGBO48/B3rABzJmIiIg2nJHs7KbLPy/6F8o/I/qXIyJaG0xJevaz5a8U/a/lnxL9q+VvFf1X5U+K/jwbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5gzfh9gjSJn9p8iIqINB9L07Pqav0+c/7D8PaL/uPznor+j/GnRnyl/qej/LH+V6H9hA5hDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZjDPYA5bABzxu8D7Jfs7B6Ur03PbqS3wWZJz/5NzV/Qe/+tDZaL87+XPy/6o+/1t+EKxY/svpV/RfRPswHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AHDaAOeN7gPWSnd1c+U9Ef3dERGvi8wLZR/kbRf9d+dIvQmb/Z/QDDFcrfmT3ufxzon+x/Keiv4sNYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA73AOawAcwZ/78A6R4gs5/7R/4B0b9f/kLf+18m+j/KPyj698qfFP3RuwRteEfxI7vDbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gzvgeYLEiZ/a/yz8q+rfK3yL6r/+FH224V/Eju0c1/5g4/2bNPy7Ov1Hzp8T5s2wAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAc7gHMIcNYM74HmCbImf2L8vfJPpvy1/oPcIh0b9b/jrR/xAREW24RPEju1/lbxD99xEL+/zYAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2DOX9uwCOV5k7p2AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F671970>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADPUlEQVR4nO2dTYuOcRSHf38mpRTSeM1zU+OlKAsLC4kFK0nsLdjY+gTEJ7C1YWFP0qxmFiQLCwtFeSvuW95mEkop0bFw7i9wZud3XftrzrO4OpvnzP9pEX0IbJmSpNYmmytyxPBBktS6i6Xp0V/J+euK87+kP1P0X6e/tei/S3990V+QJLWuokvRK+cfLc6fX1abDP8LBGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5ja+DvWEDmDMlSWrdr5Id/Yr0TxT9u+nvK/pPJKm1yZGSHsO99HcW/Zfp3yj6Z9M/VvTn0l9d9L+zAcwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMzhHsAcNoA54/sAS/3/+CW9L9Da5EzRv5n+yaJ/J/3PRX9D+seL/mz6e4r+s/Q3Ff2PbABzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzuAcwhw1gzvg+wHTJjn4x/bmif0ySWpscKOkxPMr554vzr+X8LcX579OvjY9B6S8v+n/+/YHuQu0D9FfZAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOZwD2AOG8Cc8X2ANRU5YvgmSWrd5dL06C/l/PJ79+mvKvo/0l/q7wXsKPqvJEmtO1zxFf399D8V/Y1sAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHO4BzCHDWDOeA+wvSJHDG8kSa1bWZoe/c+cf7Q4fz792aJ/PP3S548Yxs9fuoeIGMZ7iG1F/60kqXWl3wtQ9PxegDsEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjBnvAeYqsgRw+/0Txf9W+nvKvov0j9Y9B9KklpXel9A0Y/vC5wqzr+d/vqiv5D+oaL/gA1gDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDvcA5rABzBnvAfZW5Ijhafr7i/7j9NcW/a/pnyv619OfKfqv099d9J+nP130FyUt6X0GNoA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA53AOY8xcdDxHlym2E4wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGBA size=128x128 at 0x1978F6717F0>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAYAAADDPmHLAAADK0lEQVR4nO2cv6vNcRjHnw/Xj4lIYnDOLZGwSRYmTKLYLLqDW9ePMpJFFjEqrlvXcLPYKDJhYpFsSKTu+RpIIibceCzP+QfeZ/N+vfZXT9/T6zzL9+nbMgcZYMtYRERrvY2KnNm9L3+96H8s/5Do3yv/jugfKf+B6B8o/5ron46IiNZfofiRgx81f1KcP7tIGgz/DQRgDgGYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOY3Xwd6wAcwZi4iI1l8q2Tn4HRHRWm9c0rObL3+76L8s/67oHy7/pOhPl6/okdlF+VdE/2z58j0GG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOYM/w+wC5FzuyelX9R9C+UPyP6U+WfE/3L5e8U/eflnxH9q+WP+vzbRP8VG8AcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAc7gHMYQOYM7wH2KTImd278neL/tPyR3qfHq2/WPEjB39q/n5x/sPyl4n+r/LXiP6X8jeL/ls2gDkEYA4BmEMA5hCAOQRgDgGYQwDmEIA5BGAOAZhDAOYQgDncA5jDBjBneA9wUJEzu/vl7xP9RxER0fqKHpGDKH+l6H+PiGitd0PSsztR/pzoT5S/RfTfRERE62t/5Bz8ZQOYQwDmEIA5BGAOAZhDAOYQgDkEYA4BmEMA5hCAOQRgDgGYwz2AOWwAc4b3ADsUObN7ERERrb9Wmp6Dz+UvEf2F8reK/uuIiNZ6yyU9u5/lfxL9deVPiP5c+edF/xIbwBwCMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBzuAcxhA5gzvAeQ3sdndgvlrxL9b+WvFv2v5Z8S/evlj4v+fPmj/n4zoj9V/i3RP8YGMIcAzCEAcwjAHAIwhwDMIQBzCMAcAjCHAMwhAHMIwBwCMId7AHPYAOYM7wE2KHJm96H8KdGfKX+P6D8p/7jo3yx/r+g/Lv+26B8tf1L0ZyNipO8rsAHMIQBzCMAcAjCHAMwhAHMIwBwCMIcAzCEAcwjAHAIwhwDM4R7AnH//MBfl/ZSqTAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n",
      "Epoch 1830 \t reward: 4.0 \t length 3 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1831 \t reward: 10.5 \t length 5 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1832 \t reward: 10.5 \t length 5 \t mean_length 3.03 \t epsilon 0.19\n",
      "Epoch 1833 \t reward: 14.5 \t length 6 \t mean_length 3.06 \t epsilon 0.19\n",
      "Epoch 1834 \t reward: 4.0 \t length 3 \t mean_length 3.05 \t epsilon 0.19\n",
      "Epoch 1835 \t reward: 7.0 \t length 4 \t mean_length 3.1 \t epsilon 0.19\n",
      "Epoch 1836 \t reward: -0.5 \t length 1 \t mean_length 3.05 \t epsilon 0.19\n",
      "Epoch 1837 \t reward: 1.5 \t length 2 \t mean_length 3.08 \t epsilon 0.19\n",
      "Epoch 1838 \t reward: -0.5 \t length 1 \t mean_length 3.03 \t epsilon 0.19\n",
      "Epoch 1839 \t reward: 4.0 \t length 3 \t mean_length 3.04 \t epsilon 0.19\n",
      "Epoch 1840 \t reward: 1.5 \t length 2 \t mean_length 3.01 \t epsilon 0.19\n",
      "Epoch 1841 \t reward: 4.0 \t length 3 \t mean_length 3.06 \t epsilon 0.19\n",
      "Epoch 1842 \t reward: -0.5 \t length 1 \t mean_length 3.01 \t epsilon 0.19\n",
      "Epoch 1843 \t reward: -0.5 \t length 1 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1844 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1845 \t reward: 4.0 \t length 3 \t mean_length 3.0 \t epsilon 0.19\n",
      "Epoch 1846 \t reward: -0.5 \t length 1 \t mean_length 2.96 \t epsilon 0.19\n",
      "Epoch 1847 \t reward: 4.0 \t length 3 \t mean_length 2.94 \t epsilon 0.18\n",
      "Epoch 1848 \t reward: -0.5 \t length 1 \t mean_length 2.9 \t epsilon 0.18\n",
      "Epoch 1849 \t reward: 4.0 \t length 3 \t mean_length 2.9 \t epsilon 0.18\n",
      "Epoch 1850 \t reward: 1.5 \t length 2 \t mean_length 2.88 \t epsilon 0.18\n",
      "Epoch 1851 \t reward: -0.5 \t length 1 \t mean_length 2.86 \t epsilon 0.18\n",
      "Epoch 1852 \t reward: 4.0 \t length 3 \t mean_length 2.86 \t epsilon 0.18\n",
      "Epoch 1853 \t reward: 14.5 \t length 6 \t mean_length 2.84 \t epsilon 0.18\n",
      "Epoch 1854 \t reward: 10.5 \t length 5 \t mean_length 2.88 \t epsilon 0.18\n",
      "Epoch 1855 \t reward: 10.5 \t length 5 \t mean_length 2.88 \t epsilon 0.18\n",
      "Epoch 1856 \t reward: -0.5 \t length 1 \t mean_length 2.84 \t epsilon 0.18\n",
      "Epoch 1857 \t reward: 7.0 \t length 4 \t mean_length 2.82 \t epsilon 0.18\n",
      "Epoch 1858 \t reward: 4.0 \t length 3 \t mean_length 2.82 \t epsilon 0.18\n",
      "Epoch 1859 \t reward: 1.5 \t length 2 \t mean_length 2.84 \t epsilon 0.18\n",
      "Epoch 1860 \t reward: 4.0 \t length 3 \t mean_length 2.84 \t epsilon 0.18\n",
      "Epoch 1861 \t reward: 1.5 \t length 2 \t mean_length 2.8 \t epsilon 0.18\n",
      "Epoch 1862 \t reward: -0.5 \t length 1 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1863 \t reward: 4.0 \t length 3 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1864 \t reward: -0.5 \t length 1 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1865 \t reward: 4.0 \t length 3 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1866 \t reward: 10.5 \t length 5 \t mean_length 2.8 \t epsilon 0.18\n",
      "Epoch 1867 \t reward: 7.0 \t length 4 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1868 \t reward: 7.0 \t length 4 \t mean_length 2.78 \t epsilon 0.18\n",
      "Epoch 1869 \t reward: 7.0 \t length 4 \t mean_length 2.78 \t epsilon 0.18\n",
      "Epoch 1870 \t reward: -0.5 \t length 1 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1871 \t reward: 1.5 \t length 2 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1872 \t reward: 4.0 \t length 3 \t mean_length 2.76 \t epsilon 0.18\n",
      "Epoch 1873 \t reward: 4.0 \t length 3 \t mean_length 2.78 \t epsilon 0.18\n",
      "Epoch 1874 \t reward: -0.5 \t length 1 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1875 \t reward: 7.0 \t length 4 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1876 \t reward: -0.5 \t length 1 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1877 \t reward: 4.0 \t length 3 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1878 \t reward: 4.0 \t length 3 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1879 \t reward: -0.5 \t length 1 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1880 \t reward: 4.0 \t length 3 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1881 \t reward: 4.0 \t length 3 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1882 \t reward: 7.0 \t length 4 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1883 \t reward: -0.5 \t length 1 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1884 \t reward: 4.0 \t length 3 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1885 \t reward: 7.0 \t length 4 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1886 \t reward: -0.5 \t length 1 \t mean_length 2.7 \t epsilon 0.18\n",
      "Epoch 1887 \t reward: 1.5 \t length 2 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1888 \t reward: 7.0 \t length 4 \t mean_length 2.74 \t epsilon 0.18\n",
      "Epoch 1889 \t reward: 4.0 \t length 3 \t mean_length 2.66 \t epsilon 0.18\n",
      "Epoch 1890 \t reward: 1.5 \t length 2 \t mean_length 2.64 \t epsilon 0.18\n",
      "Epoch 1891 \t reward: 4.0 \t length 3 \t mean_length 2.6 \t epsilon 0.18\n",
      "Epoch 1892 \t reward: 1.5 \t length 2 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1893 \t reward: 4.0 \t length 3 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1894 \t reward: -0.5 \t length 1 \t mean_length 2.54 \t epsilon 0.18\n",
      "Epoch 1895 \t reward: 4.0 \t length 3 \t mean_length 2.52 \t epsilon 0.18\n",
      "Epoch 1896 \t reward: 14.5 \t length 6 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1897 \t reward: -0.5 \t length 1 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1898 \t reward: 4.0 \t length 3 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1899 \t reward: 4.0 \t length 3 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1900 \t reward: 10.5 \t length 5 \t mean_length 2.62 \t epsilon 0.18\n",
      "Epoch 1901 \t reward: -0.5 \t length 1 \t mean_length 2.66 \t epsilon 0.18\n",
      "Epoch 1902 \t reward: 1.5 \t length 2 \t mean_length 2.64 \t epsilon 0.18\n",
      "Epoch 1903 \t reward: 10.5 \t length 5 \t mean_length 2.68 \t epsilon 0.18\n",
      "Epoch 1904 \t reward: 4.0 \t length 3 \t mean_length 2.68 \t epsilon 0.18\n",
      "Epoch 1905 \t reward: -0.5 \t length 1 \t mean_length 2.72 \t epsilon 0.18\n",
      "Epoch 1906 \t reward: -0.5 \t length 1 \t mean_length 2.68 \t epsilon 0.18\n",
      "Epoch 1907 \t reward: -0.5 \t length 1 \t mean_length 2.68 \t epsilon 0.18\n",
      "Epoch 1908 \t reward: 1.5 \t length 2 \t mean_length 2.66 \t epsilon 0.18\n",
      "Epoch 1909 \t reward: 1.5 \t length 2 \t mean_length 2.62 \t epsilon 0.18\n",
      "Epoch 1910 \t reward: -0.5 \t length 1 \t mean_length 2.58 \t epsilon 0.18\n",
      "Epoch 1911 \t reward: 7.0 \t length 4 \t mean_length 2.56 \t epsilon 0.18\n",
      "Epoch 1912 \t reward: 10.5 \t length 5 \t mean_length 2.6 \t epsilon 0.18\n",
      "Epoch 1913 \t reward: -0.5 \t length 1 \t mean_length 2.64 \t epsilon 0.18\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c5666043241d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Let the optimizer do the backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New model\n",
    "board = Board()\n",
    "test_board = Board()\n",
    "model = Model()\n",
    "model.to(device)\n",
    "epsilon = 0.9\n",
    "epsilon_start = 0.9\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 1024\n",
    "learning_rate = 0.3\n",
    "gamma = 0.9\n",
    "batch_size = 64\n",
    "max_history = 1024\n",
    "n_epochs = 4096\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# (State, Action, NextState, Reward, Done) container for each action\n",
    "history = []\n",
    "mean_length = []\n",
    "\n",
    "# Populating the history with every possible move\n",
    "for i_epoch in range(n_epochs):\n",
    "    # Reset the board\n",
    "    done = False\n",
    "    s, signal = board.reset()\n",
    "    s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Play a full game\n",
    "    length = 0\n",
    "    total_r = 0\n",
    "    while not done:\n",
    "        length += 1\n",
    "\n",
    "        #========================== ACTION\n",
    "        # Play a random action or ask the model\n",
    "        a = 0\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * i_epoch / epsilon_decay)\n",
    "        if np.random.rand() <= epsilon:\n",
    "            a = torch.randint(0, 65, (1,)).item()\n",
    "        else:\n",
    "            a_scores = model(s.to(device))\n",
    "            a = torch.argmax(a_scores).item()\n",
    "        encoded_a = encode_action(a)\n",
    "\n",
    "        # Perform the action in the game\n",
    "        n_s, signal = board.step(encoded_a)\n",
    "        n_s = torch.tensor(n_s, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "        # Process the signal\n",
    "        r = reward_from_signal(signal, board) + length / 2\n",
    "        total_r += r\n",
    "        done = game_over_from_signal(signal, board)\n",
    "        #-------------------------- ACTION\n",
    "\n",
    "        #========================== BLACK SAMPLE MOVE\n",
    "        black_action = board.sample()\n",
    "        n_s, signal = board.step(black_action)\n",
    "        n_s = torch.tensor(n_s, dtype=torch.float).unsqueeze(0)\n",
    "        #-------------------------- BLACK SAMPLE MOVE\n",
    "\n",
    "        #========================== HISTORY\n",
    "        # Add the record to history\n",
    "        for i_history in range(length):\n",
    "            history.append((s, a, r, n_s, done))\n",
    "        \n",
    "        # Update current state\n",
    "        s = n_s\n",
    "\n",
    "        # Continue playing if we don't have enough moves to learn in the history\n",
    "        if len(history) <= batch_size:\n",
    "            continue\n",
    "\n",
    "        # Keep history the wanted size removing first element in list\n",
    "        while len(history) > max_history:\n",
    "            history.pop(0)\n",
    "        #-------------------------- HISTORY\n",
    "\n",
    "        #========================== Q LEARNING\n",
    "        # Q Learning matrix for the initial state\n",
    "        targets = torch.zeros(batch_size, 65)\n",
    "        inputs = torch.zeros(batch_size, 2, 8, 8)\n",
    "        i_batch = 0\n",
    "        samples = random.sample(history, batch_size - 1) + [history[-1]]\n",
    "        for i_batch, (s_, a_, r_, n_s_, done_) in enumerate(samples):\n",
    "            # Build bellman equation for the Q function\n",
    "            inputs[i_batch] = s_\n",
    "            targets[i_batch] = model(s_.to(device)).view(-1) # torch.zeros((65,), dtype=torch.float)\n",
    "            Q_sa = model(n_s_.to(device))\n",
    "\n",
    "            # Emplace reward\n",
    "            if done_:\n",
    "                targets[i_batch, a_] = r_\n",
    "            else:\n",
    "                targets[i_batch, a_] = r_ + gamma * torch.max(Q_sa)\n",
    "\n",
    "\n",
    "        # Compute loss on the difference between model output and target_q\n",
    "        model.train()\n",
    "        q_pred = model(inputs.to(device))        \n",
    "        targets = torch.clip(targets, -3, 3)\n",
    "        loss = criterion(targets.to(device), q_pred)\n",
    "\n",
    "        # Let the optimizer do the backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if len(mean_length) <= 50:\n",
    "        mean_length.append(length)\n",
    "    else:\n",
    "        mean_length.pop(0)\n",
    "    print(f\"Epoch {i_epoch} \\t reward: {total_r} \\t length {length} \\t mean_length {int(np.mean(mean_length) * 100)/100} \\t epsilon {int(epsilon*100)/100}\")\n",
    "    \n",
    "    # Display matrixes\n",
    "    if i_epoch > batch_size and (i_epoch - batch_size) % 100 == 1:\n",
    "        print(f\"Q_matrix of initial state, after training: step {i_epoch - batch_size}\")\n",
    "        s, signal = test_board.reset()\n",
    "        s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "        a_scores = model(s.to(device))\n",
    "        q1 = a_scores.to(\"cpu\")\n",
    "        render_q(q1.tolist()[0])\n",
    "        a = torch.argmax(a_scores).item()\n",
    "        encoded_a = encode_action(a)\n",
    "        print(a)\n",
    "        # print(a_scores.to(\"cpu\"))\n",
    "        # Perform the action in the game\n",
    "        n_s, signal = test_board.step(encoded_a)\n",
    "        if not game_over_from_signal(signal, test_board):\n",
    "            black_action = test_board.sample()\n",
    "            n_s, signal = test_board.step(black_action)\n",
    "            n_s = torch.tensor(n_s, dtype=torch.float).unsqueeze(0)\n",
    "            a_scores = model(n_s.to(device))\n",
    "            q2 = a_scores.to(\"cpu\")\n",
    "            render_q(q2.tolist()[0])\n",
    "            print(\"-------------\")\n",
    "            render_q((q2 - q1).tolist()[0])\n",
    "            a = torch.argmax(a_scores).item()\n",
    "            encoded_a = encode_action(a)\n",
    "            print(a)\n",
    "    # if i_epoch > batch_size and (i_epoch - batch_size) % 1000 == 999:\n",
    "    #     print(\"MEMOIRE ==============================================\")\n",
    "    #     print(history[:5])"
   ]
  }
 ]
}